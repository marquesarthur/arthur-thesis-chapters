


\section{Relevant text detection}
\label{cp4:corpus-relevant-text}




Next, \red{we need to determine which 
of the text in the gathered artifacts could} provide information that assists a developer in solving her task.
In our corpus, this text represents \textit{golden data} that one can use to design and evaluate automatic tools that assist developers in the identification of information useful to their tasks. 
To produce it, we 
ask human experts to
mark the text that they deem useful and that provide information for tasks assigned to them~\cite{nadi2020, Robillard2015, marques2020}.



\subsection{Annotation process}


Our intention is that golden data reflect text that instructs developers to perform important actions to accomplish their task~\cite{Robillard2015, Lotufo2012}.
To this end, we describe the text inspected by the annotators, the annotators' background as well as annotation procedures.
% FIXME: find a way to format the standard deviation acronym
\textcolor{white}{\acs{stdv}} % force acronym to appear in the glossary for the time being



\subsubsection{Text inspected}




We restrict the manual identification of text relevant to a task to a random subset of 
50  out of the 300  tasks initially gathered (Section~\ref{cp4:corpus-tasks}).
\gm{-Why select 300? Just select 25 and 25 originally.}
This decision was motivated by the fact that 
creating golden data for the entirety of our tasks 
would require asking human annotators to inspect thousands of artifacts and more than 260,000 sentences, which would be a costly and time-consuming activity. 


% For each one of the 25 Stack Overflow tasks and the 25 GitHub tasks in this subset, we randomly selected 5 artifacts of at least three different types: 
% one API document, a GitHub issue discussion, one Stack Overflow answer, aand two miscellaneous Web pages.
% In total, our corpus contains
% 12,401 unique sentences, with an average of 63.59 sentences per artifact (stdv 66.28).



Although the sentences in the tasks selected for inspection represent
a fraction of the initial number of sentences gathered, 
the number of sentences inspected in \acs{DS-android}
is still significantly higher than 
artifact-specific corpora, such as the \textit{McGill} corpus on information correspondence with 2,445 sentences~\cite{arya2020} or the information types corpus with 4,656 labeled sentences~\cite{Arya2019}.
\art{I'm trying to justify that the size of the corpus is still considerable.}



\input{sections/cp4/tbl-corpus-summary}


\subsubsection{Annotators}


We recruited 3 graduate students with professional programming experience to produce \textit{golden} data for our corpus. Annotators had to have experience with Java development and they also had to be familiar with the types of artifacts they would encounter throughout the annotation process. 
On average, annotators self-reported 4.6 years of professional
programming experience (stdv 2.49, ranging from 1 to 7 years).



\subsubsection{Annotation procedures}



We divided tasks into batches of 10 tasks each as to avoid fatigue effects~\cite{Ponzanelli2017}. For each batch, annotators had task descriptions and links to artifacts pertinent to the respective task at their disposal. We asked annotators to write a short plan (250 words max~\cite{Rastkar2010}) with instructions that a developer could follow to complete the task successfully. 
The purpose of the plan was to ensure that annotators built enough context about the task.
While perusing artifacts, annotators also had to manually highlight sentences that they deemed useful and that provided information that assisted task completion---instructions similar to the ones used for the creation of the data in the \acs{DS-synthetic} corpus~\cite{marques2020}.


The annotation process was facilitated by a tool we created for this purpose. Figure~\ref{fig:corpus-annotation-tool} shows a screenshot from the tool in 
action, which works as a browser plug-in. The top-right corner panel in the figure shows the browser extension. \red{When an annotator clicked the \texttt{highlight} button, }
the tool instrumented the HTML of a page identifying individual sentences. The tool then allowed annotators to hover over identified sentences and to select them as relevant by clicking on the hovered text. For example, in the first paragraph, an annotator selected  the sentence
``\textit{Call {\small \texttt{ActivityOptions.setLockTaskEnabled()}} ... when starting the activity}'' as relevant to the Android lock mode task (Figure~\ref{fig:lock-screen-task}).







\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{cp4/annotation-tool}
    \caption{Annotation tool and relevant sentences marked by an annotator}
    \label{fig:corpus-annotation-tool}
\end{figure}




\subsection{Results}


Annotation required a total of \red{$\approx60$} hours of manual work and it was done throughout the course of 4 weeks.
Table~\ref{tbl:corpus-annotation-summary} provides summary statistics for the text marked by 
the annotators over all the artifacts inspected as well as on an artifact type basis.
On average the text deemed useful to a software task in the artifacts inspected comprises 
\red{5} sentences. 
We observe that the highest number of sentences marked originate from \red{artifact type}
while the lowest from \red{artifact} type. Potential explanation for that...


\input{sections/cp4/tbl-corpus-annotation-summary}








Since individuals might use different criteria to
assess the usefulness of a sentence to a given task~\cite{Barry1994, Barry1998, Freund2015},
we also report how many sentences marked by the annotators overlap.
Out of \red{n}
unique marked
sentences, 
\red{x} were marked by all annotators,
\red{y} by two of them and the remainder
---\red{z} sentences---by a single annotator.
This result is not surprising and it corroborates 
results on the construction of the \acs{DS-synthetic} 
corpus, where we observe that complete agreement 
occurred on 17\% of the annotated text.
\art{I can also present this data in a bar graph}








