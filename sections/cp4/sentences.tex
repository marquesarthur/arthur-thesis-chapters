


\section{Relevant text detection}
\label{cp4:corpus-relevant-text}






Next, we need to determine which 
of the text in the gathered artifacts could provide information that assists a developer in solving her task.
In our corpus, this text represents \textit{golden data} that one can use to design and evaluate automatic tools that assist developers in the identification of information useful to their tasks. 
To produce it, we 
ask experienced developers to
mark the text that they deem useful and that provide information for tasks assigned to them~\cite{nadi2020, Robillard2015, marques2020}.



\subsection{Annotation process}


Our intention is that golden data reflect text that instructs developers to perform important actions to accomplish their task~\cite{Robillard2015, Lotufo2012}.
To this end, we describe the annotators' background, annotation procedures, and the 
the text inspected by the annotators.
% FIXME: find a way to format the standard deviation acronym
\textcolor{white}{\acs{stdv}} % force acronym to appear in the glossary for the time being





\subsubsection{Annotators}


We recruited 3 graduate students with professional programming experience to produce \textit{golden} data for our corpus. Annotators had to have experience with Java development and they also had to be familiar with the types of artifacts they would encounter throughout the annotation process. 
On average, annotators self-reported 3.0 years of professional
programming experience (stdv 1.63, ranging from 1 to 5 years).



\subsubsection{Annotation procedures}



We divided tasks into batches of 10 tasks each as to avoid fatigue effects~\cite{Ponzanelli2017}. For each batch, annotators had task descriptions and links to artifacts pertinent to the respective task at their disposal. We asked annotators to write a short plan (250 words max~\cite{Rastkar2010}) with instructions that a developer could follow to complete the task successfully. 
The purpose of the plan was to ensure that annotators built enough context about the task.
While perusing artifacts, annotators also had to manually highlight sentences that they deemed useful and that provided information that assisted task completion---instructions similar to the ones used for the creation of the data in the \acs{DS-synthetic} corpus~\cite{marques2020}.


The annotation process was facilitated by a tool we created for this purpose. Figure~\ref{fig:corpus-annotation-tool} shows a screenshot from the tool in 
action, which works as a browser plug-in. The top-right corner panel in the figure shows the browser extension. When an annotator clicked the \texttt{highlight} button, 
the tool instrumented the HTML of a page identifying individual sentences. The tool then allowed annotators to hover over identified sentences and to select them as relevant by clicking on the hovered text. For example, in the first paragraph, an annotator selected  the sentence
``\textit{Call ActivityOptions.setLockTaskEnabled() ... when starting the activity}'' as relevant to the Android lock task (Figure~\ref{fig:lock-screen-task}).







\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{cp4/annotation-tool}
    \caption{Annotation tool and relevant sentences marked by an annotator}
    \label{fig:corpus-annotation-tool}
\end{figure}




\subsubsection{Text inspected}





Annotators had to inspect a total of 12,401 sentences originating from API documentation, Stack Overflow answers, GitHub issue discussion and miscellaneous Web pages.
These sentences comprise natural language written text---in this case, English---and do not involve source code snippets that may appear alongside the text.


Table~\ref{tbl:corpus-summary} gives insight into the number of sentences inspected per artifact type. 
We observe that API documents and miscellaneous Web pages contain the highest number of sentences in our corpus.
This is not surprising because API documents often contain boilerplate text 
and all the information needed for the usage of an API element is usually found in a single document~\cite{robillard2011field}.
Miscellaneous Web pages comprise blogs or tutorials, 
which often provide step-by-step instructions and accompanying examples, 
what likely explains the large number of sentences for this type of artifact~\cite{arya2020, Jiang2016b}.


The content of GitHub issues mostly resembles conversations~\cite{Rastkar2010}
and, beyond code snippets and minimal structured fields, the 
1,890 sentences inspected in this type of artifact comprise the description of a reported bug or a feature request as well as questions, answers, and discussion from  community members who are interested in resolving the issue at hand~\cite{zimmermann2010}.



For SO answers, the fewer number of inspected sentences (1,420) potentially relates to 
the fact that Stack Overflow offers little incentive for further discussion once a post has been answered.
Nonetheless, it is worthy noting that sentences with crucial information 
are not limited to the ones in an accepted answer~\cite{nadi2020}
and that later comments can be equally or more informative,
often providing notes about updates in an API component or framework~\cite{zhang2019so}.




\input{sections/cp4/tbl-corpus-summary}




\subsection{Corpus Description}

% \gm{The important point here is what
% is in the corpus - all sentences marked by annotators and who marked which?}


% \gm{Need to move table up so it is Table 4.3} -- OK
We structure tasks and annotation results in a format similar to the one shown in 
Table~\ref{tbl:corpus-data-structure}. 
Each task has a  title, link, description, and a set of pertinent artifacts.
In turn, each artifact has a type, title, link, and content,
where each sentence within an artifact's content
is preceded by the set of 
annotators (i.e., \texttt{none}, \texttt{A1}, \texttt{A2}, or \texttt{A3}) who marked the sentence as useful to the task.
As an example, in a Stack Overflow answer (artifact$_2$), all three annotators 
marked the sentence ``\textit{Have you checked RemoteControlClient?}''
as relevant to the lock mode task.

Overall, annotation required a total of $\approx60$ hours of manual work and it was done throughout the course of 4 weeks.
Table~\ref{tbl:corpus-annotation-summary} provides summary statistics for the text marked by 
the annotators over all the artifacts inspected as well as on an artifact type basis.





\clearpage

\input{sections/cp4/tbl-corpus-datastructure}

\clearpage

\rev{
Table~\ref{tbl:corpus-summary} summarizes results from the corpus annotation.
On average the text deemed useful to a software task in the artifacts inspected comprises 
9 sentences. 
We observe that the highest number of sentences marked originate from miscellaneous artifacts
while the lowest come from GitHub issue discussions. 
The high number of sentences marked in the former might relate to the fact that tutorials 
dilute and exemplify much of the information that a developer would also find in official API documentation~\cite{arya2020, Jiang2017}.
For the latter, the content on GitHub may be too project-specific and thus, 
a developer trying to find information useful for her task in this source
may have to prune large amounts of project-specific discussions to find the
pieces that interest her.}

\input{sections/cp4/tbl-corpus-annotation-summary}








\rev{
Since individuals might use different criteria to
assess the usefulness of a sentence to a given task~\cite{Barry1994, Barry1998, Freund2015},
we also report the percentage of sentences marked by one, two or three annotators.
Out of 1145
unique marked
sentences, 
10\% were marked by all annotators,
20\% by two of them and the remainder
---801 sentences---by a single annotator.
These results follow a similar trend to the ones observed in the 
\acs{DS-synthetic},
where sentences marked by a single annotator comprise the majority of the 
data.
}


% \input{sections/cp4/tbl-corpus-annotation-overlap}



\rev{We also ask if data in the \acs{DS-android}  can be threated homogeneously. 
To answer this question, we break down results by type of task, i.e., GitHub or Stack Overflow, as shown in Tables~\ref{tbl:corpus-annotation-summary-by-task} and~\ref{tbl:corpus-annotation-overlap-by-task}.
We use a Wilcoxon-Mann-Whitney test~\cite{mannWhitneyU} to check if there are any statistically significant differences between the number of sentences marked in each type of task,
computing effect size~\cite{grissom2005effect, cohen2013statistical} in case of differences.
Results suggest a statistically significant difference ($\alpha \le .05$)
between the number of sentences 
marked in Stack Overflow tasks with an average of 10.38 sentences marked per pertinent artifact versus GitHub tasks average of 7.12 sentences per pertinent artifact.
Since a medium effect size (\textit{Cohen's d = 0.41})~\cite{cohen2013statistical} 
follows from this comparison, later analyses should provide results both for the entirety of the corpus as well as on a per type of task basis.
}
% Due to these results, 
% later analyses should 
% However, the small difference in the effect size
% does not justify distinctions and thus, later analyses treat text marked for each type of task equally. 

\input{sections/cp4/tbl-corpus-annotation-summary-by-task}


\input{sections/cp4/tbl-corpus-annotation-overlap-by-task}




\input{sections/cp4/tbl-corpus-annotation-summary-by-annotator}


% a number significantly higher than 
% artifact-specific corpora, such as the \textit{McGill} corpus on information correspondence with 2,445 sentences~\cite{arya2020} or the information types corpus with 4,656 labeled sentences~\cite{Arya2019}.
% Although only small parts of API documents or miscellaneous Web pages are likely to
% apply to a particular task~\cite{robillard2011field, Jiang2016b, Jiang2017},
% studies have shown that fragmenting API documentation 
% makes the information less discoverable~\cite{robillard2011field}. Hence,



% We restrict the manual identification of text relevant to a task to a random subset of 
% 50  out of the 300  tasks initially gathered (Section~\ref{cp4:corpus-tasks}).
% \gm{-Why select 300? Just select 25 and 25 originally.}
% This decision was motivated by the fact that 
% creating golden data for the entirety of our tasks 
% would require asking human annotators to inspect thousands of artifacts and more than 260,000 sentences, which would be a costly and time-consuming activity. 


% For each one of the 25 Stack Overflow tasks and the 25 GitHub tasks in this subset, we randomly selected 5 artifacts of at least three different types: 
% one API document, a GitHub issue discussion, one Stack Overflow answer, aand two miscellaneous Web pages.
% In total, our corpus contains
% 12,401 unique sentences, with an average of 63.59 sentences per artifact (stdv 66.28).

