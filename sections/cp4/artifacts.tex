
\section{Artifact Selection}
\label{cp4:corpus-artifacts}


When selecting artifacts pertinent to a task in our corpus, we seek to simulate everyday practices on how developers search the Web~\cite{rao2020, Xia2017}.
We formulate a query for each task and use a Web search engine to retrieve artifacts that are pertinent to that task, as described below.


\subsubsection{Artifact sources}

% \gm{Aren't most of these artifact
% types chosen because there are existing
% techniques? Have you considered doing
% this more empirically - report on the
% kinds of artifact types and select
% most common?}


The artifacts sought to find useful information or knowledge for completing a task
 depend on the type of task a developer performs.
For example, when using a new framework or library, a developer refers to official API documentation~\cite{Li2013,robillard2011field} while, for debugging or error diagnostic tasks, community-based sources are preferred~\cite{Li2013,Breu2010}.
Despite such variability, researchers have observed that 
technical blogs, API documentation, and community forums are sources
commonly used by developer to forage information that assists task completion~\cite{Li2013, josyula2018}.



We use this knowledge to restrict artifact selection to well-known and studied artifact types within these sources~\cite{Starke2009,Kevic2014, Li2013}, namely Android and Java SE API documentation, GitHub issues, Stack Overflow answers, and Web tutorials or blog posts on Java and Android development.





\subsubsection{Query formulation}


%  \gm{Seems like
% motivation that is in next paragraph
% should come first.}


Coming up with proper search terms is a critical step of any search~\cite{Haiduc2013}
and, ideally, we should be able to formulate a query with terms able to retrieve the most pertinent artifacts for a software task.
However, studies have shown that developers perform poorly in identifying good search terms~\cite{Starke2009,Kevic2014} and thus, using a task's title
as an educated approximation to terms that a developer might use is a common procedure adopted by other studies in the field (e.g.,~\cite{Xu2017} or ~\cite{Silva2019}).
Hence, we use a task's title (i.e., SO question or GitHub issue title) as the seed to search for pertinent artifacts.



\subsubsection{Search results}


We use \texttt{googlesearch} API~\cite{googlesearch} to get the results of a query. 
We fetch a maximum of 5 resources per artifact source --- a limitation necessary due to throttling or even blocking mechanisms in the APIs used to get the content of each source considered.
\gm{Isn't the real reason because you
don't want more?}
\art{I think it's both. I don't want more and there's throttling/IP blocking}
\gm{-the throttling argument is weak. You could just wait more time to search. I think this
is about the human evaluation capability so suggest you rely on that part of the argument.}


When selecting results, we exclude any entry that does not appear in the Amazon Alexa~\cite{alexa} Web traffic for Java and Android development in the period from April 2020 to March 2021. 
While applying this filter can significantly decrease the number of artifacts per task, it ensures that results are indeed related to software development. 
For instance, for a task discussing ``\textit{left and right-hand swap}'' 
filtering avoided fetching resources about  \textit{stock swap} operations.
Table~\ref{tbl:googlesearch-example-git} shows one search result per artifact source for the GitHub task introduced in Section~\ref{cp4:corpus-tasks}.


\input{sections/cp4/tbl-googlesearch-results.tex}

\subsubsection{Artifact's content}

Last, we need to extract the natural language text within an artifact so that 
techniques that automate the identification of text relevant to a task can be built 
using our corpus.  This step requires processing an artifact's content 
into a sequence of individual sentences.
Given a search result \texttt{URL}, we use \texttt{BeautifulSoup}~\cite{beautifulsoup4},
\texttt{StackAPI}~\cite{StackAPI} and \texttt{PyGithub}~\cite{PyGithub}
to fetch the artifacts' content
and the Stanford CoreNLP toolkit~\cite{CoreNLP} for the identification of 
individual sentences,
which are common procedures for processing the artifact types found in our corpus~\cite{Arya2019, nadi2020}.








