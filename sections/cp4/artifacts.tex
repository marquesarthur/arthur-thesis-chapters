
\section{Artifact Selection}
\label{cp4:corpus-artifacts}


When selecting artifacts pertinent to a task in our corpus, we seek to simulate everyday practices on how developers search the Web~\cite{rao2020, Xia2017}.
We formulate a query for each task and use a Web search engine to retrieve artifacts that are pertinent to that task, as described below.


\subsubsection{Artifact sources}

% \gm{Aren't most of these artifact
% types chosen because there are existing
% techniques? Have you considered doing
% this more empirically - report on the
% kinds of artifact types and select
% most common?}


The artifacts sought to find useful information or knowledge for completing a task
 depend on the type of task a developer performs.
For example, when using a new framework or library, a developer refers to official API documentation~\cite{Li2013,robillard2011field} while, for debugging or error diagnostic tasks, community-based sources are preferred~\cite{Li2013,Breu2010}.
Despite such variability, researchers have observed that 
technical blogs, API documentation, and community forums are sources
commonly used by developer to forage information that assists task completion~\cite{Li2013, josyula2018}.



We use this knowledge to restrict artifact selection to well-known and studied artifact types within these sources~\cite{Starke2009,Kevic2014, Li2013}, namely Android and Java SE API documentation, GitHub issues, Stack Overflow answers, and Web tutorials or blog posts on Java and Android development.





\subsubsection{Query formulation}


%  \gm{Seems like
% motivation that is in next paragraph
% should come first.}


Coming up with proper search terms is a critical step of any search~\cite{Haiduc2013}
and, ideally, we should be able to formulate a query with terms able to retrieve the most pertinent artifacts for a software task.
However, studies have shown that developers perform poorly in identifying good search terms~\cite{latoza2006, Starke2009,Kevic2014} and thus, using a task's title
as an educated approximation to terms that a developer might use is a common procedure adopted by other studies in the field (e.g.,~\cite{Xu2017} or ~\cite{Silva2019}).
Hence, we use a task's title (i.e., SO question or GitHub issue title) as the seed to search for pertinent artifacts.



\subsubsection{Search results}


We use \texttt{googlesearch} API~\cite{googlesearch} to get the results of a query,
\red{fetching a maximum of 5 resources per task 
(one API document, a GitHub issue discussion, one Stack Overflow answer, and two miscellaneous Web pages).
Our decision to limit the number of artifacts fetched for a task relates to the
time-consuming~\cite{Zhang2013, al2017} and cognitively demanding~\cite{Piorkowski2016, Bystrom1995, clark2013relevance} 
nature of asking annotators to carefully read, understand, and decide whether the text
within the fetched artifacts is relevant to a given task, as Section~\ref{cp4:corpus-relevant-text} further details.
}



When selecting results, we exclude any entry that does not appear in the Amazon Alexa~\cite{alexa} Web traffic for Java and Android development in the period from April 2020 to March 2021. 
While applying this filter can significantly decrease the number of artifacts per task, it ensures that results are indeed related to software development. 
For instance, for a task discussing ``\textit{left and right-hand swap}'' 
filtering avoided fetching resources about  \textit{stock swap} operations.
Table~\ref{tbl:googlesearch-example-git} shows one search result per artifact source for the GitHub task introduced in Section~\ref{cp4:corpus-tasks}





\input{sections/cp4/tbl-googlesearch-results.tex}

\subsubsection{Artifact's content}

Last, we need to extract the natural language text within an artifact so that 
techniques that automate the identification of text relevant to a task can be built 
using our corpus.  This step requires processing an artifact's content 
into a sequence of individual sentences.
Given a search result \texttt{URL}, we use \texttt{BeautifulSoup}~\cite{beautifulsoup4},
\texttt{StackAPI}~\cite{StackAPI} and \texttt{PyGithub}~\cite{PyGithub}
to fetch the artifacts' content
and the Stanford CoreNLP toolkit~\cite{CoreNLP} for the identification of 
individual sentences,
which are common procedures for processing the artifact types found in our corpus~\cite{Arya2019, nadi2020}.










% Note that 
%  nature of reading, understanding, and annotating each artifact
%  is time-consuming.
% For example, a quick inspection of a random task in our corpus shows that, 
% on top of the time a developer would take to , 
% a developer would take and average of 18 minutes to read 
% all of the artifacts' content~\cite{Just1980},
% a value similar to the average work related browsing 
% of a developer's work day~\cite{Meyer2017}.





% --- a limitation necessary due to throttling or even blocking mechanisms in the APIs used to get the content of each source considered.
% \gm{Isn't the real reason because you
% don't want more?}
% \art{I think it's both. I don't want more and there's throttling/IP blocking}
% \gm{-the throttling argument is weak. You could just wait more time to search. I think this
% is about the human evaluation capability so suggest you rely on that part of the argument.}


% We restrict the manual identification of text relevant to a task to a random subset of 
% 50  out of the 300  tasks initially gathered (Section~\ref{cp4:corpus-tasks}).
% \gm{-Why select 300? Just select 25 and 25 originally.}

% creating golden data for the entirety of our tasks 
% would require asking human annotators to inspect thousands of artifacts and more than 260,000 sentences, which would be a costly and time-consuming activity. 


% This imperative process cannot be re- placed by existing natural language processing techniques, which are inca- pable of accurately determining the level of relatedness between highly tech- nical text (Cleland-Huang and Guo, 2014; Al Omran and Treude, 2017).
