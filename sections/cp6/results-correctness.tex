
\subsection{Tasks Correctness}
\label{cp6:correctness}



When assisted by a tool that automatically highlights text identified as relevant to a task, we expect that a developer can produce a solution 
that is equally or more correct than the solution of a developer who attempted a task without tool support. 


\subsubsection{Metrics}

To compute how correct a participant's solution is, 
we compile their submitted code and run it against a set of 10 test cases (not provided to participants) that check whether it produces the correct output for each given test input. 
Hence, \textit{correctness} represents the number of passing test cases of a solution (Equation~\ref{eq:cp6-correctness}).
For example, if the solution of a participant passes 7 out of 10 test cases, we would assign a 
correctness score of $7$ to this solution. 
A solution with compile errors has a correctness score of $0$.


\begin{small}
\begin{equation}
    Correctness = \text{\textit{\# of passing test cases}}
    \label{eq:cp6-correctness}
\end{equation}
\end{small}



\subsubsection{Data}

From all submitted solutions (24 manual and 24 tool-assisted ones), two 
solutions from tool-assisted tasks had compile errors or failed all test cases. 
One of these
was from a participant who indicated that they decided to not finish their tool-assisted task due to time constraints; 
the other, from an exception thrown in the code of a participant who misused the \texttt{geopy} module. We do not ignore these two solutions when reporting results.



\subsubsection{Results}


Figure~\ref{fig:correctness-overall} aggregates all correctness scores for tasks done with and without tool support.
We compare correctness scores first using a Shapiro-Wilk test~\cite{wohlin2012} to check for normality and then, 
due to deviation from a normal distribution ($\text{\textit{p-value}} < 0.05$), applying a 
Wilcoxon-Mann-Whitney test~\cite{mannWhitneyU}.
Results from this test indicate that we cannot draw statistically significant conclusions
for the solution scores of the two groups ($\text{\textit{p-value}} \ge 0.10$)\footnote{
As Section~\ref{cp6:threats} details, the lack of statistical power is not surprising and is a common risk to between groups comparisons~\cite{Lazar2017}.}. Hence, we evaluate scores on a per-task basis, as shown in Figure~\ref{fig:correctness-by-task}.




\medskip
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{cp6/correctness_aggregated.pdf}
    \caption{Boxplots showing aggregated correctness scores for task performed with and without tool support}
    \label{fig:correctness-overall}
\end{figure}




Comparing the correctness scores for each individual task, we observe that participants with tool support performed worse in the \texttt{distances} task. 
Among potential reasons for the lower score, we believe that the text automatically identified 
might not have been relevant for this task, what can explain the higher percentage of participants
who indicated that the text identified by the tool was not helpful (Section~\ref{cp6:usefulness}).



The two groups have the same correctness scores in the \texttt{NYTimes} tasks.
We found that the most notable differences in the solutions of this task arise from participants who did (or not) consider corner-case scenarios.
To illustrate this, we quote one participant who stated that their solution could ``\textit{fetch all the articles, but it could also get some noise}'',
e.g., getting all the headlines of the web page but also an unrelated ad, which is indeed one of the test cases for this task. 



The \texttt{titanic} task required using two data science modules and it is the one with the most differences. 
Participants who did the task with tool support were able to produce more correct solutions, with an average 
correctness score of $6$. In contrast, participants in the control group had an average correctness of $4$. 
As we detail in the next sections, we found that the tool identified much of the text that the participants deemed relevant
and that participants also indicated that
the text automatically identified was indeed useful.





\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{cp6/correctness_overall.pdf}
    \caption{Boxplots showing correctness scores for each task performed with and without tool support}
    \label{fig:correctness-by-task}
\end{figure}





