\clearpage

\subsubsection{Brainstorm - A}

% \begin{small}
\begin{description}[leftmargin=\parindent, font=\normalfont\itshape]

    
    \item[procedures:] given a software task from a Stack Overflow post or a GitHub issue, write a short plan with instructions to solve the task

    
    \item[follow-up:] given user comments extracted from the Stack Overflow post or the GitHub issue, evaluate their correctness (whether the comment is correct or not)

    \item[tasks:] 5 random SO tasks and 5 random github tasks from Android
    
    \item[participants:] experienced developers
    
    \item[design:] between subjects. Independent variable is presence (or not) of the technique that highlights text that might assist in the task
    
    \item[measurement-1:] Likert scale on participant's confidence about the correctness of their answer
    
    \item[measurement-2:] If the assessment provided by a participant for each statement matches the ones of an oracle. To create the oracle:
    
        -- True statements are picked from the comment history on SO or GitHub
        
        -- False statements are created by picking a comment and modifying it
        
    \item[tooling:] fixed set of artifacts associated with each task. Highlights in each artifact are cached
    
    \item[pros:] easier to run online

        -- might be doable on MTurk, which increases participant pool

    \item[cons:]  if tasks are just to evaluate truthness, participants are more likely to just search for the bits that answer each statement

        -- participants do not get to perform the software task itself
\end{description}
% \end{small}