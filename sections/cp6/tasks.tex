
\section{Tasks}
\label{cp6:tasks}


We opted for an experiment with tasks that could be completed by participants on their own time and computer.
This decision was motived by the COVID-19 pandemic and challenges related to recruiting participants and conducting an in-person experiment safely\red{~\cite{}}. 
Since participants would only have written instructions to guide them, we decided to use tasks that are easy to understand and perform in a single experimental session, but that still required a participant  
to seek information in artifacts associated with that task.



Based on these criteria and guided by~\cite{thiselton2019},
Table~\ref{tbl:python-tasks-modules} details the tasks that we have selected. 
These tasks are based on 
Python w3resource\footnote{\url{https://www.w3resource.com/python-exercises/}} tasks
that require usage of at least one module external to the Python core library.
By using external modules, we aim to reduce the likelihood that a participant 
can provide a solution for a task without consulting any of the artifacts (Section~\ref{cp6:experiment-artifacts})
that detail how to use such modules. 



While we leave details about the procedures associated with manual and tool-assisted tasks to another section, Figure~\ref{fig:nytimes-task-github} provides an excerpt of the information shown in a task\footnote{Full descriptions are available in the experiment's supplementary material~\red{\cite{}}.}.
For each task, participants had the task description and examples of input and output scenarios at their disposal. A task contained a list of resources that participants could consult 
so that they could come up with a solution for the task.
Each task also contained a link to an online coding environment (Section~\ref{cp6:coding-environment})
where they could write and test their solution. 


\input{sections/cp6/tbl-task-descriptions.tex}


\subsection{Artifacts}
\label{cp6:experiment-artifacts}


Each task requires a set of artifacts that a participant could peruse for information that could assist them complete the task.
We select artifacts for a task following procedures similar to the ones we used to create the \acs{DS-android} dataset (Chapter~\ref{ch:android-corpus}). 
That is, for each of the tasks in Table~\ref{tbl:python-tasks-modules}, we use the Google search engine to obtain up to ten artifacts that likely contain 
information that could help a participant correctly complete that task. 


Note that our decision to control the artifacts shown per task relates to our need to study if there is overlap between the text that participants manually identify as relevant and the text that \acs{beskar} identifies for these same artifacts. 
Three pilot runs ensured that the information available in the curated list of artifact of each task was sufficient to complete the task. 


\subsection{Coding environment}
\label{cp6:coding-environment}



To ensure that participants had the same conditions to perform each task
and also to minimize setup instructions, we used Google Colab\footnote{\url{https://colab.research.google.com/}} as our coding environment. 


Colab provided participants with a code editor with amenities commonly found in modern IDEs, e.g., code completion and syntax highlighting. It also ensured that all the participants 
performed the tasks in the same Python version and it lifted 
burdens that could arise from installing dependencies associated with the external modules used in each of our tasks. 


Figure~\ref{fig:nytimes-task-colab} shows an example of the Colab coding environment. 
First it handled dependencies management and then, 
it presented a class containing a single method with a \texttt{TODO} block where 
participants should write their solution. 
The environment also provided a main function where participants could see the output
of their code. Alternatively, a participant could use test cases to test their solution
against the examples shown alongside the a description.



\clearpage

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{cp6/task-github.pdf}
    \caption{Information shown in a task}
    \label{fig:nytimes-task-github}
\end{figure}



\clearpage

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{cp6/task-colab.pdf}
    \caption{Colab environment}
    \label{fig:nytimes-task-colab}
\end{figure}



\clearpage




% We chose these tasks due to how they focus on modules widely adopted both by open source systems and by private companies.
% As an example, the \texttt{NYTimes} task involves using a HTML parser, namely \texttt{BeautifulSoup},
% which Reddit---a social news aggregator with approximately 430 million monthly users---uses 
% to parse urls and identify images shown on its posts' headlines~\cite{bs4-reddit}. 
% Similarly, the \texttt{Titanic} task encompasses using \texttt{Pandas}, a data analysis and manipulation module
% used in many open source data science projects~\cite{ma2017, shrestha2020}.