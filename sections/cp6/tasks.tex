
\section{Tasks}
\label{cp6:tasks}



We opted for an experimet with tasks that could be completed by participants on their own time and computer.
This decision was motived by the COVID-19 pandemic and the need for social distance~\cite{}. 
Therefore, we decided to use tasks that are easy to understand and perform in a single experimental session, but that still required a participant  
to seek information in artifacts associated with a task
so that they could come up with a solution for the task.




Based on these criteria and guided by~\cite{thiselton2019}, we select Python w3resource\footnote{\url{https://www.w3resource.com/python-exercises/}} tasks
that required usage of at least one module external to the Python core library as our experimental tasks.
By using an external module, we aim to reduce the likelihood that a participant 
can provide a solution for a task without consulting any of the artifacts accompanying that task. 






Table~\ref{tbl:python-tasks-modules} details the tasks that we have selected. 
We chose these tasks due to how they focus on modules widely adopted both by open source systems and by private companies.
For example, the \texttt{NYTimes} task involves using a HTML parser, namely \texttt{BeautifulSoup},
which Reddit---a social news aggregator with approximately 430 million monthly users---uses 
to parse urls and identify images shown on its posts' headlines~\cite{bs4-reddit}. 
Similarly, the \texttt{Titanic} task encompasses using \texttt{Pandas}, a data analysis and manipulation module
used in many open source data science projects~\cite{ma2017, shrestha2020}.



While we leave details about the experimental procedures associated with manual and tool-assisted tasks to Section~\ref{cp6:procedures}, Figure~\ref{fig:nytimes-task-colab} provides details of the information shown in a task.
For each task, participants had the task description and examples of input and output scenarios at their disposal. A task contained a list of resources (Section~\ref{cp6:experiment-artifacts}) that participants could consult
as well as a link to an online coding environment
where they could write and test their solution (Section~\ref{cp6:coding-environment}). 
\art{Add note to the full replication package}

\clearpage

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{cp6/task-github.pdf}
    \caption{Details about the information shown in a task}
    \label{fig:nytimes-task-colab}
\end{figure}

\clearpage

\input{sections/cp6/tbl-task-descriptions.tex}







\subsection{Artifacts}
\label{cp6:experiment-artifacts}


Each task requires a set of artifacts that a participant could peruse for information that could assist them complete the task.
We select artifacts for a task following procedures similar to the ones we used to create the \acs{DS-android} dataset (Chapter~\ref{ch:android-corpus}). 
For each of the tasks in Table~\ref{tbl:python-tasks-modules}, we use the Google search engine to obtain up to ten artifacts that likely contain relevant
information for that task. 









\subsection{Coding environment}
\label{cp6:coding-environment}






To ensure that participants had the same conditions to perform each task
and also to minimize setup instructions, we used Google Colab\footnote{\url{https://colab.research.google.com/}} as our coding environment. 



Figure~\ref{fig:nytimes-task-colab} shows an example of our online environment.
At the left-hand side, 
where they had a code editor with amenities commonly found in modern IDEs, such as code completion and syntax highlighting. 



Through Colab, a participant could compile their solution and test it against the examples shown alongside the task description.
Upon testing, the system would display full details about the test cases, e.g., the test's input, which assertion failed, and why. 




\clearpage

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{cp6/task-colab.pdf}
    \caption{Colab environment}
    \label{fig:nytimes-task-colab}
\end{figure}



\clearpage