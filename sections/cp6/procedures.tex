\clearpage

\section{Experimental Setup}
\label{cp6:procedures}



Our experiment's main hypothesis is that:


\medskip
\begin{bluequote}
    \textit{text automatically identified by a semantic-based technique assists a 
    developer complete her software development task.} 
\end{bluequote}



To test this hypothesis, we have designed an experiment where participants
 had to complete two programming tasks when assisted (or not) by a
 tool that embeds one of the best performing semantic-based techniques that we have explored previously ({Section~\ref{cp5:results-all}). 


We first evaluate how the text that a participant manually identifies as relevant (while she performs a task) 
compares to text that an automatic technique identifies. 
We then consider if a participant found the text automatically identified helpful, i.e., the text identified provides useful information needed to correctly complete her task.
Experimental setup is as follows.


\art{Add a figure to help summarizing the experimental setup?}






\subsection{Design}


We follow a \textit{within-subjects} design. Each participant is exposed to a control task and a tool assisted task. For each of these tasks, we provided to the participants a set of natural language artifacts related to the task
and we asked them to code a solution for the task. While tasks were randomly assigned to each participant, 
we ensured that the tasks were evenly distributed among control and tool assisted groups. 






\subsection{Participants}



We advertised our study to both developers in our professional network and to computer science students at the several universities. 
This provides for breadth of experience where both novice and expert developers attempt the tasks in our experiment. 


With regards to computer science students, we advertised the study only to third and fourth-year students to ensure that they had the necessary background required to perform the study's taks.
At this point in the curriculum, they should be familiar with Python and they should be able to come up with a solution 
for a software task when provided with artifacts containing information for that task.


To compensate participants for their time, we offered them the opportunity to enter a raffle for one of two iPads 64 GB.




\subsection{Tasks}
\label{sec:experiment-tasks}

Our experiment requires programming tasks for which a developer will likely benefit from the use of additional information to complete. We also need to ensure that the tasks are self-contained enough so that they can be finished in a single experimental session lasting approximately 2 hours.


These criteria lead us to 
follow Thiselton and Treude's task selection procedures~\cite{thiselton2019}, 
where we selected tasks from the Python w3resource website\footnote{\url{https://www.w3resource.com/python-exercises/}}.
We focused on tasks that required usage of at least one module external to the Python core modules.
In turn, we required that these modules were widely used both by open source systems and by private companies.
For example, one of our selected tasks involve using \texttt{BeautifulSoup},
which Reddit---a social news aggregator with approximately 430 million monthly users---uses 
to parse urls and identify images shown on its posts' headlines~\cite{bs4-reddit}. 





Table~\ref{tbl:python-tasks-modules} details the tasks and modules that we have selected. 

\art{add short description for each task}




% \footnote{\url{https://www.crummy.com/software/BeautifulSoup/}} 





\input{sections/cp6/tbl-task-descriptions.tex}


\clearpage




\subsection{Artifacts}
\label{sec:experiment-artifacts}


Each task requires a set of artifacts that a participant could peruse to locate information that could assist them complete the task.
We select artifacts for a task following procedures similar to the ones we used to create the \acs{DS-android} dataset (Chapter~\ref{ch:android-corpus}). 
For each of the tasks in Table~\ref{tbl:python-tasks-modules}, we use the Google search engine to find a total of ten artifacts that likely contain relevant
information for that task in top-ranked development websites. 






\subsection{Procedures}
\label{cp6:evaluation-procedures}



The experiment was completely online and it could be performed at a participant's personal computer
without the presence of one of the researchers. Each participant had to complete a practice task and two tasks, i.e., a control and a tool assisted task, drawn from the tasks presented in Table~\ref{tbl:python-tasks-modules}. 


Each experimental session lasted no more than two hours; this length of time was selected based on two pilot sessions. 
Feedback from these pilots also helped to refine the experiment's instructions, where we included a short video showing how to install the web browser plugin and how to use Colab, the online environment where participants performed each task (Section~\ref{cp6:environment}).






We began each session gathering consent and requesting participants to install a web browser plugin which we used to gather data.
Setup was followed by a short tutorial explaining the experiment and describing how to use the plugin and Colab. 
A practice task---separated from the experimental tasks---allowed participants to familiarize themselves with the content of a task, the web browser plugin and Colab. 



For each task, including the practice tasks, we asked a participant to write a solution for the task
when provided with a fixed set of artifacts, e.g., official API documentation, Stack Overflow posts, or web tutorials, 
associated with that task.  


In the control task, a participant was instructed to use the web browser plugin to highlight any text that they deemed relevant to the task-at-hand. 
In the tool assisted task, the plugin automatically highlighted sentences that our semantic-based technique identified as relevant for the task. 
For this second task, we had one extra step asking the participant to rate---using a Likert scale---how helpful were the highlights automatically identified per artifact available. 


We concluded the study by asking participants if they would like to provide any additional information and 
by giving them the opportunity to join the raffle, if so they wished. 



% \subsubsection{Summary of procedures}

% Based on our experimental procedures, we could gather:



% \begin{enumerate}
%     \item a participant's submitted solution (written Python code) for each task;
%     \item a participant's highlights for the control task; 
%     \item a participant's perception on the usefulness of the automatically identified highlights for the tool assisted task, and;
%     \item any additional feedback (written text) that a participant wished to provide us.
% \end{enumerate}


Table~\ref{tbl:experiment-data} summarizes the data we collected based on experimental procedures.

\input{sections/cp6/tbl-summary-of-procedures.tex}


\subsection{Colab}
\label{cp6:environment}


To ensure that participants had the same conditions to perform each task, we used Google Colab\footnote{\url{https://colab.research.google.com/}} as our coding environment. 
By using Colab, we also expect setup instructions to be minimal 
so that we allow a participant to focus on the task and experimental procedures.



Figure~\ref{fig:nytimes-task-colab} shows an example of our online environment for the \texttt{NYTimes} task.
At the left-hand side, participants had the task description and examples of input and output scenarios as well as a list of resources associated with that task. 
By clicking on the coding environment link, participants were redirected to Colab (right-hand side),
where they had a code editor with amenities commonly found in modern IDEs, such as code completion and syntax highlighting. 



Through Colab, a participant could compile their solution and test it against the examples shown alongside the task description.
Upon testing, the system would display full details about the test cases, e.g., the test's input, which assertion failed, and why. 




\clearpage

\begin{landscape}
\begin{figure}
    \centering
    \includegraphics[width=1.5\textwidth]{cp6/task-colab.pdf}
    \caption{NYTimes task and Colab environment.}
    \label{fig:nytimes-task-colab}
\end{figure}
\end{landscape}

\clearpage


\subsection{Analysis}




We use the gathered data to investigate our main hypotheses according to the following metrics.



\subsubsection{Submitted Solution}

 
The correctness of a submitted solution is measured by the number of passing test cases
when running each participant's solution against a set of 10 test cases. 
A solution with compile errors has a correctness score of zero.


\smallskip
\begin{small}


\begin{equation}
    Correctness = \frac{ \text{\textit{\# of passing test cases}}}{\text{\textit{\#  of test cases}}}
\end{equation}
\end{small}

% \smallskip
% According to this definition, 


\subsubsection{Manual Highlights}


We compare the participants' manual highlights  against the ones automatically identified by our semantic-based technique measuring the amount of overlap between the two. 
For that, we use \textit{precision} and \textit{recall} metrics. 




Precision measures the fraction of the automatically identified text that was  considered relevant
by the participants.

\smallskip
\begin{small}


\begin{equation}
    Precision = \frac{
        \text{\textit{automatic highlights}~} \cap 
        \text{~\textit{manual highlights}}
    }{\text{\textit{automatic highlights}}}
\end{equation}
\end{small}


Recall represents how many of all the manual highlights were identified by the semantic-based technique that we applied.

\smallskip
\begin{small}

\begin{equation}
    Recall = \frac{
        \text{\textit{automatic highlights}~} \cap 
        \text{~\textit{manual highlights}}
    }{\text{\textit{manual highlights}}}
\end{equation}
\end{small}

\medskip
\art{Argue which metric matters the most here}


\art{Discuss how to account for variability in what the participants highlighted}


\subsubsection{Usefullness of the Automatic Highlights}


We use a diverging stacked bar chart~\cite{Heiberger2014} to analyze the  Likert scale
responses on the usefulness of the text automatically identified.


Usefulness indicates the percentage of responses agreeing or disagreeing with whether sentences
automatically highlighted assisted a participant in completing a task.


\smallskip
\begin{small}

\begin{equation}
Usefullness = \frac{
    \text{\textit{\# of responses at i}}
}{
    \text{\textit{total \# of responses}}
}
\end{equation}
        

\begin{equation*}
i \in \{ 
    \text{\textit{
        (strongly) agree, neither agree or disagree, (strongly) disagree
    }}  
\}
\end{equation*}
\end{small}

% This analysis complements the quantitative comparison of manual and automatic highlights.
% By asking participants to reflect on the usefulness of the text automatically identified, 
% we seek to minimize risks related to how a participant might have missed highlighting
% text that assisted them complete a task~\cite{easterbrook2008}.





\subsubsection{Written Feedback}

 
We use qualitative methods to analyze participants' responses to the open-ended questions. 

\art{Think this through...}




\clearpage

