
\section{Procedures}
\label{cp6:procedures}





In this section, we detail the instructions provided to the participants so that 
they could perform our experiment. 
First, we present an overview of the whole experiment and then, we detail
instructions specific to the manual and tool-assisted tasks, respectively.



We advertised the experiment via mailing lists. The email disclosed the purpose of the experiment, how long we estimated that it would take to complete it, as well as a link 
to the web form containing the experiment's consent and tasks. 




Once a participant consented to participate, we provided them with further instructions 
about how to perform each task and we requested them to install \acs{beskar}.
Setup was followed by a short practice---separate from the experimental tasks---task allowed participants to familiarize themselves with the content of a task, the tool and coding environment that we used (Colab). 



All participants had to complete the practice and then, attempt two randomly assigned tasks, i.e., a \textit{manual} and a \textit{tool-assisted} tasks. While tasks were randomly assigned, we made sure that an even number of participants attempted a task with and without tool support.
For each task, including the practice tasks, we provided to the participants a task description, examples of inputs and outputs and we asked them to write a solution, in the form of written Python code, for the task. 
Participants were allowed to consult a fixed set of artifacts, e.g., official API documentation, Stack Overflow posts, or web tutorials, 
associated with each of their assigned task. 
Figure~\ref{fig:nytimes-task-github}
shows an example of the information available in a task.


Once participants submitted their solutions, we gathered demographics and offered them the opportunity to enter a raffle for one of two iPads 64 GB 
to compensate participants for their time.
The web form concluded thanking participants for their participation.



\subsection{Manual Task}



In the \textit{manual} task, we instrumented \acs{beskar}


a participant was instructed to use the web browser plugin to highlight any text that they deemed relevant to the task-at-hand. 
\gm{Do they highlight text as they
work or after the task?}

\subsection{Manual Task}

In the tool assisted task, the plugin automatically highlighted sentences that our semantic-based technique identified as relevant for the task. 
For this second task, we had one extra step asking the participant to rate---using a Likert scale---how helpful were the highlights automatically identified per artifact available. 
\gm{More detail about exactly
what participants were asked to
do and experimental materials
should be published or in Appendix
of thesis.}


We concluded the study by asking participants if they would like to provide any additional information and 
by giving them the opportunity to join the raffle, if so they wished. 



\subsection{Summary}



Table~\ref{tbl:experiment-data} summarizes the data we collected based on experimental procedures.


\input{sections/cp6/tbl-summary-of-procedures.tex}



% \clearpage


% \subsection{Analysis}






% \subsubsection{Submitted Solution}


% % \smallskip
% % According to this definition, 


% \subsubsection{Manual Highlights}



% \subsubsection{Usefulness of the Automatic Highlights}



% % This analysis complements the quantitative comparison of manual and automatic highlights.
% % By asking participants to reflect on the usefulness of the text automatically identified, 
% % we seek to minimize risks related to how a participant might have missed highlighting
% % text that assisted them complete a task~\cite{easterbrook2008}.






% \clearpage

