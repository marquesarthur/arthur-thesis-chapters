
\subsection{Comparison of the Task-relevant Text Identified}
\label{cp6:comparison}


To assist a developer correctly complete a task, a tool that
automatically identifies task-relevant text in an artifact pertinent to that task would ideally 
identify text that humans have also considered as relevant, which we gather as part of the procedures 
of the control task in our experiment. 



\paragraph{\textbf{Metrics.}}

To investigate the overlap between the participants' manual highlights and the 
the automatic highlights identified by the study tool we use \textit{precision}, \textit{recall}~\cite{manning2010IR}, and \textit{pyramid precision}~\cite{Nenkova2004}.
We compute these metrics for each artifact of each task and report their average.


For this analysis, we consider any text marked by any participant as relevant
and we examine whether 
the text automatically identified represents text that multiple participants deemed relevant
 via \textit{pyramid precision}~\cite{Lotufo2012}.


Precision measures the fraction of the text automatically identified  that participants also deemed relevant (Equation~\ref{eq:precision-cp6}). 

\smallskip
\begin{small}
\begin{equation}
    Precision = \frac{
        \text{\textit{automatic highlights}~} \cap 
        \text{~\textit{manual highlights}}
    }{\text{\textit{automatic highlights}}}
\label{eq:precision-cp6}    
\end{equation}
\end{small}


Recall represents how many of all the manual highlights were identified by the semantic-based technique that we applied (Equation~\ref{eq:recall-cp6}). 



\smallskip
\begin{small}
\begin{equation}
    Recall = \frac{
        \text{\textit{automatic highlights}~} \cap 
        \text{~\textit{manual highlights}}
    }{\text{\textit{manual highlights}}}
\label{eq:recall-cp6}    
\end{equation}
\end{small}

\medskip


\textit{Pyramid precision} compares the text automatically identified to an optimal output, i.e., one where---for the same number of sentences---we identify sentences selected by the most number of participants (Equation~\ref{eq:pyramid-precision-cp6}). The more we identify text that more participants indicated as relevant, the higher pyramid precision is.



\smallskip
\begin{small}
\begin{equation}
    \triangle Precision = \frac{
        weight(\text{\textit{automatic highlights}~})
    }{weight(\text{\textit{optimal highlights}})}
\label{eq:pyramid-precision-cp6}    
\end{equation}
\end{small}
    


To illustrate these metrics, consider an artifact with 4 sentences $\{s_1, s_2, s_3, s_4\}$ that have been selected by $\{2, 0, 1, 1\}$ participants, respectively.
% For an output identifying two sentences for this artifact, an optimal solution would identify sentences $\{s_1, s_3\}$. 
Table~\ref{tbl:metrics-example} shows precision, pyramid precision, and recall metrics in a scenario where we output sentences $\{s_2, s_3\}$ as relevant.
    


\input{sections/cp6/tbl-metrics-example}


Note that pyramid precision is usually equal or lower than precision. For example, $Precision(s_1, s_3) = Precision(s_3, s_4) = 1.0$, but 
results for pyramid precision differ, i.e., 
$\triangle Precision(s_1, s_3) = 1.0$ and $\triangle Precision(s_3, s_4) = 0.66$. This allows us to check if our tool 
identified the text deemed relevant by most of the participants who inspected an artifact.



\paragraph{\textbf{Data.}}

Participants who indicated what text was relevant to their assigned control task produced a total of 415 highlights with an average of 7 highlights (std $\pm 3$) per artifact inspected.
On average, this comprises 9\% of the content of each of the artifacts in our experiment. 


Some participants also selected code snippets as relevant to a task---a threat that we discuss in Section~\ref{cp6:threats}. 
Code snippets account for 30\% of the highlights produced, but we remove them from our analysis since our semantic-based approach 
operates on text only. For the textual highlights,
Krippendorf's alpha indicates good agreement of what text in an artifact participants deemed relevant (or not) ($\alpha = 0.68$)~\cite{Krippendorff1980, passonneau2006}.
We compare these manually produced highlights to the text automatically identified by our tool.



% \subsubsection{Results}


\paragraph{\textbf{Results.}}




Table~\ref{tbl:comparison-task-wise} summarizes the average of precision, pyramid precision, and recall metrics for each of the tasks in the experiment.
For the tasks in our study, we observe that precision scores range from 0.55 to 0.68 and that 
the lower pyramid precision scores, ranging from 0.55 to 0.57, suggest that our tool failed to identify some of the text that participants deemed the most relevant.


\input{sections/cp6/tbl-comparison-overall}

These metrics are not surprising and they corroborate 
the correctness scores detailed in Figure~\ref{fig:correctness-by-task}. For example, 
in the \texttt{distances} task, participants who had tool assistance  task had less correct solutions than participants in the control group.
This was the task with the lowest precision, recall and pyramid precision values. 
In contrast, the task were participants assisted by our tool obtained the best correctness scores, namely \texttt{titanic}, is the one with the best precision, recall and pyramid precision values.








Table~\ref{tbl:comparison-artifact-type-wise} details evaluation metrics artifact-type wise. 
Stack Overflow posts and API documentation have the highest precision scores. For these types of artifacts, pyramid precision indicates that the 
text automatically identified on Stack Overflow was the text that several participants deemed relevant. 
Miscellaneous web pages were the artifact type with the lowest scores. 

\art{I'm stuck here}


We discuss this and other differences in our analysis of the usefulness of the text 
automatically identified (Section~\ref{cp6:usefulness}).



\input{sections/cp6/tbl-comparison-per-artifact-type}





% \art{Discuss how to account for variability in what the participants highlighted}

