
\subsection{Comparison of the Task-relevant Text Identified}
\label{cp6:comparison}


To assist a developer correctly complete a task, a tool that
automatically identifies task-relevant text would ideally 
identify text that humans have also considered as relevant to that particular task. 
This lead us to compare the participants' manual highlights against the ones 
identified by semantic-based technique applied by the tool in our study.
 



Analogous to the evaluation procedures of Chapter~\ref{ch:identifying}, we use 
\textit{precision} and \textit{recall} metrics
to compare human and tool identified task-relevant text.
Precision measures the fraction of the automatically identified text of an artifact that was  considered relevant
by the participants (Equation~\ref{eq:precision-cp6}). 
In turn, recall represents how many of all the manual highlights of an artifact were identified by the semantic-based technique that we applied (Equation~\ref{eq:recall-cp6}). 

\smallskip
\begin{small}


\begin{equation}
    Precision = \frac{
        \text{\textit{automatic highlights}~} \cap 
        \text{~\textit{manual highlights}}
    }{\text{\textit{automatic highlights}}}
\label{eq:precision-cp6}    
\end{equation}
\end{small}


% \smallskip
\begin{small}
\begin{equation}
    Recall = \frac{
        \text{\textit{automatic highlights}~} \cap 
        \text{~\textit{manual highlights}}
    }{\text{\textit{manual highlights}}}
\label{eq:recall-cp6}    
\end{equation}
\end{small}

\medskip


We also compute \textit{pyramid precision} to examine whether the text automatically identified represents text that multiple participants deemed relevant to a certain task.
Pyramid precision weights the automatically identified text based on the number of participants who selected it as relevant and compares the result to a scenario with an optimal output, i.e., one where we identify sentences selected by the most number of participants.   
The more the tool identifies text that more participants indicated as relevant, the higher pyramid precision is.

As an example, consider an artifact with 3 sentences $\{s_1, s_2, s_3\}$ that have been selected by $\{2, 0, 1\}$ participants, respectively.
For an output identifying two sentences from this artifact, an optimal solution would return sentences $\{s_1, s_3\}$. 
Equation~\ref{eq:pyramid-precision} shows this and other pyramid precision scores for this example.



\begin{small}
\begin{equation}
\begin{split}
\triangle  Precision(s_2, s_3) = ( 0 + 1) \div 3 =  0.33 \\
\triangle  Precision(s_1, s_2) = ( 2 + 0) \div 3 =  0.66 \\
\triangle  Precision(s_1, s_3) =  ( 2 + 1) \div 3 =  1.00 \\
\label{eq:pyramid-precision}
\end{split}
\end{equation}
\end{small}





\subsubsection{Results}



In the manual tasks, participants produced a total of 415 highlights with an average of 7 highlights (std $\pm 3$) per artifact inspected.
On average, this comprises 9\% of the content of each of the artifacts in our experiment. 


Some participants also selected code snippets as relevant to a task---a threat that we discuss in Section~\ref{cp6:threats}. 
Code snippets account for 30\% of the highlights produced, but we remove them from our analysis since our semantic-based approach 
operates on text only. For the textual highlights,
Krippendorf's alpha indicates good agreement of what participants deemed relevant (or not) to each task and artifact ($\alpha = 0.68$)~\cite{Krippendorff1980, passonneau2006}
and we compare these manually produced highlights to the text automatically identified by our tool.



Table~\ref{tbl:comparison-task-wise} summarizes the average of precision, pyramid precision, and recall metrics for each of the tasks in the experiment.
Based on the overall results, we observe that precision scores range from 0.55 to 0.68 and that 
the lower pyramid precision scores, ranging from 0.55 to 0.57, suggest that our tool failed to identify some of the text that participants deemed the most relevant.


Results from table~\ref{tbl:comparison-task-wise} also corroborate 
the correctness scores detailed in Figure~\ref{fig:correctness-by-task}. For example, 
tool-assisted solutions for the \texttt{distances} task were less correct than manual ones and 
we observe that this was the task with the lowest precision, recall and pyramid precision values. 
In contrast, the task were participants assisted by our tool obtained the best correctness scores, namely \texttt{titanic}, is the one with the best precision, recall and pyramid precision values.




\input{sections/cp6/tbl-comparison-overall}




Table~\ref{tbl:comparison-artifact-type-wise} details evaluation metrics artifact-type wise. 
Both Stack Overflow posts and API documentation have considerably higher precision scores than 
miscellaneous web pages. For these types of artifacts, pyramid precision indicates that the 
text automatically identified on Stack Overflow is the text that several participants deemed relevant. Miscellaneous web pages were the artifact type with the lowest scores and we 
discuss this and other differences on our analysis of the usefulness of the text 
automatically identified (Section~\ref{cp6:usefulness}).



\input{sections/cp6/tbl-comparison-per-artifact-type}





% \art{Discuss how to account for variability in what the participants highlighted}

