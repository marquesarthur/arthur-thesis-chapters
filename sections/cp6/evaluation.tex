\clearpage

\section{Evaluation}
\label{cp6:evaluation}



The goal of our evaluation is to investigate whether text automatically identified
as relevant to a task and shown to a developer assist them complete their software task.
This evaluation helps answer: 


\medskip
\begin{bluequote}
    \textit{ Do semantic-based techniques help a 
    developer more effectively complete a software development task?} 
\end{bluequote}




To answer this question, we report on a controlled experiment where participants 
had to complete three programming tasks when assisted (or not) by a
 tool that embeds such a semantic-based technique (\red{Section~\ref{}}). 
% ---bugs or feature requests obtained from popular \red{[programming language]} GitHub repos---


We gather metrics related to the text that participants in the experiment indicated as helpful to the task-at-hand, their agreement on the extent to which the text automatically identified by our tool 
assisted them, and how complete were their solutions, as measured by test cases derived from a task's original solution available online.




% For each task, we measure \red{effectiveness} using a test cases that 
% exercise a participant's submitted solution. 
% A \red{complete} solution must pass all test cases of a given task. 
% In turn, a \red{correct} solution must pass a subset of these test cases 
% which cover the core functionalities needed in a task's solution.).


% We use \textit{\red{TODO}} and \textit{\red{TODO}} metrics~\cite{manning2010IR} to measure \red{TODO}~\cite{Murphy2005}.




\clearpage

\subsection{Experimental Design}

\red{TODO}

\subsubsection{Participants}



We advertised our study to third and fourth year Computer Science students at the University of British Columbia. 
At this point in the curriculum, students should be familiar with \red{Java} and they should be able to come up with a solution 
for a software task when provided with artifacts containing information for that task.


Students were offered the opportunity to enter a raffle for one of two \red{\$50} online shopping gift cards.
We obtained answers from a total of \red{\#} students. \red{Demographics}




% We manually inspected each of the pull requests merged to the master branch in these repos and selected issues that had changes in a single file and that are related to different components core to the Android API.




% tasks from the Practice Python and 
% w3resource web 
% sites\footnote{\url{https://www.practicepython.org/} and \url{https://www.w3resource.com/python-exercises/}}. 



\subsubsection{Tasks}

Due to our target population, we scope tasks in our experiment to three 
feature requests obtained from popular Python GitHub repos,
which encompass a social network crawler (\textit{Sherlock}),
a web-archiving tool (\textit{ArchiveBox}), and a \red{last project}.








We manually inspected each of the feature requests available in these repos identifying
feature requests that
\textit{(1)}  required understanding interactions between different and well-known Python modules, 
\textit{(2)}  had pull requests merged to the repo's main branch, and that
\textit{(3)}  had medium or small size changes (\red{$\le 100$ LOC}).


These criteria are based on the fact that we need tasks that are easy to understand and that are enough self-contained so that a student can come up with a solution to them in the amount of time alloted in our experimental procedures (Section~\ref{cp6:evaluation-procedures}).
According to these requirements, we selected the following feature requests:


\begin{itemize}
    \item \texttt{BeautifulSoup}: use BeautifulSoup4 to parse Google chrome bookmark dumps\footnote{https://github.com/ArchiveBox/ArchiveBox/pull/20}
    \item  \texttt{Threads}: use multiprocessing and subprocess modules to find all the files that contain some value\footnote{https://github.com/sherlock-project/sherlock/pull/3}
    \item \red{find a third example/task}
\end{itemize}








% ~\cite{Thiselton2019}



These tasks expect that a participant understands and integrates modules and method calls 
in a logic flow and thus, it is unlikely that a student will find a direct solution to the task in one of the artifacts perused for that task. 
Hence, these tasks align with a scenario where a tool could assist a developer inspecting natural language artifacts and trying to find information relevant to the task-at-hand. 



% We start corpus creation by identifying software tasks 
% for which a developer will likely benefit from 
% the use of  to complete.
% We scope task selection to 
% because the 
% Android \acf{SDK} evolves constantly due to 
% functionality, security and performance-related improvements~\cite{Li2018android, Mateus2020}.
% These improvements impact its development community, requiring them to often  seek information regarding changes in the SDK~\cite{linares2014, bavota2014b, mcdonnell2013}.
% For example, over 35,000 developers have used Q\&A forums to discuss tasks covering 87\% of the classes in the Android API~\cite{parnin2012}.


% Two common places where Android task can be found are:

% 3 programming tasks using well-known python core modules:

% - pandas

% - strings and regular expressions

% - date time manipulation


% We design each task in such a way that its solution is not obvious. 
% That is, a participant cannot simply copy a code snippet from some API documentation, Stack Overflow post, or blog post to solve the task. Rather, we expect 
% that the solution for a task integrates distinct method calls in a logic way...


\subsubsection{Procedures}
\label{cp6:evaluation-procedures}

Online. Using an online judge-like system


\subsubsection{Design}


Within subjcts OR between subjects



\subsubsection{Data Analysis}


Within subjcts OR between subjects



\clearpage



\smallskip
\paragraph{\textbf{Current problems:}}

\begin{itemize}
    \item Installing and running the Android SDK is not trivial
    \item The Android emulators consume too much memory and it may be difficult to perform the tasks in certain environments
    \item Most of the tasks are visual and they don't have easy tasks cases that I can use to provide feedback to the students
\end{itemize}



\subsubsection{Tasks}

We use bugs or feature requests obtained from three popular \red{Android} GitHub repos (i.e., \textit{Omni-Notes}, \textit{K-9}, and \textit{AntennaPod}) as the tasks of our experiment. 
We manually inspected each of the pull requests merged to the master branch in these repos and selected issues that had changes in a single file and that are related to different components core to the Android API.
This manual inspection lead us to select the following tasks:
\footnote{
\textcolor{steelblue}{{\textit{AM:} one challenge with Android tasks is that, while more representative, they are harder to debug and test in a self-contained environment.
A potential alternative are Python tasks that require using well-known modules. https://www.practicepython.org/ has examples for \texttt{beautifulsoup}}}}.

% ---\textit{Omni-Notes}, \textit{K-9}, and \textit{AntennaPod}---



\begin{itemize}
    \item \texttt{Notifications}: notify users about new emails in their inbox;
    \item \texttt{Permissions}: request user permissions to and external storage; and
    \item \texttt{Storage}: show size of all the data used by the application.
\end{itemize}


While these tasks are small enough so that a student can come up with a solution in a single programming session,
they require understanding interactions between different API components 
and likely require the students to seek information in the Android API documentation or in community forums. 
Therefore, we deem the tasks as appropriate to a scenario where our tool could assist a student trying to find information relevant to these tasks in a set of natural language artifacts. 


\subsubsection{Android Application}


Ideally, all tasks could have been selected from the same open-source project. However, our requirements on the number of classes or methods in a task's solution as well as the need that the tasks relate to core Android API components made it difficult 
to identify a single project with such self-contained tasks. Since asking a student to familiarize themselves with running, debugging and modifying three different and complex Android applications 
poses challenge to experimental procedures, we decided to use the 2019 Google I/O Android demo application as the core project in which the selected tasks must be implemented. 
% \footnote{\url{https://github.com/marquesarthur/iosched}}


The I/O Android app displays a list of conference events---sessions, office hours, app reviews, codelabs, etc.---and it lets a user filter these events by event types and by topics.
This project has \red{[statistics about number of classes, lines, etc.]} and we believe that we can transpose the tasks that we have selected to this Android app without loss of context or complexity. 
That is, to perform each of the selected tasks, a developer still has to make changes to a single file and they still need to consult supporting material to identify information that guides them towards 
a solution.






% \subsubsection{Computing Task Correctness}



% We compute values for \textit{precision} and \textit{recall} metrics based on the sentences 
% deemed relevant to a task by \textit{at least two} human annotators.
% To minimize risks from a scenario where one of the techniques and their underlying approaches has a peak or
% bottom performance due to training procedures or due to factors beyond our control, 
% we compute results for each technique over 10 distinct executions over the test data, 
% reporting the average of each metric over all runs.



% We compute values for \textit{precision} and \textit{recall} metrics based on the sentences 
% deemed relevant to a task by \textit{at least two} human annotators.
% To minimize risks from a scenario where one of the techniques and their underlying approaches has a peak or
% bottom performance due to training procedures or due to factors beyond our control, 
% we compute results for each technique over 10 distinct executions over the test data, 
% reporting the average of each metric over all runs.


% For a detailed definition of each metric, we refer to the evaluation outcomes in Table~\ref{tbl:type-I-II-errors}, where  columns represent  labels provided by the annotators and rows,
% the text identified as relevant or not by a technique.

% % 

% \medskip
% \input{sections/cp5/tbl-eval-outcomes}



% \paragraph{\textbf{Precision}}

% Precision measures the fraction of the sentences identified that are relevant over the total number of target sentences identified, as shown in Equation~\ref{eq:cp5:precision}.



% \begin{equation}
% \label{eq:cp5:precision}    
%     Precision = \frac{TP}{TP + FP}
% \end{equation}


% \subsubsection{Computing Task Completeness}


% \medskip
% Precision means identifying only text that is relevant, whereas recall means identifying all relevant text.
% Ideally, we would aim for a technique with high precision and high recall. Unfortunately, this is often not possible and we must reach a compromise.
% As described earlier, our goal is to support developers to locate text that might be relevant to their task, 
% and not locating all the relevant text may lead to incomplete or incorrect solutions, thus the reason why we favour recall.






% \subsection{Results}
% \label{cp6:analytic}





% \red{TODO}




% Table~\ref{tbl:techniques-results-answerbot} shows values of precision and recall for  \texttt{AnsBot} and the semantic techniques that we explore. 
% In the table, rows provide details about a specific technique while columns discriminate 
% precision and recall values and which post-processing filters were applied. 
% We also mark results from paired Wilcoxon-Mann-Whitney tests~\cite{mannWhitneyU} 
% that check if the results between each technique and \texttt{AnsBot} are statistically different.




% Results from our comparison to a state-of-the-art approach, namely AnswerBot, 
% indicate that text-based semantic techniques achieve comparable accuracy  
% when identifying relevant information in Stack Overflow artifacts.





% \subsection{Qualitative Evaluation}
% \label{cp6:qualitative}





% \red{TODO}




% Table~\ref{tbl:techniques-results-answerbot} shows values of precision and recall for  \texttt{AnsBot} and the semantic techniques that we explore. 
% In the table, rows provide details about a specific technique while columns discriminate 
% precision and recall values and which post-processing filters were applied. 
% We also mark results from paired Wilcoxon-Mann-Whitney tests~\cite{mannWhitneyU} 
% that check if the results between each technique and \texttt{AnsBot} are statistically different.




% Results from our comparison to a state-of-the-art approach, namely AnswerBot, 
% indicate that text-based semantic techniques achieve comparable accuracy  
% when identifying relevant information in Stack Overflow artifacts.


