
\section{Design}
\label{cp6:design}



To assess how a tool that shows text automatically identified as relevant to a task
might assist a developer complete a task, we are guided by three driving factors:


First, we consider whether---in an artifact pertinent to a task---the  text automatically identified by such a tool is similar to the text that humans deemed relevant to a task at hand. 
In case of overlap, one could argue that such a tool  
would direct a developer's attention to information that others found useful for completing a task.


If the text identified is not similar, there is a chance that the tool directed a developer to the wrong information, or that 
the tool actually found complementary information (not identified by humans) that it is still helpful to a developer working on a particular task. 
Therefore, it would be beneficial to not only compare the amount of overlap between the text manually
and automatically identified, but also to evaluate if developers performing a task with and without such a tool would 
both reach equally correct solutions.


As a final factor, this quantitative analysis could be complemented by qualitative feedback about the tool. 
This feedback could be used to identify other means through which we could locate 
text helpful to a developer working on a task, or perhaps to explore
situations in which the text automatically identified 
was more (or less) helpful.



The experiment that we describe in Figure~\ref{fig:tool-experiment-procedures} is designed around these driving factors.  
Each participant attempted two randomly assigned Python programming tasks on their own time and computer\footnote{Literature define this as an \textit{offline} experiment~\cite{wohlin2012, DeLucia2012}}.
In a first task, i.e., \textit{manual task}, a participant indicated 
what text they deemed relevant to the task at hand while 
they consulted a curated list of artifacts that could assist them in writing a solution for that task.
In a second task, i.e., \textit{tool-assisted task},
participants wrote their solution consulting a similar list of curated artifacts, but
assisted by a tool that highlighted the text identified as potentially relevant to their task; for this task, we also gathered feedback on the usefulness of the highlights shown.

 

With this experiment, we evaluate different factors that can help us answer whether 
a tool embedding a semantic-based technique helps developers complete a software task. 
The following sections elaborate our experimental design. 

% The following sections further elaborate such experimental design.

% As we further elaborate in the following sections, 
% this design allows us to investigate the driving factors that we have just outlined.






% To assess how a tool that shows text automatically identified as relevant to a task
% might assist a developer complete a task, we consider whether---in an artifact pertinent to a task---the  text automatically identified by such a tool is similar to the text that humans deemed relevant to a task at hand. 
% In case of overlap, one could argue that such a tool  
% would direct a developer's attention to information that others found useful for completing a task.


% If the text identified by such a tool is not similar, there is a chance that the tool directed a developer to the wrong information, or that 
% the tool actually found complementary information that was still helpful to a developer working on a particular task. 
% Therefore, it would be beneficial to not only compare the amount of overlap between the text manually
% and automatically identified, but also to evaluate if developers performing a task with and without such a tool would 
% both reach an equally correct solution.


% In turn, this quantitative analysis could be complemented by qualitative feedback about the tool. 
% This feedback could be used to identify other means through which we could locate 
% text helpful to a developer working on a task, or perhaps to explore
% situations in which the text automatically identified 
% was more (or less) helpful.



% The experiment that we describe in Figure~\ref{fig:tool-experiment-procedures} addresses these needs. 
% On their own time and computer\footnote{Literature define this as an \textit{offline} experiment~\cite{wohlin2012, DeLucia2012} }, 
% each participant attempted two randomly assigned Python programming tasks.
% In a first task, i.e., \textit{manual task}, a participant indicated 
% what text they deemed relevant to the task at hand while they worked on that task.
% In a second task, i.e., \textit{tool-assisted task},
%  they were assisted by a tool that highlighted the text identified by 
% the semantic-based approach as potentially relevant to their task,
% and we collected feedback on the usefulness of the highlights shown. 


% As we further elaborate in the following sections, 
% this design allows us to investigate the driving factors that we have just outlined.