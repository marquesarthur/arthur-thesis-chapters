
\section{Design}
\label{cp6:design}



To assess how a tool (Section~\ref{cp6:tool}) that shows text automatically identified as relevant to a task
might assist a developer complete a task, we are guided by three driving factors:


First, we consider whether---in an artifact pertinent to a task---the  text automatically identified by such a tool is similar to the text that humans deemed relevant to a task at hand. 
If the manually and automatically identified text are similar, one could argue that such a tool  
would direct a developer's attention to information that others found useful for completing a task.


If the text identified is not similar, there is a chance that the tool directed a developer to the wrong information, or that 
the tool actually found complementary information (not identified by humans) that it is still helpful to a developer working on a particular task. 
Therefore, it would be beneficial to not only compare the amount of overlap between the text manually
and automatically identified, but also to evaluate if developers performing a task with and without such a tool  
both reached equally correct solutions.


As a final factor, this quantitative analysis could be complemented by qualitative feedback about the tool. 
This feedback could be used to identify other means through which we could locate 
text helpful to a developer working on a task, or perhaps to explore
situations in which the text automatically identified 
was more (or less) helpful.



The experiment that we describe in Figure~\ref{fig:tool-experiment-procedures} is designed around these driving factors.  
Each participant attempted two randomly assigned Python programming tasks on their own time and computer\footnote{Literature define this as an \textit{offline} experiment~\cite{wohlin2012, DeLucia2012}}.
In a first task, i.e., \textit{manual task}, a participant indicated 
what text they deemed relevant to the task at hand while 
they consulted a curated list of artifacts that could assist them in writing a solution for that task.
In a second task, i.e., \textit{tool-assisted task},
participants wrote their solution consulting a similar list of curated artifacts, but
assisted by a tool that highlighted the text identified as potentially relevant to their task; for this task, we also gathered feedback on the usefulness of the highlights shown.

 

With this experiment, further detailed in the next sections, we evaluate different factors that can help us answer whether 
a tool embedding a semantic-based technique helps developers complete a software task. 
