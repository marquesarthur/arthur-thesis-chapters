


\section{Method}
\label{cp6:method}


To design an experiment
that examines how a tool that embeds a semantic-based technique can affect a developer's work,
we consider three main factors that might impact experimental design.




\paragraph{\textbf{Factors}}



To be helpful, a tool must direct a developer's attention to text that assist them complete a task~\cite{Robillard2015}. If we can gather data about the text in an artifact considered by humans as relevant to the task at hand, we could assess whether the text automatically identified by the tool is \textit{similar} to the text manually identified. 


In case the text is not similar,
there is a chance that the tool directed a developer to the wrong information, or that 
the tool actually found complementary information that humans missed, which might impact the \textit{correctness} of a task. Therefore, it would be beneficial to compare the correctness of solutions submitted by participants who perform a task using our semantic-based tool against that of developers who attempted the same tasks without tool support, i.e., a control group.


Regardless of how correct participants' solutions are, there is a chance that the text shown by the tool is not \textit{useful}---either because it is not relevant for the task at hand or because it is unsurprising, i.e., 
a developer finds the text identified by the tool as ``common-knowledge''~\cite{cwalina2008, Robillard2015}. Hence, experimental design must take into account qualitative factors 
related to the tool that are not captured by the previous factors. For example, the \textit{usefulness} of the text identified by the tool. 




\paragraph{\textbf{Design}}




Figure~\ref{fig:tool-experiment-procedures} presents an experiment designed considering these driving factors. In this experiment, 24 participants with software development background each attempted a
\textit{manual} and \textit{tool-assisted} task randomly drawn from a list of well-known Python programming tasks.
Alongside the solution submitted for each task, we use the manual task to collect what text a participant deems relevant to the task that they perfom.
In turn, in the tool-assisted task, we gather input on the usefulness of the text automatically identified and shown by our tool. 
This design allow us to:




\begin{itemize}
    \item assess the correctness of the tasks performed \textit{with} or \textit{without} tool support;
    \item compare  the text that participants deemed relevant to the text automatically identified
    and shown in the tool-assisted task, and;
    \item discuss the usefulness of the automatically identified text according to the feedback provided by the participants.
\end{itemize}
 





\begin{figure}
\centering
\includegraphics[width=1.05\textwidth]{cp6/tool-experiment-pipeline.pdf}
\caption{Summary of experimental procedures}
\label{fig:tool-experiment-procedures}
\end{figure}


The next subsections futher detail our experimental design.



\subsection{Programming Tasks}
\label{cp6:tasks}


We opted for an experiment with tasks that could be completed by participants on their own time and computer.
This decision was motived by the COVID-19 pandemic and challenges related to recruiting participants and conducting an in-person experiment \red{~\cite{}}. 
Since participants would follow instructions on their own, we decided to use tasks that are easy to understand and perform in a single experimental session, but that still required a participant  
to seek information in artifacts associated with that task.



Based on these criteria and guided by~\cite{thiselton2019},
Table~\ref{tbl:python-tasks-modules} details the tasks that we have selected. 
These tasks are similar to 
Python w3resource\footnote{\url{https://www.w3resource.com/python-exercises/}} tasks
that require usage of at least one module external to the Python core library.
By using external modules, we aim to reduce the likelihood that a participant 
can provide a solution for a task without consulting any of the artifacts (Section~\ref{cp6:coding-environment})
that detail each of the modules associated with each task. 



While we leave details about the procedures specific to the manual and tool-assisted tasks to another section, Figure~\ref{fig:nytimes-task-github} provides an excerpt of the information shown in a task\footnote{Full descriptions are available in the experiment's supplementary material~\red{\cite{}}.}.
For each task, participants had the task description and examples of input and output scenarios at their disposal. A task contained a list of resources that participants could consult 
so that they could write their solution.
Each task also contained a link to an online coding environment (Section~\ref{cp6:coding-environment})
where a participant could write and test their solution. 


\input{sections/cp6/tbl-task-descriptions.tex}


\subsection{Artifacts}
\label{cp6:experiment-artifacts}


% Note that our decision to control the artifacts shown per task relates to our need to study if there is overlap between the text that participants manually identify as relevant and the text that our semantic-based tool identifies for these same artifacts. 


Each task requires a set of artifacts that a participant could peruse for information that could assist them in writing their solution.
Ideally, participants could find these artifacts on their own. However, our need to compare solutions between participants who perform a task 
assisted by our tool and without it as well as our need to compare the text in an artifact a participant deems relevant to the text
automatically identified by our tool means that we must control the artifacts that  a
participant can consult.


Therefore,
we follow procedures similar to the ones we used to create the \acs{DS-android} dataset to produce the list of artifacts for each of the tasks in Table~\ref{tbl:python-tasks-modules}. 
That is, we use the Google search engine to obtain up to ten artifacts that likely contain 
information that could help a participant correctly complete that task. 
Three pilot runs ensured that the artifacts collected using such procedures had sufficient information to complete a task without 
the need of additional resources. 






\subsection{Coding environment}
\label{cp6:coding-environment}



To ensure that participants had the same conditions to perform each task
and also to minimize setup instructions, we used Google Colab\footnote{\url{https://colab.research.google.com/}} as our coding environment. 


Colab provided participants with a code editor with amenities commonly found in modern IDEs, e.g., code completion and syntax highlighting. It also ensured that all the participants 
performed the tasks in the same Python version and it lifted 
burdens that could arise from installing dependencies associated with the external modules used in each of our tasks. 


Figure~\ref{fig:nytimes-task-colab} shows an example of the Colab coding environment. 
First it handled dependencies management and then, 
it presented a class containing a single method with a \texttt{TODO} block where 
participants should write their solution. 
The environment also provided a main function where participants could see the output
of their code. Alternatively, a participant could use test cases to test their solution
against the examples shown in each task description.



\clearpage

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{cp6/task-github.pdf}
    \caption{Information shown in a task}
    \label{fig:nytimes-task-github}
\end{figure}



\clearpage

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{cp6/task-colab.pdf}
    \caption{Colab environment}
    \label{fig:nytimes-task-colab}
\end{figure}



\clearpage



\subsection{Participants}
\label{cp6:participants}


We advertised our study to professionals in our network and to computer science students at the several universities. 
Our target population comprised professionals and third, fourth-year or graduate students.
We expected participants to have experience in Object-Oriented programming languages, and to consult API documentation when performing a programming task.
We gathered a participant's background background information as part of our demographics (Figure~\ref{fig:experiment-demographics}), but no participants were excluded
based on their background.







We obtained twenty four responses (3 identified as female and 21 as male) to our study advertisement. 
At the time of the experiment, 10 participants were working as software
developers and 14 were students (11 graduate and 3 undergrad).
The majority of the students (71\%) also reported habing some previous professional experience.


On average, participants self-reported 8 years of programming experience ({\small $\pm$} 3.8, ranging from 3 to 17 years).
The majority of the participants (54\%) had between 5 to 10 years of experience in Object-Oriented programming languages,
closely followed by participants  with  3 to 4 years of experience (29\%). 
Most of the participants also indicated that they did check API documents almost every time they performed a programming task. 


\input{sections/cp6/tbl-demographics.tex}