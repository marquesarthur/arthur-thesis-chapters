\subsection{Relevant Text in Natural Language Artifacts}




To characterize the task-relevant text found in natural language 
software artifacts (\textit{RQ1}), 
we analyze the participants' produced highlights.
We focus
our analysis on highlights participants made of natural language
text. We do not
consider highlights participants made of source code snippets or in
tables, leaving the consideration of this information to future work.



We define a \textbf{H}ighlighted \textbf{U}nit (HU) as the full sentence containing any
highlight by a participant. 
We use sentences as the unit of analysis 
because this was the most common unit considered by the participants.
That is, of the 2,463
distinct highlights created by participants, 1,777 are sentences
(72\%), 621 are portions of sentences (25\%) and 65 are combinations
of consecutive sentences (3\%). 
Thus, if a participant highlighted just a
phrase in a sentence, we consider the full sentence as an HU or if a
participant highlighted more than one portion of a sentence we still
consider the full sentence as one HU. 


\subsubsection{How much text in an artifact is deemed relevant to a task?}
\label{cp3:ratio}




We first ask how much text within an artifact participants found relevant to a task
since if almost all of the text  is relevant then there would not be a need for
identifying text within an artifact.
Figure~\ref{fig:task-artifact-ratio} presents the ratio of relevant and non-relevant text in each type of document.
We compute the average ratio of HUs and text for each  artifact from each type of document.
We report the mean of the ratio artifact-wise to prevent misinterpretations due to outliers,
i.e., a lengthy document with few HUs or a document concentrating almost all the HUs






Considering all HUs, between 1\% to 20\% of an artifact's text is considered relevant, depending on its type.
API documents have the smallest ratio of
relevant information (mean of 3\%) and for this kind of artifact,
at most 6\% of all sentences were considered relevant.
This result is not surprising as API documents may describe many API features or
methods of which only a few are likely to apply to
a particular task~\cite{robillard2011field}.
% Such quantitative analysis provides further
% evidence supporting qualitative research on API
% documentation obstacles.
% Most notably, that the structure of APIs
% and boilerplate text can bloat the documentation~\cite{robillard2011field, Aghajani2019}.
Similarly, only a small portion (4\% on average) of
bug reports are considered relevant.
This result is also not
surprising as bug reports can contain long discussions encompassing
several topics~\cite{Breu2010, Rastkar2010}.
Even the kind of document with the highest
percentage of highlights, Q\&A entries with, on average, 11\%
of the sentences being relevant,
contain a substantial amount of information not relevant to a
specific task. 







\medskip
\begin{bluequote}
    \textit{Finding task-relevant information in a bug report,
    API document and Q\&A documents require filtering to less than
    a 20\% of the documents' text.}
\end{bluequote}



\begin{figure}
    \centering
    \includegraphics[width=.98\textwidth]{cp3/task-artifact-ratio}
    \caption{Percentage of highlighted sentences per artifact grouped by artifact's type}
    \label{fig:task-artifact-ratio}
\end{figure}

\subsubsection{How much agreement is there between participants about the text relevant to a task?}
\label{cp3:agreement}


With this question, we seek to identify if there is a set of key sentences that
participants unanimously consider relevant to a task. 
We use Nenkova and Passonneau's
pyramid method~\cite{Nenkova2004}
to investigate the degree to which participants agree upon
the text relevant to a task. This method was originally used
to quantify the content of summaries produced by different annotators.
The more annotators who include the same content in their summaries,
the more consistent the view of information relevant to a summary and
the more weight given the selected content. We apply the same rationale
to assess the level of agreement of which HUs are relevant to a task
and whether agreement relates to how correctly
a participant completes a task.




% To construct the pyramid, we assign a weight to each HU
% corresponding to the number of participants who highlighted that HU during
% a task.
% From the 602 HUs, we observe that 18\% of them are considered relevant by eight or more participants while 58\% are relevant to only one or two participants.
Figure~\ref{fig:highlights-distribution} shows the distribution of HUs over the pyramid we produce.
To facilitate interpreting the results,
we take into consideration how other studies provide categories for the relevance of
text (i.e.,~\cite{Petrosyan2015} and~\cite{Jiang2017}) and 
we aggregate HUs selected by a range of participants into three tiers.
Table~\ref{tbl:task-hu} presents HUs per tier.
Starting from the bottom tier, the pyramid represents the perceived relevance of information from less to more relevant.
With this information, we test if the number of HUs identified at each tier affects how correct were the solutions provided by the participants (as measured by the questionnaires applied for each task). 


Since have three independent variables (i.e., tiers) and one outcome variable (i.e., scores),
we use a multivariate analysis of variance to test this hypothesis~\cite{wohlin2012} 
Results indicate a significant differences ($p\text{-value} < 0.01$) between participants' score and their HUs.
We then conducted univariate tests on each tier,
finding that 
the number of HUs identified at the mid and top-tiers 
positively affect participants scores, 
what suggests that HUs from these two tiers contain key
information for completing a task.
As expected, HUs at the bottom tier vary more per participant.
Due to such variability, there is no clear indication
that bottom tier HUs are from participants from a certain group, such as all those
unfamiliar with a particular technology or  who successfully
completed a task and those who did not.






\begin{figure}
    \centering
    \includegraphics[width=.95\textwidth]{cp3/highlights-distribution}
    \caption{Distribution of the number of participants who deem an HU as relevant}
    \label{fig:highlights-distribution}
\end{figure}




\input{sections/cp3/tables/task-hu.tex}



\medskip
\begin{bluequote}
    \textit{Text perceived as relevant by more participants likely relates
    to key information for completing a task whereas information
    highlighted by a few participants may be more dependent on the knowledge of individuals.}
\end{bluequote}
