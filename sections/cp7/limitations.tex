



%\section{Limitations \& Trade-offs}
\section{Relevance}
\label{cp7:tools}

\gcm{A primary assumption in this dissertation
is that there will be text in documents
associated with a software development
task that is commonly seen as \textit{relevant}
to the task.} Chapter~\ref{} demonstrates
that sufficient commonality of relevant
text does exist to support the development
of techniques and Chapter~\ref{} shows
that the automatically identified text
is seen as relevant. For instance,
some participants in the \acs{tool}
experiment indicated:

\gcm{Is footnote size allowed?}
\smallskip
\begin{footnotesize}
    \begin{quote}
        ``\textit{The highlights were super useful, without them I would definitely had not been able to rapidly do the task.}''
    \end{quote}
\end{footnotesize}


\begin{footnotesize}
\begin{quote}
    ``\textit{With the highlighted references, I was able to move much quicker. I quickly glanced at each resource, reading just the highlights to determine how valuable that resource was. The highlights allowed me to focus on the most relevant resources, gathering the necessary information to complete the task. 
}''
\end{quote}
\end{footnotesize}

Yet, others who participated in the \acs{tool}
experiment disagreed with the concept that text
in these documents was important at all:

%\paragraph{\textbf{When is tool support needed?}} 


%A notable trade-off of answering which approaches can identify task-relevant text (Chapter~\ref{ch:characterizing}) and whether developers benefit from them (Chapter~\ref{ch:assisting}), is that our studies did not focus on when tool support is needed and for whom.
%For example, certain participants in the \acs{tool} experiment indicated that they focus more on code snippets
and that they did not make use of the highlights provided by our tool:




\smallskip
\begin{footnotesize}
\begin{quote}
``\textit{I realized going through [the experiment] that I read very little free-form text when looking for solutions. Mainly code samples with clear, succinct examples or type/method definitions.}''
\end{quote}
\end{footnotesize}

\begin{footnotesize}
\begin{quote}
``\textit{I'm not going to bother reading the text if I don't have to, especially when the code snippets are easy to understand.}''
\end{quote}
\end{footnotesize}


%While other participants indicated that textual highlights were essential to complete a task, even when the task required writing code:



%\smallskip
%\begin{footnotesize}
%    \begin{quote}
%        ``\textit{The highlights were super useful, without them I would definitely had not been able to rapidly do the task.}''
%    \end{quote}
%\end{footnotesize}


%\begin{footnotesize}
%\begin{quote}
%    ``\textit{With the highlighted references, I was able to move much quicker. I quickly glanced at each resource, reading just the highlights to determine how valuable that resource was. The highlights allowed me to focus on the most relevant resources, gathering the necessary information to complete the task. 
%}''
%\end{quote}
%\end{footnotesize}



\smallskip
This feedback suggests that the kind of information
considered useful in a document varies with the
developer. It suggests that techniques will need to
be even more general than to work across document
types: techniques will also need to work across
the different kinds of information in a document.
Future studies should more deeply investigate
the kinds of information developers deem as
relevant and explore how developers gauge relevance.

How and what developers consider relevant in
documents may also impact how techniques trade-off
precision versus recall. 
In Chapter~\ref{ch:identifying}, we indicated
the techniques we explored favored locating
all relevant text within an
artifact, in other words recall, rather
than focusing on 
correctly determining relevant text, in other
words precision. 
We made this decision because, as described in 
 (Chapter~\ref{ch:introduction}),
missing relevant text would mean that a developer might have an incomplete or partial view of the information needed, which may lead to
sub-optimal decisions.

%This feedback suggests that we could have observed different usage behaviours if participants had the chance to decide when to invoke \acs{tool}.
%However, 
%our experimental design focused on comparing tasks using or not \acs{tool} and thus, participants did not have the option to 
%request \acs{tool} to automatically identify task-relevant text on the fly. 


%To understand who would use  \acs{tool} in which types of tasks or 
%what factors  influence the tool's usage,
%we would have to consider a different experiment with challenges and risks of its own.
%For example, designing a longitutional study investigating how developers 
%with different expertise and background
%of some open source community would use \acs{tool}.



% This feedback also brings to light one core limitation of our techniques:
% automatically identifying text relevant to a task is of little to no help
% to developers who focus on code snippets or to developers who disregard 
% reading the text in a natural language artifact.




%\paragraph{\textbf{Precision vs Recall.}}




However, when favoring recall,  we may obtain more false positives; non-relevant text indicated as relevant.
Due to the limited time developers spend inspecting a natural language artifact~\cite{Starke2009}, there is a chance that
 a developer could discard reading an artifact due to a false positive. 
 Although abandoning reading an artifact and moving to another artifact could lead to non-efficient work
 (i.e., a developer might have to perform further searches and perhaps revisit artifacts that they have already inspected),
we believe that the benefits of automatically identifying relevant text outweighs these risks. 
That is, inspecting an artifact without tool support is more time-consuming
than inspecting each sentence retrieved to judge their relevance to the task at hand. 
Future studies could explore the ramifications
of this trade-off in detail.

\section{Technique Deployment}

%\paragraph{\textbf{Training Data.}}

\gcm{I think both training data and costs are
more about how you would actually deploy
your technique. I'm a bit confused
on the training data whether you are suggesting
that the technique needs to be trained on
project-specific data? On Costs, I'm a bit
confused as to whether needing a server 
is just because of current hardware that
might be eased in the future.}

In Chapter~\ref{ch:identifying}, we have shown that semantic-based approaches identify task-relevant textual information across
different types of artifacts without relying on
assumptions about the nature of an artifact or its meta-data.
However, we
observe that the techniques with the best accuracy require training data
and this could be considered as an impediment to the using or improving of such  techniques. 



Although we acknowledge that the need for training data is a limitation inherent to supervised techniques,
we mitigated some of the challenges of requiring large amounts of training data by using pre-trained models~\cite{erhan2010pre-train}.
Furthermore, the fact that artifact-specific techniques, such as AnswerBot, have accuracy comparable to semantic-based approaches
 provides
a unique opportunity for the creation of synthetic data that one might use for training purposes.
In a similar manner to how researchers created word embeddings for the software engineering domain~\cite{Efstathiou2018}, one might gather significantly large amounts of data from online resources
and use techniques that apply to such resources to create the data needed for training.
A requirement to using such a bootstrapping mechanism is that we must show that models trained on synthetic data do not simply replicate the underlying heuristics of the techniques used for bootstrapping. That is, we have to show that a technique trained
in this manner correlates data from a task and an artifact in ways that
extend beyond what a technique used for bootstrapping already provides. For example, showing that
it can identify task-relevant textual information in a different type of artifact or showing
that its accuracy is significantly better than the technique used for bootstrapping.



\paragraph{\textbf{Costs.}}


A second limitation of the semantic-based techniques we explored
relates to the cost associated with these techniques. 
First, the hardware needed by 
the neural embeddings and the neural networks
often requires dedicated servers with significant 
memory and high-throughput computational power. Due to the high demand for commercial servers with such properties, deploying our tools for usage by software development communities might incur significant financial burdens. 
The need for dedicated servers also means that our proposed techniques and our web browser plug-in cannot run offline. If we consider a standard client-server architecture, such as the one of \acs{tool}, it means that one must send data about the task and artifacts that they are working to a remote server for processing. This might not always be possible due to privacy reasons, i.e., outside the public domain, organizations would not be willing to our tool and, depending on their size,  organizations might not have the resources needed for in-house solutions. Due to these limitations, the techniques and tools described in this dissertation 
must be treated as proofs-of-concept.

