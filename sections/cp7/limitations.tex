



\section{Limitations \& Trade-offs}
\label{cp7:tools}






\paragraph{\textbf{When is tool support needed?}} 


A notable trade-off of answering which approaches can identify task-relevant text (Chapter~\ref{ch:characterizing}) and whether developers benefit from them (Chapter~\ref{ch:assisting}), is that our studies did not focus on when tool support is needed and for whom.
For example, certain participants in the \acs{tool} experiment indicated that they focus more on code snippets
and that they did not make use of the highlights provided by our tool:




\smallskip
\begin{footnotesize}
\begin{quote}
``\textit{I realized going through [the experiment] that I read very little free-form text when looking for solutions. Mainly code samples with clear, succinct examples or type/method definitions.}''
\end{quote}
\end{footnotesize}

\begin{footnotesize}
\begin{quote}
``\textit{I'm not going to bother reading the text if I don't have to, especially when the code snippets are easy to understand.}''
\end{quote}
\end{footnotesize}


While other participants indicated that textual highlights provided by our tool were essential to complete a task, even when the task required writing code:



\smallskip
\begin{footnotesize}
    \begin{quote}
        ``\textit{The highlights were super useful, without them I would definitely had not been able to rapidly do the task.}''
    \end{quote}
\end{footnotesize}


\begin{footnotesize}
\begin{quote}
    ``\textit{With the highlighted references, I was able to move much quicker. I quickly glanced at each resource, reading just the highlights to determine how valuable that resource was. The highlights allowed me to focus on the most relevant resources, gathering the necessary information to complete the task. 
}''
\end{quote}
\end{footnotesize}





\smallskip
This feedback suggests that we could have observed different usage behaviours if participants had the chance to decide when to invoke \acs{tool}.
However, 
our experimental design focused on comparing tasks using (or not) \acs{tool} and participants did not have the option to 
request \acs{tool} to automatically identify task-relevant text on the fly. 
To understand who would use  \acs{tool} in which types of tasks or 
what factors  influence the tool's usage,
we would have to consider a different experiment with challenges and risks of its own.
For example, designing a longitutional study investigating how developers 
of some open source community would use \acs{tool}.



% This feedback also brings to light one core limitation of our techniques:
% automatically identifying text relevant to a task is of little to no help
% to developers who focus on code snippets or to developers who disregard 
% reading the text in a natural language artifact.




\paragraph{\textbf{Precision vs Recall.}}

In Chapter~\ref{ch:identifying}, we discuss that our semantic-based techniques were designed favoring locating all relevant text within and
artifact (\textit{i.e.,} recall) instead of 
correctly determining the text's relevance (\textit{i.e.,} precision). 
As we motivated in the introduction of this thesis (Chapter~\ref{ch:introduction}),
missing relevant text means that a developer might have an incomplete or partial view of the information needed,
and thus the reason why we favor recall. 
Nonetheless, this decision comes with a set of challenges that we now discuss.


When favoring recall,  we may obtain more false positives---non-relevant text indicated as relevant---and
due to the limited time developers spend inspecting a natural language artifact~\cite{Starke2009}, there is a chance that
 a developer could discard reading an artifact due to a false positive. 
 Although abandoning reading an artifact and moving to another artifact could lead to non-efficient work 
 because
a developer might have to perform further searches and perhaps revisit artifacts that they have already inspected,
we believe that the benefits of automatically identifying relevant text outweighs these risks. 
That is, inspecting an artifact without tool support is more time-consuming
than inspecting each sentence retrieved to judge their relevance to the task at hand. 




\paragraph{\textbf{Training Data.}}


In Chapter~\ref{ch:identifying}, we have shown that semantic-based approaches identify task-relevant textual information across
different types of artifacts without relying on
assumptions about the nature of an artifact or its meta-data.
However, we
observe that the techniques with the best accuracy require training data. Producing this
 data might require significant manual effort and this could be considered as an impediment to the use of such  techniques. 



Although we acknowledge that the need for training data is a limitation inherent to supervised techniques,
the fact that artifact-specific techniques, such as AnswerBot, have accuracy comparable to semantic-based approaches
 provides
a unique opportunity for the creation of synthetic data that one might use for training purposes.
In a similar manner to how researchers created word embeddings for the software engineering domain~\cite{Efstathiou2018}, one might gather significantly large amounts of data from online resources
and use techniques that apply to such resources to create the data needed for training.
A requirement to using such a bootstrapping mechanism is that we must show that models trained on synthetic data do not simply replicate the underlying heuristics of the techniques used for bootstrapping. That is, we have to show that a technique trained
in this manner correlates data from a task and an artifact in ways that
extend beyond what a technique used for bootstrapping already provides. For example, showing that
it can identify task-relevant textual information in a different type of artifact or showing
that its accuracy is significantly better than the technique used for bootstrapping.



\paragraph{\textbf{Costs.}}


A second limitation of the semantic-based techniques we explored in Chapter~\ref{ch:identifying}
relates to the cost of applying and deploying these techniques. 
First, the hardware required by 
the neural embeddings and the neural networks
requires dedicated servers with significant 
memory and high-throughput computational power. Due to the high demand for commercial servers with such properties, deploying our tools for usage by software development communities might incur significant financial burdens. 
The need for dedicated servers also means that our proposed techniques and our web browser plug-in cannot be run offline. If we consider a standard client-server architecture, such as the one of \acs{tool}, it means that one must send data about the task and artifacts that they are working to a remote server for processing. This might not always be possible due to privacy reasons, i.e., outside the public domain, organizations would not be willing to use such type of tool and, depending on their size,  organizations might not have the resources needed to deploy such tools in-house. Due to these limitations, the techniques and tools described in this dissertation 
must be treated as proofs-of-concept.

