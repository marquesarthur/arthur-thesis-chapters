






\section{Empirical Studies on Determining Relevancy}
\label{cp7:empirical-studies}



At multiple points in this dissertation, participants produced annotated data 
indicating the text that they deemed relevant to a software task. 
These annotations reflect their \textit{explicit} reasoning about the information 
available in the text and what they consider relevant to the tasks 
that we presented them. 
By reflecting 
on what is relevant, 
the annotated data in ou corpora is an approximation 
of what developers implicitly find relevant to a task. 
Due to differences that might arise from explict and implicit reasoning,
 we argue for the need for more empirical data 
originating from
software developers performing daily tasks
and gathered in a non-obstructive way.



Conducting empirical
 experiments in a more realistic environment is challenging~\cite{Kevic2015}.
This effort is worthwhile as
the richness of collected data can provide valuable insights
to provide a foundation for tool development.
Recent studies with eye-trackers~\cite{Cutrell2007, Petrusel2013, sharafi2020}
have shown that the technology provides new
methods for
data collection without disrupting a developer's workflow,
which could lead to more realistic data on which text developers perceive as task-relevant.
For instance, one could
extend the work done by Kevic and colleagues on
tracing developers' eye for change tasks to
also consider tracing data outside a developer's IDE~\cite{Kevic2015}.
In fact, early stages in the design of the 
\acs{DS-android} corpus (Chapter~\ref{ch:android-corpus}) had considered using 
an eye-tracker for this purpose.
However, we were unable to pursue this line of work 
due to the COVID-19 pandemic and challenges related to
recruiting participants and conducting an in-person experiments.




Earlier studies
on extracting relevant
text from natural language artifacts have typically
relied on annotated corpora created using coding guidelines.
These guidelines have procedures to resolve disagreements and the 
final corpora often does not include iterations of the data considered relevant 
throughout the coding process.
In our formative study, we have shown that---although there is consistency in the key information 
needed to complete a task---there is variability in what text developers deem relevant to a 
certain task. Such observation corroborates 
findings from other studies which observe that 
individuals assessing the same artifact may have different
information needs~\cite{Bavota2016, Walters2014}
and that there are behaviour differences on how
experts and novices seek information~\cite{Crosby1990, Busjahn2015}.
Therefore, we pose the need to further investigate
 such differences in the scope of
natural language artifacts and this  
could lead to benchmarks for evaluating techniques
focused on certain population.



We leave the investigation of other methods on how to determine which text 
developers deem relevant 
and the creation of benchmarks with different properties
as part of future research.
