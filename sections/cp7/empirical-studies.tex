






\section{Empirical Studies on Determining Relevancy}
\label{cp7:empirical-studies}




At multiple points in this dissertation, participants produced annotated data 
indicating the text that they deemed relevant to a software task. 
These annotations reflect their \textit{explicit} reasoning about the information 
available in the text and what they consider relevant to the tasks 
that we presented them. However, 
asking participants to indicate what was relevant might have 
introduced deviations from what information
they would implicitly use and deem relevant 
when performing a task~\cite{Lazar2017}.




Due to differences that might arise from explicit and implicit reasoning,
 we argue for the need for more empirical data gathered in a non-obstructive way
and originating from software developers performing daily tasks.
Conducting empirical
 experiments in a more realistic environment is challenging~\cite{Kevic2015}.
This effort is worthwhile as
the richness of collected data can provide valuable insights
to provide a foundation for tool development.


One potential method for
data collection 
considers 
how recent studies with eye-trackers~\cite{Cutrell2007, Petrusel2013, sharafi2020}
have shown that the technology is not as disruptive as other 
methods such as think aloud protocols or manual input. 
Hence, we believe that eye-tracking 
could be used in a developer's working environment, 
leading to more realistic data on which text developers perceive as task-relevant.
For example, one could
extend the work done by Kevic and colleagues on
tracing developers' eye for change tasks~\cite{Kevic2015} to
also consider tracing data outside a developer's IDE
for such a purpose.




A second venue to enrich datasets on the text that is relevant to a task 
is to identify who deemed what relevant. 
In Chapter~\ref{ch:related-work}, we described 
a series of studies that mine text from natural language artifacts.
These studies provide annotated data 
resulting from coding procedures that usually do not take into account
differences in what might be relevant; or disagreements are resolved during the annotation process. 
However, in our formative study (Chapter~\ref{ch:characterizing}),
we have observed that there is variability in what text may be relevant 
to a software task, a fact that other researchers have also brought to light~\cite{Bavota2016,Robillard2015}.
Therefore, if future empirical studies
provide more data about who perceived which text as relevant, it could lead to benchmarks 
for evaluating techniques
focused on a certain population  (e.g., expert versus novice developers~\cite{Crosby1990, Busjahn2015}). 





We leave the investigation of other data collection methods 
and the creation of benchmarks with different properties
as part of future research.





