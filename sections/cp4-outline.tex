\setcounter{chapter}{3}
\setcounter{rq}{1}


\chapter{Identifying Task-Relevant Text}
\label{ch:identifying}


\stepcounter{rq}

\vspace{1mm}

\begin{enumerate}[label=\textit{RQ\arabic*},leftmargin=1.4cm]
\setcounter{enumi}{\the\numexpr \arabic{rq} - 1\relax}

\item \textit{What techniques can we use to automatically extract text relevant to a software developer's task?} 

\end{enumerate}

\vspace{1mm}

--- Chapter introduction 

------ Use findings of characterization studies (cp3) to motivate the need for the techniques proposed in this chapter

------ Brief summary of techniques we propose

------ Brief summary of evaluation and results



\section{Approach}

--- We address the problem of automatically detecting task-relevant text in a \textit{artifact-agnostic} way. \vspace{3mm}

--- To design our approaches, we use the manually curated task-relevant sentences produced in our characterization study~\cite{marques2020} \vspace{3mm}

--- We investigate two main approaches:

------ \textit{Heuristic-based}: uses a set of properties in the text (including \textit{frame semantics}~\cite{fillmore1976frame}) to determine if a given sentence is relevant to the input task

------ \textit{ML-based}: uses the novel \textit{Transformer}~\cite{Vaswani2017attention} neural network to jointly model the relevancy of a given input sentence with respect to a task. 

\clearpage

--- Explain why we have selected these two approaches

------ Given the unbounded number of artifact types a developer may refer to, a light-weight approach applicable to any artifact is appealing to us

------ Although our dataset is fairly small (6 tasks and 20 artifacts), we want to investigate 
the applicability of pre-trained models that do not require large amounts of training data
to our domain problem~\cite{devlin2018bert, Ye2016, Bavota2016}

\section{Task-relevant Artifacts Corpus}

--- To evaluate the techniques proposed in this chapter, we require a sufficiently large corpus containing 
software tasks and associated artifacts originating from heterogeneous sources.
Unfortunately, such a corpus is not promptly available and thus, 
this section outlines the steps we took for its creation.


\section{Software Tasks}

--- We consider a software task as a piece of work undertaken by a developer that often has to be finished within a certain time~\cite{2004merriam}. 
Two common places a software task can be found are:

\begin{itemize}
    \item the description of a bug or feature request reported in a bug tracking systems; or in
    \item a post in a community forum, development mailing lists, and others.
\end{itemize}

\vspace{3mm}

--- Given this definition, we select tasks from StackOverflow and GitHub to build our corpus

------ We scope task selection to the \textit{Android} development domain such that we can select common tasks across the two sources \vspace{3mm}



--- Detail that StackOverflow tasks were selected from Baltes et al. dataset~\cite{baltes2019-rep}

------ Give example of a StackOverflow task 


\clearpage

--- Because there is no GitHub dataset promptly available, we selected GitHub issues from top starred Android projects on GitHub 

------ Projects were selected among Open Source Systems that had the \textit{Java} and \textit{Android} tags. 

------ We randomly sampled issues from a total of 10 distinct projects

------ Give example of a GitHub task 

\section{Artifact Selection}


--- Our artifact selection approach seeks to simulate everyday practices on how developers search the Web~\cite{rao2020, Xia2017} \vspace{3mm}

--- To that end, we consider a task's title (i.e., SO question or GitHub issue title) as the seed used to search for pertinent artifacts in a search engine

------ This is a common procedure adopted by many studies in the field (e.g.,~\cite{Xu2017} or ~\cite{Silva2019}). \vspace{3mm}


--- As there are many different sources of artifacts, we restrict artifact selection to well known and studied sources~\cite{Starke2009,Kevic2014, Li2013}:


\begin{itemize}
    \item Android and Java SE API documentation;
    \item Github issues;
    \item StackOverflow answers; and
    \item websites from the java and android categories on Alexa.com~\cite{alexa}.
\end{itemize}


\vspace{3mm}

--- For these sources, we use the \texttt{googlesearch} API~\cite{googlesearch} to perform Web searches

------ Results were limited to English and maximum of 5 resources were fetched for each type

------ These limitations were necessary because of throttling or even blocking mechanisms in the APIs used the fetch data in each of the sources considered \vspace{3mm}

--- Provide descriptive statistics for the corpus

------ 300 tasks, 150 from SO and 150 from Git

------ Approximately 2,500 artifacts

------ Almost 260,000 sentences


\section{Analytic Evaluation}

--- To be able to judge the accuracy of any technique used to automatically identify sentences relevant to a task,
we require test data comprising sentences that contain information needed for task completion. \vspace{3mm}


--- That is, for each artifact pertinent to a task, its text should be marked as relevant (or not) for that task. \vspace{3mm}

--- Since we are not aware of a dataset with such data, we rely on human experts to produce it \vspace{3mm}


--- With this data, we compare the accuracy of our approaches to state-of-the-art techniques

------ Due to such a comparison, this evaluation focuses on artifact types that have been evaluated in a similar context, e.g., API documentation~\cite{Robillard2015}, GitHub issues~\cite{Lotufo2012}, and StackOverflow answers~\cite{Xu2017}



\subsection{Golden Standard}

--- Creating test data for the entirety of our corpus is infeasible so we restrict our evaluation to a subset of 10 tasks evenly sampled from the two types of tasks in the corpus (i.e., 5 GitHub tasks and 5 StackOverflow tasks) \vspace{3mm}

--- For these tasks, golden data consists of any sentence that two or more experts have deemed as useful for task-completion \vspace{3mm}



\subsubsection{Annotators}

--- We recruited \red{n} graduate students with professional programming experience to produce \textit{golden} data for these 10 tasks. \vspace{3mm}


\subsubsection{Annotation Procedures}

--- Our intention is that goldens reflect text that an \textit{expert} would deem as useful for task completion. \vspace{3mm}


--- To produce such data, annotators had a set of randomly assigned tasks description and links to artifacts 
pertinent to the respective task at their disposal \vspace{3mm}

--- We asked annotators to write a short comment with instructions that a newcomer could follow to successfully complete the task.

------ The purpose of the comment was to ensure that annotators built enough context about the task \vspace{3mm}

--- Once the comment was written,  annotators had to manually highlight sentences that they deemed useful and that provide information that assists task completion.

--- Three different individuals performed each task. \vspace{3mm}

--- An in-house tool in the form of a Web browser plugin facilitated annotation procedures

\subsection{Method}

--- For each task, we have a set of artifacts with sentences---identified by experts--- that contain information relevant to the task \vspace{3mm}

--- These artifacts originate from three sources (i.e., API documentation, GitHub issues, and StackOverflow answers) and we apply a state-of-the-art technique able to automatically identify text considered key for the input task and artifact~\cite{nadi2020, Robillard2015, Lotufo2012, Xu2017}. \vspace{3mm}

--- We use the accuracy of these techniques as a means to compare how our artifact-agnostic approaches perform for the same task and artifact source



\subsubsection{Baselines}
\label{cp4:comparison-techniques}

--- We systematically reviewed related work and we identified techniques applicable to our domain problem

------ Selection criteria considered each technique's availability in existing replication packages and their readiness for use.

------ We also refrained from using approaches with training procedures (e.g., ~\cite{liu2020} or ~\cite{Treude2016}) because of ~\cite{Chaparro2017, fucci2019} \vspace{3mm}


--- Based on these criteria, we selected the following techniques for comparison:


\begin{itemize}[leftmargin=\parindent, font=\normalfont\itshape]
    \item \textit{SO Answers} (\texttt{AnsBot})~\cite{Xu2017} --- short description of the approach;
    
    \item \textit{API Documentation} (\texttt{APIRef})~\cite{Robillard2015} --- short description of the approach;
    
    \item \textit{GitHub issues} (\texttt{HurriedBug})~\cite{Lotufo2012} --- short description of the approach;
\end{itemize}





\subsubsection{Evaluation Metrics}

--- For each task and artifact type, we apply the appropriate state-of-the-art technique to automatically detect task-relevant text in that artifact

--- We then compute a technique's precision against the experts' highlights

--- We use such information to investigate how our proposed approaches compare for the same task and artifact type


\subsection{Results}

--- Discuss results \vspace{3mm}

\subsection{Threats to Validity}

--- Discuss threats \vspace{3mm}

\clearpage


\section{Human Evaluation}

--- This section reports on an empirical experiment where human evaluators 
indicated the perceived usefulness of text identified by automatic approaches on several software tasks.

--- For a random sample of 30 tasks in our corpus, software developers indicated if automatically detected text provided important/useful information needed to correctly accomplish a task.

--- To avoid confirmation biases, developers provided input for both text detected by our approaches and text detected by approaches in the literature (Section~\ref{cp4:comparison-techniques})



\subsection{Participants}

--- For this evaluation, we consider the challenges of recruiting software developers in the local area and we instead opt to use 
\textit{Amazon Mechanical Turk}~\cite{mturk} (MTurk).

--- Background questions ensured that individuals had Java development experience and that they had familiarity with the artifact sources in our corpus.

--- Specifically, a MTurk evaluator---\textit{turker}--- had to answer the following background questions:


\begin{enumerate}[leftmargin=\parindent, font=\normalfont\itshape, label=BQ\textsubscript{\arabic*.}]
    \item Is developing software part of your job? Yes, no 
    \item For how many years have you been developing software? Free text
    \item How would you rate your development expertise in Java? No experience at all developing Java, Beginner, Intermediate, Expert
    \item How would you rate your development expertise in Android? No experience at all developing Android, Beginner, Intermediate, Expert
    \item When performing a software task, what sources do you normally seek information on? Multiple choice: API documentation, Stack Overflow, Github, Medium, ...
\end{enumerate}

   

\subsection{Method}


--- All the evaluation was performed in the Mechanical Turk platform. \vspace{3mm}

--- Turkers indicated the relevancy of a sentence to a given task  by answering the question: \vspace{3mm}

\begin{enumerate}[leftmargin=\parindent, font=\normalfont\itshape, label=SR\textsubscript{\arabic*}]
    \item Which of the following statements best describes this sentence? 
    \textit{(a)} The sentence is meaningful and provides important/useful information needed to correctly accomplish the task in question, 
    \textit{(b)} The sentence is meaningful, but does not provide any important/useful information to correctly accomplish the task in question, 
    \textit{(c)} The sentence does not make sense to me.
\end{enumerate}

--- We deliberately word the question as close as possible to other peer-reviewed studies in the field~\cite{nadi2020, Xu2017}. \vspace{3mm}


--- Due to constraints on the platform, the evaluators judged individually, i.e., without access to the context in which the sentences appeared. \vspace{3mm}

--- Turkers also had to answer a \textit{quality-gate task} derived from the experts' highlights.

------ The task showed five expert-curated sentences (all marked by the experts as relevant) and a turker had to indicate the sentences' usefulness as in any other task.

------ We discarded responses from any turker who deemed less than three out of the five sentences as useful for the task



\subsection{Results}

---  \vspace{3mm}

\subsection{Threats to Validity}

---  \vspace{3mm}

\clearpage

\section{Summary}

--- Summary of contributions for the chapter \vspace{3mm}

------ A corpus containing 300 software tasks originating from StackOverflow and Github and associated artifacts containing potentially relevant information to correctly completing the task \vspace{3mm}

------ Two artifact-agnostic techniques for automatically detecting text relevant to an input task in a given artifact

------ Empirical comparison of our proposed approach to existing techniques in the literature

------ Study with human evaluators on whether they consider automatically detected text as useful for task completion

