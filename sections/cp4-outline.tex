\setcounter{chapter}{3}
\setcounter{rq}{1}


\chapter{Android Task Corpus}
\label{ch:android-corpus}



One of the challenges for characterizing task-relevant information as well as for proposing automatic techniques to automatically identify task-relevant text is
the lack of large corpora containing
software tasks and associated artifacts originating from heterogeneous sources.
To fill this gap, this chapter contributes with the Android tasks corpus (\textbf{\acs{DS-android}}), a dataset with 300 tasks comprising Android development and originating from Stack Overflow questions
as well as from GitHub issues on a variety of Android Open-Source Systems.



This chapter main contributions are \textit{(i)} the procedures for the corpus creation, and; \textit{(ii)} the corpus itself. A well-structured corpus lays the foundation for several studies that explore relationships between software tasks, natural language artifacts, and text within these artifacts. \red{eval \& corpus summary}



\input{sections/cp4/motivation.tex}

\input{sections/cp4/methodology.tex}


\clearpage

\section{Evaluation}
\label{cp4:evaluation}


To construct the \acs{DS-android} corpus, we apply a set of techniques 
to artifacts outside the ones where the techniques were originally proposed and evaluated.
For instance, \acs{AnsBot} was evaluated in a set of 100 general Java programming tasks~\cite{Xu2017} while our tasks comprise Android development.
Generalizability is a common threat emphasized in each of the techniques original studies~\cite{Xu2017, Lotufo2012, Robillard2015} and thus, there is a risk that the techniques do no apply to 
the tasks and artifacts in our corpus.




To mitigate this risk, we evaluate the accuracy of each technique on a subset of our corpus. We rely on three reference answers produced by 
experienced software developers to define \textit{golden relevant sentences} 
for a sample of 10 tasks and associated artifacts in our corpus. 
We use the golden data to evaluate and report each technique's accuracy in our corpus.
If we can show that the techniques identify relevant text with some \textit{margin of error},
future research can use our corpus for the design and comparison of techniques that identify task-relevant text automatically.


% --- To be able to judge that our techniques \textit{correctly} identify sentences relevant to a task,
% we require test data comprising sentences that contain information needed for task completion. \vspace{3mm}

% ---  this data for 10 tasks randomly sampled from the \acs{DS-android} corpus. \vspace{3mm}


% --- With this data, we report a technique's accuracy

% ------ For sources that have been evaluated in other studies, i.e., API documentation~\cite{Robillard2015}, GitHub issues~\cite{Lotufo2012}, and StackOverflow answers~\cite{Xu2017}, we compare the accuracy of our techniques 
% to the state-of-the-art

% ------ We also report our techniques accuracy on miscellaneous sources to show that our techniques apply to a variety of artifact sources



\subsection{Golden Data}


Creating golden data for the entirety of the \acs{DS-android} corpus is infeasible.
since it would require asking human evaluators to inspect thousands of artifacts and more than 260,000 sentences.
Due to this reason, we restrict this evaluation to a subset of 10 tasks evenly sampled from the two types of tasks in our corpus (i.e., 5 GitHub tasks and 5 Stack Overflow tasks). 
From now on, we refer to this subset as the \textbf{\acs{DS-android-small}} corpus.


For each one of the tasks in the \acs{DS-android-small} corpus, we also randomly selected 
one artifact per source type to a maximun of 4 artifacts per tasks, i.e., one API document, a Github issue discussion, one Stack Overflow answer, and a blog or Web tutorial.
Overall, this corpus has 2,375 sentences with an average of 64 sentences per artifact---
a size comparable to the \acs{DS-synthetic} corpus.
% ($\mypm$ 72)


We asked human evaluators to read the content of these artifacts and 
to mark sentences that they deemed useful and that provide information that assisted task completion.
Golden data in \acs{DS-android-small} consists of any sentence that two or more evaluators have deemed as useful for task-completion.



\subsubsection{Annotators}
\textcolor{white}{force ident} % this is just for the chapter outline

--- We recruited \red{n} graduate students with professional programming experience to produce \textit{golden} data for our tasks sample. \vspace{3mm}


\subsubsection{Annotation Procedures}
\textcolor{white}{force ident} % this is just for the chapter outline

Our intention is that goldens reflect text that a experienced developer would deem as useful for task completion and that they would share with someone eager to make their \textit{first contribution} in an open-source project.


To produce such data, annotators had  at their disposal a set of randomly assigned tasks description and links to artifacts pertinent to the respective task. We asked annotators to write a short plan (250 words max~\cite{Rastkar2010}) with instructions that a newcomer could follow to successfully complete the task. 
The purpose of the plan was to ensure that annotators built enough context about the task.
While perusing artifacts, annotators also had to manually highlight sentences that they deemed useful and that provide information that assisted task completion. 


The annotation process was facilitated by an in-house tool, in the form of a Web browser plugin shown in Figure~\ref{fig:corpus-annotation-tool}. In the figure, the top-right corner panel shows the browser extension. Annotators could start an annotation session and click the highlight buttom.
This would instrument the HTML of a page and identify each sentence in a paragraph. The tool allowd annotators to hove over individual sentences and select them as relevant by clicking on the hovered text (text in orange). For example, the figure depicts that an annotator deemed the sentence
``\textit{Call {\small \texttt{ActivityOptions.setLockTaskEnabled()}} ... when starting the activity}'' as relevant for the lock mode task.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{cp4/annotation-tool}
    \caption{Annotation tool and relevant sentences marked by an annotator}
    \label{fig:corpus-annotation-tool}
\end{figure}



\subsection{Method}
\textcolor{white}{force ident} % this is just for the chapter outline

% \gm{Separate the corpus creation from the method. Name each corpus.}

--- For each task, we have a set of artifacts with sentences identified by experienced developers that contain information relevant to the task \vspace{3mm}

--- We then apply the appropriate technique (baseline and ours) to automatically detect task-relevant text in that artifact

\gm{Where will the baseline techniques be explained? This is why I think you probably need to put corpus creation in a separate earlier chapter.}

--- We compute the technique's accuracy against the highlights in \acs{DS-android-small}

--- We use the obtained values to discuss the correctness of each technique






\subsubsection{Evaluation Metrics}
\textcolor{white}{force ident} % this is just for the chapter outline

--- We use the golden data in \acs{DS-android-small} to compute precision and recall


--- For a given task $t$ and artifact $a$, precision is computed as the ratio between the sentences identified that are indeed relevant (i.e., sentences highlighted by the annotators) and the total number of sentences identified by the technique


\begin{equation}
    Precision(t, a) = \frac{\# \text{\textit{sentences identified that were marked by annotators}}}{\# \text{\textit{sentences identified}}}
\end{equation}

\vspace{3mm}

--- Recall represents how many relevant sentences are identified by a technique


\begin{equation}
    Recall(t, a) = \frac{\# \text{\textit{sentences identified that were marked by annotators}}}{\text{\textit{total \# of sentences marked by annotators}}}
\end{equation}

\vspace{3mm}

--- When comparing techniques, we favor precision instead of recall because false positives may contribute to a developer abandoning reading of an artifact that would otherwise provide crucial information for her task~\cite{Rastkar2010}.


\subsection{Results}
\textcolor{white}{force ident} % this is just for the chapter outline

--- Discuss results \vspace{3mm}

\subsection{Threats to Validity}

--- Discuss threats \vspace{3mm}




% \acs{DS-android-small}

% \acs{DS-android-large}