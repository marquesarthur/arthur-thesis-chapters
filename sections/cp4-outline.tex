\setcounter{chapter}{3}
\setcounter{rq}{1}


\chapter{Identifying Task-Relevant Text}
\label{ch:identifying}


\stepcounter{rq}

\vspace{1mm}

\begin{enumerate}[label=\textit{RQ\arabic*},leftmargin=1.4cm]
\setcounter{enumi}{\the\numexpr \arabic{rq} - 1\relax}

\item \textit{What techniques can we use to automatically extract text relevant to a software developer's task?} 

\end{enumerate}

\vspace{1mm}

--- Chapter introduction 

------ Use findings of characterization studies (cp3) to motivate the need for the techniques proposed in this chapter

------ Brief summary of techniques we propose

------ Brief summary of evaluation and results



\section{Approach}

--- We address the problem of automatically detecting task-relevant text in a \textit{artifact-agnostic} way. \vspace{3mm}

--- To design our approaches, we use the manually curated task-relevant sentences produced in our characterization study~\cite{marques2020} \vspace{3mm}

--- We investigate two main approaches:

------ \textit{Unsupervised}: describe a set of features (including \textit{frame semantics}~\cite{fillmore1976frame}) used to determine if a given sentence is relevant to the input task

------ \textit{Supervised}: uses the novel \textit{Transformer}~\cite{Vaswani2017attention} neural network to jointly model the relevancy of a given input sentence with respect to a task. 

\clearpage

--- Explain why we have selected these two approaches

------ Given the unbounded number of artifact types a developer may refer to, a light-weight unsupervised approach applicable to any artifact is appealing to us

------ Although our dataset is fairly small (6 tasks and 20 artifacts), we want to investigate 
the applicability of pre-trained models that do not require large amounts of training data
to our domain problem~\cite{devlin2018bert, Ye2016, Bavota2016}

\section{Task-relevant Artifacts Corpus}

--- To evaluate the techniques proposed in this chapter, we require a sufficiently large corpus containing 
software tasks and associated artifacts originating from heterogeneous sources.
Unfortunately, such a corpus is not promptly available and thus, 
this section outlines the steps we took for its creation.


\section{Software Tasks}

--- We consider a software task as a piece of work undertaken by a developer that often has to be finished within a certain time~\cite{2004merriam}. 
Two common places a software task can be found are:

\begin{itemize}
    \item the description of a bug or feature request reported in a bug tracking systems; or in
    \item a post in a community forum, development mailing lists, and others.
\end{itemize}

\vspace{3mm}

--- Given this definition, we select tasks from StackOverflow and GitHub to build our corpus

------ We scope task selection to the \textit{Android} development domain such that we can select common tasks across the two sources \vspace{3mm}



--- Detail that StackOverflow tasks were selected from Baltes et al. dataset~\cite{baltes2019-rep}

------ Give example of a StackOverflow task 


\clearpage

--- Because there is no GitHub dataset promptly available, we selected GitHub issues from top starred Android projects on GitHub 

------ Projects were selected among Open Source Systems that had the \textit{Java} and \textit{Android} tags. 

------ We randomly sampled issues from a total of 10 distinct projects

------ Give example of a GitHub task 

\section{Artifact Selection}


--- Our artifact selection approach seeks to simulate everyday practices on how developers search the Web~\cite{rao2020, Xia2017} \vspace{3mm}

--- To that end, we consider a task's title (i.e., SO question or GitHub issue title) as the seed used to search for pertinent artifacts in a search engine

------ This is a common procedure adopted by many studies in the field (e.g.,~\cite{Xu2017} or ~\cite{Silva2019}). \vspace{3mm}


--- As there are many different sources of artifacts, we restrict artifact selection to well known and studied sources~\cite{Starke2009,Kevic2014, Li2013}:


\begin{itemize}
    \item Android and Java SE API documentation;
    \item Github issues;
    \item StackOverflow answers; and
    \item websites from the java and android categories on Alexa.com~\cite{alexa}.
\end{itemize}


\vspace{3mm}

--- For these sources, we use the \texttt{googlesearch} API~\cite{googlesearch} to perform Web searches

------ Results were limited to English and maximum of 5 resources were fetched for each type

------ These limitations were necessary because of throttling or even blocking mechanisms in the APIs used the fetch data in each of the sources considered \vspace{3mm}

--- Provide descriptive statistics for the corpus

------ 300 tasks, 150 from SO and 150 from Git

------ Approximately 2,500 artifacts

------ Almost 260,000 sentences


\section{Analytic Evaluation}

--- To be able to judge the accuracy of any technique used to automatically identify sentences relevant to a task,
we require test data comprising sentences that contain information needed for task completion. \vspace{3mm}


--- That is, for each artifact pertinent to a task, its text should be marked as relevant (or not) for that task. \vspace{3mm}

--- Since we are not aware of a dataset with such data, we rely on human experts to produce it \vspace{3mm}


--- With this data, we compare the accuracy of our approaches to state-of-the-art techniques

------ Due to such a comparison, this evaluation focuses on artifact types that have been previously evaluated in the literature, i.e., API documentation, GitHub issues, and StackOverflow answers



\subsection{Golden Standard}

--- Creating test data for the entirety of our corpus is infeasible so we restrict our evaluation to a subset of 10 tasks evenly sampled from the two types of tasks in the corpus (i.e., 5 GitHub tasks and 5 StackOverflow tasks) \vspace{3mm}

--- For these tasks, golden data consists of any sentence that two or more experts have deemed as useful for task-completion \vspace{3mm}



\subsubsection{Annotators}

--- We recruited \red{n} graduate students with professional programming experience to produce \textit{golden} data for these 10 tasks. \vspace{3mm}


\subsubsection{Annotation Procedures}

--- Our intention is that goldens reflect text that an \textit{expert} would deem as useful for task completion. \vspace{3mm}


--- To produce such data, annotators had a set of randomly assigned tasks description and links to artifacts 
pertinent to the respective task at their disposal \vspace{3mm}

--- We asked annotators to write a short comment with instructions that a newcomer could follow to successfully complete the task.

------ The purpose of the comment was to ensure that evaluators built enough context about the task \vspace{3mm}

--- Once the comment was written,  evaluators had to manually highlight sentences that they deemed useful and that provide information that assists task completion.

--- Three different individuals performed each task. \vspace{3mm}

\subsection{Method}

--- For each task, we have a set of artifacts with sentences---identified by experts--- that contain information relevant to the task \vspace{3mm}

--- These artifacts originate from three sources (i.e., API documentation, GitHub issues, and StackOverflow answers) and we apply a state-of-the-art technique able to automatically identify text considered key for the input task and artifact~\cite{nadi2020, Robillard2015, Lotufo2012, Xu2017}. \vspace{3mm}

--- We use the accuracy of these techniques as a means to compare how our artifact-agnostic approaches perform for the same task and artifact source



\subsubsection{Baselines}

--- We systematically reviewed related work and we identified techniques based on availability in existing replication packages and their readiness for use.

------ We also refrained from using approaches with training procedures (e.g., ~\cite{liu2020} or ~\cite{Treude2016}) because of ~\cite{Chaparro2017, fucci2019} \vspace{3mm}


--- Based on these criteria, we selected the following techniques for comparison:


\begin{itemize}[leftmargin=\parindent, font=\normalfont\itshape]
    \item \textit{SO Answers} (\texttt{AnsBot})~\cite{Xu2017} --- short description of the approach;
    
    \item \textit{API Documentation} (\texttt{APIRef})~\cite{Robillard2015} --- short description of the approach;
    
    \item \textit{GitHub issues} (\texttt{HurriedBug})~\cite{Lotufo2012} --- short description of the approach;
\end{itemize}





\subsubsection{Evaluation Metrics}

--- For each task and artifact type, we apply the appropriate state-of-the-art technique for automatically detecting task-relevant text in that artifact and report the technique's precision

--- We use such information to investigate how our proposed approaches compare for the same task and artifact type


\subsection{Results}

--- Discuss results \vspace{3mm}

\subsection{Threats to Validity}

--- Discuss threats \vspace{3mm}

\section{Human Evaluation}

--- Ultimately, we seek to reduce the time and effort developers need to locate task-relevant information.
This is only possible if a developer, working on a given task, deems the sentences detected by our proposed technique as useful \vspace{3mm}

--- To provide empirical data

 

\subsection{Participants}

---  \vspace{3mm}

\subsection{Method}

---  \vspace{3mm}

\subsection{Results}

---  \vspace{3mm}

\subsection{Threats to Validity}

---  \vspace{3mm}

\section{Summary}

--- Summary of contributions for the chapter \vspace{3mm}