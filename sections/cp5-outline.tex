\setcounter{chapter}{4}
\setcounter{rq}{1}


\chapter{Identifying Task-Relevant Text}
\label{ch:identifying}

The information a developer seeks to help aid the completion of a task typically exists
across a range of artifacts (\red{Chapter~\ref{}}). Prior work has used syntactic properties of the text
in these artifacts (alongside an artifact's meta-data)
to automatically identify relevant text for particular kinds of artifacts and tasks (\red{Chapter~\ref{}}).
\gm{Consider strengthening above sentence. The important aspect is not just syntactic
but that it is per kind of artifact type. I think this needs to come out more clearly.}
We have  shown that, in a small set of tasks and artifacts, task-relevant text might be identified through its semantics (\gm{Chapter 3?}). \gm{By this point in the thesis have you shown
that semantics help - is this the synthetic chapter? Isn't it about automating 
identification by semantics in this chapter?}
A natural question that follows is whether we can use semantic or other properties to identify relevant text across a wider variety of software tasks and artifacts.

In this chapter, we explore a design space of possible techniques building on approaches to
interpret the meaning, or semantics, of text. We compare proposed approaches to properties
used in existing \gm{artifact-specific?} techniques to assess the improvements possible. \gm{Is it
improvements or addressing a broader set of artifacts?}

%To answer this question, we explore a set of approaches from previous related work and of our own 
%for the automatic identification of text that might contain information relevant to a particular software task.
%Through usage of human-annotated data produced earlier in this thesis, we 
%investigate how accurately can these techniques identify text relevant to software development task.
We start by outlining the hypotheses that motivate the techniques that we explore (Section~\ref{cp5:motivation}) followed by detailed descriptions of the
techniques (Section~\ref{cp5:approaches}).
We then present an evaluation (Section~\ref{cp5:evaluation}) \gm{... Consider 'We show..."}
Section~\ref{cp5:evaluation} describes evaluation results while
Section~\ref{cp5:comparison} compares the techniques explored to related work.
Section~\ref{cp5:summary} summarizes our key findings.

\clearpage




\input{sections/cp5/motivation}
\input{sections/cp5/background}
\input{sections/cp5/approach}
\input{sections/cp5/evaluation}
\input{sections/cp5/comparison}
\input{sections/cp5/summary}




% \setcounter{chapter}{4}
% \setcounter{rq}{1}


% \chapter{Identifying Task-Relevant Text}
% \label{ch:identifying}



% Designing an automatic technique able to identify text relevant to a task across the range of artifacts a developer might seek information on means solely using data common to these different artifacts.
% We have shown how  prior work uses syntactic properties (alongside an artifact's meta-data)
% to detect relevant text in specific artifact types.
% We have also shown that, in a small set of tasks and artifacts, task-relevant text might be identified through its semantics.
% A natural question that follows is whether we can use these properties to identify relevant text across a wide variety of software tasks and artifacts.




% To answer this question, we explore a set of approaches from previous related work and of our own 
% for the automatic identification of text that might contain information relevant to a particular software task.
% Through usage of human-annotated data produced earlier in this thesis, we 
% investigate how accurately can these techniques identify text relevant to a particular software development task.
% We start by outlining the hypotheses that motivate the techniques that we explore in Section~\ref{cp5:motivation}.
% We present details about the techniques themselves in Section~\ref{cp5:approaches}.
% Section~\ref{cp5:evaluation} describes evaluation results while
% Section~\ref{cp5:summary} summarizes our key findings.



% \input{sections/cp5/motivation}
% \input{sections/cp5/approach}
% \input{sections/cp5/evaluation}
% \input{sections/cp5/summary}

