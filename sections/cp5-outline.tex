\setcounter{chapter}{4}
\setcounter{rq}{1}


\chapter{Identifying Task-Relevant Text}
\label{ch:identifying}

The information a developer seeks to help aid the completion of a task typically exists
across a range of artifacts (\red{Chapter~\ref{}}). Prior work has used syntactic properties of the text
in these artifacts (alongside an artifact's meta-data)
to automatically identify relevant text for particular kinds of artifacts and tasks (\red{Chapter~\ref{}}).
We have  shown that, in a small set of tasks and artifacts, task-relevant text might be identified through its semantics.
A natural question that follows is whether we can use semantic or other properties to identify relevant text across a wider variety of software tasks and artifacts.

In this chapter, we explore a design space of possible techniques building on approaches to
interpret the meaning, or semantics, of text. We compare proposed approaches to properties
used in existing techniques to assess the improvements possible. 

%To answer this question, we explore a set of approaches from previous related work and of our own 
%for the automatic identification of text that might contain information relevant to a particular software task.
%Through usage of human-annotated data produced earlier in this thesis, we 
%investigate how accurately can these techniques identify text relevant to software development task.
We start by outlining the hypotheses that motivate the techniques that we explore in Section~\ref{cp5:motivation}.
We present details about the techniques themselves in Section~\ref{cp5:approaches}.
Section~\ref{cp5:evaluation} describes evaluation results while
Section~\ref{cp5:summary} summarizes our key findings.

\clearpage

\gm{Perhaps you could consider a structure for this 
chapter that has: 1) Problem Definition (which is the 'all approaches
below' 2) Baseline 3) Approaches. The reason is to try to make
it clear what is a baseline and what are new proposed
approaches. Let's discuss.}

\art{I tried to move the background needed to understand the approaches to an earlier section. I'm still thinking if `Baseline' should have its own section of fall under evaluation}



\input{sections/cp5/motivation}
\input{sections/cp5/approach}
\input{sections/cp5/evaluation}
\input{sections/cp5/summary}




% \setcounter{chapter}{4}
% \setcounter{rq}{1}


% \chapter{Identifying Task-Relevant Text}
% \label{ch:identifying}



% Designing an automatic technique able to identify text relevant to a task across the range of artifacts a developer might seek information on means solely using data common to these different artifacts.
% We have shown how  prior work uses syntactic properties (alongside an artifact's meta-data)
% to detect relevant text in specific artifact types.
% We have also shown that, in a small set of tasks and artifacts, task-relevant text might be identified through its semantics.
% A natural question that follows is whether we can use these properties to identify relevant text across a wide variety of software tasks and artifacts.




% To answer this question, we explore a set of approaches from previous related work and of our own 
% for the automatic identification of text that might contain information relevant to a particular software task.
% Through usage of human-annotated data produced earlier in this thesis, we 
% investigate how accurately can these techniques identify text relevant to a particular software development task.
% We start by outlining the hypotheses that motivate the techniques that we explore in Section~\ref{cp5:motivation}.
% We present details about the techniques themselves in Section~\ref{cp5:approaches}.
% Section~\ref{cp5:evaluation} describes evaluation results while
% Section~\ref{cp5:summary} summarizes our key findings.



% \input{sections/cp5/motivation}
% \input{sections/cp5/approach}
% \input{sections/cp5/evaluation}
% \input{sections/cp5/summary}

