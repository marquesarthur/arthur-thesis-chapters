\section{Motivation}
\label{cp5:motivation}

\gm{Presumably a lot of this text will eventually move forward in the thesis.}
When provided with a set of artifacts that potentially contain information relevant to their task,
software developers must inspect the artifact's content and locate the portion of the text that might be relevant to the task-at-hand. 
Some of the artifacts inspected are short enough that a developer can find if they contain any helpful information at a quick glance.
Some others are lengthy~\cite{Rastkar2013t} and factors such
as high time pressure or
the need to meet deadlines~\cite{meyer2019}
may lead a developer to quickly skim the document
in an attempt to find any of the text that is relevant to their task~\cite{Starke2009},
which Robillard and Chhetri summarize as the information \textit{filtering problem}:

\smallskip
\begin{bluequote}
    \textit{The burden  of having to sit through large amounts of irrelevant text, e.g., because of legacy information, boilerplate text, or because the text is intended for a reader other than the developer who is currently inspecting the artifact, to locate the pieces relevant to the developer's task}~\cite{Robillard2015}
\end{bluequote}



Failing to locate task-relevant text in these quick search episodes can 
cause a developer to abandon an artifact that could otherwise contain information  helpful to her task~\cite{Brandt2009a, Starke2009}.
Hence, several studies have proposed automatic approaches to assist this activity. 
These approaches leverage textual cues as well as an artifact's meta-data and, ultimately, they have the goal of surfacing 
the most important information in a document so that a developer can quickly 
locate it.


As examples of approaches, Xu et al.~\cite{Xu2017} and Ponzanelli et al.~\cite{Ponzanelli2015}
argue that words that appear in a task description and that also appear in sentences within an artifact serve as indicators of the sentence's relevance to the developer's task.
In turn, Silva et al.~\cite{silva2019} discuss that 
there is a lexical gap between a task description and the information associated 
with the solution for that task, which lead them to identify
relevant sentences based on word semantics.
In a similar vein, Di Sorbo et al. suggest that relevant information might be identifiable 
according to the text's purpose, such as if it details a feature request, a problem, a solution proposal and others~\cite{Sorbo2015}.


Although effective, the cues used to automatically identify text relevant to a software task in the aforementioned studies have focused on specific tasks (\red{example + ref}) and artifact types (\red{example + ref}).
Therefore, the question of whether these cues can identify relevant text across a wider range of artifact types and tasks remains open. 

\gm{I think you may wish to redirect this section towards
being about the design space, drawing on the related work
chapter. I think leave for now and revist later.}

As a first step towards answering this question, we set out to investigate:


\begin{enumerate}
    \item \textit{How accurately can lexical properties identify text deemed relevant to a task?}
    \gm{Are you really assessing or is this just simply the baseline and not a question? Describing as a design space would likely make this easier.}
    We assess to what extent can a lexical similarity approach automatically identify text relevant to a software task.
    We use lexical similarity as a baseline since both our card sorting analysis (\red{Section~\ref{aaa}}) and related work~\cite{Ko2006a, Freund2015} has shown
    that developers often use keyword-matching as a simple and quick search strategy to locate task-relevant information in a software artifact.


    \item \textit{How accurately can word semantics identify text deemed relevant to a task?}
    Guided by studies that discuss that there are lexical gaps between a task description and the text related to that task's solution~\cite{silva2019, Huang2018, Ye2016},
    we ask if we can identify text relevant to a software task through semantic matching.
    To answer this question, we explore how can we use language models~\cite{Devlin2018Bert, Mikolov2013} to identify  task-relevant text.

    
    \item \textit{Does the relevance of a sentence depend on the sentence's meaning?}
    Our study on characterizing task-relevant text (\red{Section~\ref{aaa}}) has show that text deemed relevant often contains semantic frames describing 
     required events, obligations, actions evoking system calls, etc.
    Given how these frames capture key factors used to understand a sentence's meaning
    and also their prominence in relevant text, 
    we assess if semantic frames help to filter task-relevant text.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\clearpage

\section{Problem Statement}
\label{cp5:motivation}


% \gm{I think you may wish to redirect this section towards
% being about the design space, drawing on the related work
% chapter. I think leave for now and revist later.}



% Designing an automatic technique able to identify information relevant to a task across the range of artifacts a developer might 
% seek means solely using data common to these different artifacts. Natural language text is an example of such data---if not the most common one~\cite{Bavota2016}---available in 
% versioning systems, issue trackers, archived communications, and many other software artifacts. 
% Due to its prompt availability, we consider its usage for the automatic identification of information relevant to a software task. For that, we must take into account one underlying problem related to the text available in a software task and the text in an artifact pertinent to that task:




Researchers have long recognized the value of natural language
text, utilizing various techniques to extract
information from this text that can be embedded in
tools for software developers.
A number of the techniques commonly employed by researchers are based on the
frequency of co-occurrence of words (or phrases) in documents. 
An early example is Maarek and Smadja's use of lexical relations to index
software libraries~\cite{maarek1989}. 
Since this early use, software engineering
researchers have continued to leverage advances in
these approaches, such as when 
Maletic and Marcus applied \acf{LSI}~\cite{deerwester1990LSI} to help cluster software components to aid
program comprehension of a software system~\cite{Marcus2003}.


However, researchers have also identified limitations on the applicability of lexical-based techniques~\cite{silva2019, Ye2016, Sorbo2015}. For example, Ye et al. has shown that \acs{VSM}~\cite{salton1975vector} 
fails at retrieving API references that are relevant to Stack Overflow Java questions~\cite{Ye2016} while
Di Sorbo and colleagues observed that lexicon analysis, like \acs{LDA}~\cite{blei2003latent}, was insufficient to classify 
emails based on developers' intentions~\cite{Sorbo2015}.








\medskip
\begin{bluequote}
    \textit{The text  that contains information associated with the solution for a task in a natural language artifact often differs from the text in the software task itself.}
\end{bluequote}
% \smallskip


Differences in the two sources (i.e., software task and natural language artifact) pose challenges to establishing relationships between the text within them and, consequently, identifying the text that is relevant to a task is not  trivial \red{ref}.


As a first step towards addressing this problem, we consider a set of techniques
 building on approaches to interpret the meaning, or semantics, of text.
 For that, we first provide theoretical background for the approaches that we explore. Then, we detail how we use these approaches in the design space of a technique that automatically identifies text relevant to a software task.




\subsection{Background}
\label{cp5:background}









In this section, we detail approaches that have been used in related-work to extract information from natural language software artifacts. We introduce core concepts related to each approach, provide examples of their usage in software engineering research, and detail their potential limitations.








\subsubsection{Language Models}




% Although lexicon-based approaches are a cheap and efficient way to identify relevant information in some domains, researchers have observed that lexicon-based approaches
% may apply to natural language software artifacts~\cite{Huang2018, Kevic2014}.


\red{need intro do previous semantic techniques}








% introduce language models

A language model provides  vector representations, namely \textit{word embeddings}, for each of the words in a text corpus, where similar vector embeddings represent words that are similar in meaning~\cite{Ye2016}, as in inferring that the word \textit{king} and \textit{queen} refer to \textit{royalty}~\cite{Mikolov2013}. 







\subsubsection{BERT Model}

Context in the Skip-gram model refers to the positive/negative examples used during the model's training procedures; this, however, does not allow the model to disambiguate words based on their surrounding text. In other words, a Skip-gram model will have a single vector representation for the word \textit{company} even when it can have different meanings, i.e., a business organization or being with someone. In contrast, state-of-the-art models, such as \textit{BERT}~\cite{Devlin2018Bert}, provide different representations for the same word based on the sentence in which a word appears.
This additional layer allows for more complex operations, such as word disambiguation \red{ref}.












  


\subsection{Design Space}
\label{cp5:design-space}



% Short, state that we first explore VSM
% Then jump to semantic techniques
% say how we use these in 


%  traditional  (e.g., or ~\cite{}) might fail at automatically identifying information that is relevant to a developer's task. 




As an example, Nguyen and colleagues applied word semantic representations~\cite{Mikolov2013space} to improve  retrieval of API examples~\cite{nguyen2017} while Huang et al. have used word semantics 
to more accurately classify the intentions of text in development emails~\cite{huang2018automating}.



Guided by these studies, we first consider if a general lexicon-based approach suffices to identify text that is relevant to a software task in a natural language artifact pertinent to that task. 
If we can show that an off-the-shelf approach, such as \acs{VSM}, is able to accurately identify relevant text across a wide variety of software tasks and artifacts, 
it may be embedded into a tool that assists developers in this endeavor without the need for more complex or costly approaches~\cite{Rastkar2013}.




However, we may also observe that lexical approaches are not sufficient, what raises the question of whether approaches that infer the meaning of the text apply to our domain problem.
Thus, we also explore approaches able to infer the semantic meaning of text at the word and sentence level 
and we evaluate how accurately can these approaches identify text deemed relevant to a software task.




\subsection{Background}









% when Nguyen and colleagues
% applied Word2Vec~\cite{mikolov2013word2vec} to support the retrieval of API
% examples~\cite{nguyen2017}.










% Start with tf-idf and lexicon approaches. 
% Show their limitation, jump to word embeddings 
% Jump to BERT



Many studies have explored techniques building on traditional \acf{IR}~\cite{Manning2009IR} or lexicon-based approaches to identify relevant text in a natural language artifact pertinent to  
certain software tasks (e.g., \red{examples} ). % ~\cite{Ponzanelli2015, Xu2017}
However, when we consider the need to automatically identify information that is relevant to a developer's task
in a variety of types of artifact, traditional \acs{IR} or  because:








% \subsubsection{Vector Space Model}


% Considering lexicon-based techniques, \acf{VSM}~\cite{Salton1975vsm} has been commonly used to compute lexical similarities between an element of interest, often referred to a as a \textit{query}, and artifacts in the software system, commonly called \textit{document}. \red{example}





% \reversemarginpar
% \marginnote{{\scriptsize \texttt{Vector Space Model}}}[3cm]




% VSM represents both the text in a query and individual sentences within a document as vectors of term weights,
% where the weight of a term
% can be computed using a Term-Frequency Inverse-Document-Frequency scheme (\textit{tf.idf})~\cite{Manning2009IR}. 
% Subsequent to obtaining vector representations $q$ and $s$ 
% for a query and a arbitrary sentence, 
% their lexical similarity can be computed 
% using the cosine similarity between their vectors, as Equation~\ref{eq:lex-sim} shows:


% \begin{equation}
%     cos(q,s) = \frac{q^Ts}{\|q\| \|s\|}
%     \label{eq:lex-sim}
% \end{equation}
% \smallskip



% Researchers have used VSM as a means to extract information that
% is relevant to certain software tasks by ranking the text in a document according to its similarity scores, i.e., from highest to lowest. 









% \textit{How accurately can lexical properties identify text deemed relevant to a task?}
% \gm{Are you really assessing or is this just simply the baseline and not a question? Describing as a design space would likely make this easier.}
%     We assess to what extent can a lexical similarity approach automatically identify text relevant to a software task.
%     We use lexical similarity as a baseline since both our card sorting analysis (\red{Section~\ref{aaa}}) and related work~\cite{Ko2006a, Freund2015} has shown
%     that developers often use keyword-matching as a simple and quick search strategy to locate task-relevant information in a software artifact.



% \red{begin with research question}


% We use our approach to identify sentences relevant to
% the tasks in the \acs{DS-android} corpus,
% comparing the sentences identified against the 
% sentences in the corpus' golden data. 
% For this evaluation, we set the target number of sentences identified (i.e., length of a summary) to 20\% of the content of an artifact,
% which is a value derived from our study on characterizing 
% task-relevant information in natural language artifacts~\cite{marques2020}.





% \subsection{Accuracy}


% Motivated by the fact that the text marked by the annotators in the \acs{DS-android} differs
% (Section~\ref{cp4:corpus-relevant-text}),
% we follow procedures outlined by Lotufo et al.~\cite{Lotufo2012}
% to measure how accurately our approach identifies text relevant to a task considering two evaluation scenarios:


% \begin{enumerate}
%     \item one where the text marked only by two or more annotators is considered relevant, and;
    
%     \item a second where we consider that any of the text marked by the annotators contributes towards task completion.
% \end{enumerate}

% \red{flip the above discuss this as a classification problem?}


% Standard precision and recall metrics~\cite{Manning2009IR} are suitable metrics for the first scenario. In the second, we use pyramid precision and pyramid recall metrics
% to compute accuracy based on how many annotators marked the text~\cite{Nenkova2004, Lotufo2012}. 
% To understand these metrics, we refer to the evaluation outcomes of Table~\ref{tbl:type-I-II-errors}. 
% The \textit{relevant} and \textit{not-relevant} columns represent the text 
% marked (or not) by annotators~\cite{Lotufo2012} for a particular task and a pertinent artifact. Rows represent the text automatically identified by our approach for the same task and pertinent artifact.


% \input{sections/cp5/tbl-eval-outcomes.tex}


% \subsubsection{Precision and recall}




% For standard precision and recall metrics, the relevant column in Table~\ref{tbl:type-I-II-errors} represents \textit{text marked by two or more annotators}. 
% In this scenario, $precision$ is the fraction of the sentences
%  identified that are relevant over the total number of sentences identified, as shown in Equation~\ref{eq:cp5:precision}.



% \vspace{2mm}
% Recall represents how many of all marked sentences are identified by our approach (Equation~\ref{eq:cp5:recall}).







% \subsubsection{Pyramid precision and pyramid recall}


% For pyramid precision and pyramid recall, the relevant column in Table~\ref{tbl:type-I-II-errors} represents \textit{text marked by any annotator}. 


% Pyramid precision considers the ``optimal'' sentences that could be identified, i.e., the sentences marked by most of the annotators, and compares that to the sentences actually identified. Let $marked(s)$ be a function that returns the number of annotators who marked the sentence $s$, pyramid precision is defined as 
% the summation of $marked(s)$ for the set of sentences identified ($s \in S$) over the set of optimal sentences derived from the golden data ($s \in G$).



% \begin{equation}
% \label{eq:cp5:pyramid-precision}    
%     Pyramid Precision =  \frac{
%         \sum_{s \in S} \text{ } marked(s)
%     }{
%         \sum_{s \in G} \text{ } marked(s)
%     }
% \end{equation}


% % \vspace{3mm}
% % As an example consider an artifact with a total of 6 marked sentences,
% % where 2 sentences were marked by the three annotators, 3 by two annotators and 1 sentence by a single annotator. 
% % When identifying three sentences, the optimal solution is $8$ $(2 \times 3 + 1 \times 2)$.
% % Hence, an approach that identifies three sentences maked by one, two and all annotators 
% % would have a pyramid precision score of $0.75$ or $\frac{ (1 + 2 + 3)}{(2 \times 3 + 1 \times 2)}$.




% \vspace{2mm}
% Pyramid recall calculates the fraction of the sentences identified by our approach over 
% any of the sentences marked by the annotators. Since we updated the set of $TP$ and $FN$ of Table~\ref{tbl:type-I-II-errors}, we can still compute pyramid precision using Equation~\ref{eq:cp5:recall}.




% \subsubsection{Results}


% When interpreting results, we favour precision/pyramid precision instead of recall/pyramid recall.
% Our rationale is that a developer might completely abandon reading of an artifact due to a false positive.
% Thus, even if we are not able to identify all sentences that are indeed relevant, true positives may encourage a developer to inspect a pertinent artifact more thoroughly~\cite{Singer1998, Brandt2009a}.




% Table~\ref{tbl:approach-results-overall} shows the accuracy of our approach 
% over all features as well as per group of features (i.e., lexical, word-semantics, and sentence-semantics).


% \input{sections/cp5/tbl-results-overall}


% Table~\ref{tbl:approach-results-task} shows evaluation metrics by type of task, i.e., GitHub or StackOverflow.

% \input{sections/cp5/tbl-results-by-task}



% We also investigate if an artifact type influences the observed accuracy. 
% Table~\ref{tbl:approach-results-artifacts} gives insight into the accuracy of our approach for the artifact types in the \acs{DS-android} corpus. Results comprise the set of features 
% that provide the best accuracy results. A breakdown of results per group of features is available under Appendix~\ref{a}.


% We use a Wilcoxon-Mann-Whitney test~\cite{mannWhitneyU} to check if differences in the  results for each 
% type of artifact are statistically significant. Results of the test indicate that... 

% \input{sections/cp5/tbl-results-by-artifact}




% \subsection{Threats to validity}


