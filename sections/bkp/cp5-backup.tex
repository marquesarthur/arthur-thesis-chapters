
% \clearpage




% \subsubsection{Baselines}
% \label{cp4:comparison-techniques}
% \textcolor{white}{force ident} % this is just for the chapter outline

% --- We systematically reviewed related work and we identified techniques applicable to our domain problem

% ------ Selection criteria considered each technique's availability in existing replication packages and their readiness for use.

% ------ We also refrained from using approaches with training procedures (e.g., ~\cite{liu2020} or ~\cite{Treude2016}) because of ~\cite{Chaparro2017, fucci2019} \vspace{3mm}


% --- Based on these criteria, we selected the following techniques for comparison:


% \begin{itemize}[leftmargin=\parindent, font=\normalfont\itshape]
%     \item \textit{SO Answers} (\texttt{\acs{AnsBot}})~\cite{Xu2017} --- short description of the approach;
    
%     \item \textit{API Documentation} (\texttt{\acs{Krec}})~\cite{Robillard2015} --- short description of the approach;
    
%     \item \textit{GitHub issues} (\texttt{\acs{Hurried}})~\cite{Lotufo2012} --- short description of the approach;

%     % \item \textit{Miscellaneous} (\texttt{LexRank})~\cite{Erkan2004} --- in the lack of an artifact-specific technique for this category, we use LexRank as a baseline because it has been evaluated in several studies that address the identification of textual information in natural language software artifacts~\cite{nadi2020, Ponzanelli2017}
% \end{itemize}




% ------------------------------ OLD ------------------------------





% \section{Large-scale Evaluation}
% \textcolor{white}{force ident} % this is just for the chapter outline

% \gm{Isn't this about scale more than human evaluation?}

% \rev{PLACEHOLDER - revisit this section once I finish the correctness evaluation}

% \rev{argument to justify this second evaluation}

% --- Although our analytic evaluation gives insight on how accurately can we identify text relevant to a task within a natural language artifact, we seek to provide further evidence on whether software developers consider automatically detected text as useful for a task 

% --- In this study, we asked software developers to indicate if automatically detected text provided important/useful information needed to correctly accomplish a task.

% --- This evaluation was performed on a random sample of 30 tasks in our corpus (distinct from the expert tasks)

% --- To avoid confirmation biases, developers provided input for both text detected by one of our approaches (i.e., the approach that had the best results in our analytic evaluation) and text detected by approaches in the literature (Section~\ref{cp4:comparison-techniques})



% \subsection{Participants}
% \textcolor{white}{force ident} % this is just for the chapter outline

% --- For this evaluation, we consider the challenges of recruiting software developers in the local area and we instead opt to use 
% \textit{Amazon Mechanical Turk}~\cite{mturk} (MTurk).

% --- Background questions ensured that individuals had Java development experience and that they had familiarity with the artifact sources in our corpus.

% --- Specifically, a MTurk evaluator---\textit{turker}--- had to answer the following background questions:


% \begin{enumerate}[leftmargin=\parindent, font=\normalfont\itshape, label=BQ\textsubscript{\arabic*.}]
%     \item Is developing software part of your job? Yes, no 
%     \item For how many years have you been developing software? Free text
%     \item How would you rate your development expertise in Java? No experience at all developing Java, Beginner, Intermediate, Expert
%     \item How would you rate your development expertise in Android? No experience at all developing Android, Beginner, Intermediate, Expert
%     \item When performing a software task, what sources do you normally seek information on? Multiple choice: API documentation, Stack Overflow, Github, Medium, ...
% \end{enumerate}

   

% \subsection{Method}
% \textcolor{white}{force ident} % this is just for the chapter outline


% --- All the evaluation was performed in the Mechanical Turk platform. \vspace{3mm}

% --- Turkers indicated the relevancy of a sentence to a given task  by answering the question: \vspace{3mm}

% \begin{enumerate}[leftmargin=\parindent, font=\normalfont\itshape, label=SR\textsubscript{\arabic*}]
%     \item Which of the following statements best describes this sentence? 
%     \textit{(a)} The sentence is meaningful and provides important/useful information needed to correctly accomplish the task in question, 
%     \textit{(b)} The sentence is meaningful, but does not provide any important/useful information to correctly accomplish the task in question, 
%     \textit{(c)} The sentence does not make sense to me.
% \end{enumerate}

% --- We deliberately word the question as close as possible to other peer-reviewed studies in the field~\cite{nadi2020, Xu2017}. \vspace{3mm}


% --- Due to constraints on the platform, the evaluators judged sentences individually, i.e., without access to the context in which the sentences appeared. \vspace{3mm}

% --- Turkers also had to answer a \textit{quality-gate task} randomly drawn from the \rev{experts}' tasks.

% ------ The task showed five expert-curated sentences (all marked by the \rev{experts} as relevant) and a turker had to indicate the sentences' usefulness as in any other task.

% ------ We discarded responses from any turker who deemed less than three out of the five sentences as useful for the task


% \subsubsection{Evaluation Metrics}
% \textcolor{white}{force ident} % this is just for the chapter outline

% --- We report a quantitative analysis of the ratings provided by the turkers




% \subsection{Results}
% \textcolor{white}{force ident} % this is just for the chapter outline

% --- Discuss results \vspace{3mm}

% \subsection{Threats to Validity}

% --- Discuss threats \vspace{3mm}

% \clearpage

% \section{Summary}
% \textcolor{white}{force ident} % this is just for the chapter outline

% --- Summary of contributions for the chapter \vspace{3mm}

% ------ A technique for automatically detecting text relevant to an input task in a given artifact

% ------ Empirical comparison of our technique to existing techniques in the literature

% ------ Study with human evaluators on whether they consider automatically detected text as useful for task completion





% to summarize software artifacts usually using artifact-specific properties~\cite{Xu2017, Li2018}.





% In a later work, they use this model to 
% help developers navigate through the artifacts retrieved in a Web search~\cite{Ponzanelli2017}.
% Orthogonal to Ponzanelli et al.~\cite{Ponzanelli2015},
% the same concept has been used by Xu et al. to 
% select relevant and
% salient sentences  in a Stack Overflow answer for summarization.






% We present an approach to automatically detect task-relevant text in a \textit{artifact-agnostic} way. \vspace{3mm}

% % --- Our goal is to to identify task-relevant text over distinct types of natural language software artifacts with high accuracy.  \vspace{3mm}

% % --- To this end, we explore a set of approaches (or their combination)
% % that exploit 
% % natural language processing techniques at 
% % the word and sentence level.  \vspace{3mm}


% --- When developing our approach, we use the \acs{DS-synthetic} dataset of manually curated task-relevant sentences produced in our characterization study~\cite{marques2020} \vspace{3mm}


\subsection{Word-level properties}

\rev{REVIEW}

% --- uses the novel \textit{Transformer}~\cite{Vaswani2017attention} neural network to jointly model the relevancy of a given input sentence with respect to a task. 

% --- Explain why we have selected the approach:

% ------ Our second approach investigates the applicability of pre-trained models that do not require large amounts of training data to our domain problem~\cite{devlin2018bert, Ye2016}. With this approach, we revisit findings on the trade-offs of using machine learning techniques to mine textual data~\cite{Chaparro2017, Bavota2016}.



\subsection{Sentence-level properties}

\rev{REVIEW}

% uses a set of properties in the text (including \textit{semantic frames}~\cite{fillmore1976frame}) to determine if a given sentence is relevant to the input task



% --- Explain why we have selected the approach:

% ------ With our first approach, we investigate whether a light-weight technique addresses the need of automatically identifying task-relevant text, what could potentially discourage the need for more complex and computationally expensive solutions~\cite{Bavota2016}





% Pyramid 
% precision and pyramid recall metrics, as reported by Lotufo et al.~\cite{Nenkova2004, Lotufo2012}, are suitable metrics for the second evaluation scenario. 
% If more annotators marked a sentence, it is likely that that sentence contains key information for completing a task albeit the metrics do not exclude sentences marked by a single annotator because these sentences provide a glimpse of an individual's perception of relevancy~\cite{Petrosyan2015}. 





% \vspace{3mm}
% --- Emphasize properties of our approach.


% ------ Approach is unsupervised and thus, does not require training procedures. 


% ------ Allowing the creation of shorter or longer summaries might be beneficial to accomodate the amount of information sought by newcomers or experienced developers

% ------ Approach can be augmented with artifact-specific meta-data, what might tailor the identification of task relevant information to specific types of artifacts.








% about steps towards designing







% We can follow the edges from these central nodes to identify a number of them,
% eventually hoping to a new node and starting this process once again. 









% Our goal is to design an \textit{artifact-agnostic} approach
% to identify sentences relevant to a task in an artifact pertinent to that task.
% We formulate this goal as an extractive query-based summarization problem~\cite{Goldsteinet1999}.
% That is, given a query (i.e., a software task),
% and a natural language software artifact as inputs,
% identify a number of sentences from the input artifact that
% most likely contain information relevant to that task. 
% In this context, a summary, or the output of our approach, 
% merely represents the set of sentences identified as 
% the most relevant to a particular task.







\red{Can I discuss this as a classification problem?}



\subsection{PageRank}


\red{LexRank appeared out of nowhere. I need to be up-front about why am I basing my technique on it}


LexRank~\cite{Erkan2004} is the backbone of our approach. 
It is based on the PageRank~\cite{Page1999} algorithm
and, on its most simplistic form, LexRank threats each sentence in an artifact as a node.
The algorithm
establishes edges between all the node pairs using a similarity function, what produces a graph. 
The more edges a node has, the more central the node is to the graph and the more likely it is that the sentence which that node represents contains key information. 
\red{confusing:} Thus, we can obtain a summary of the content of a pertinent artifact selecting $n$ of the most central nodes of the graph. 


To include information about a task, one can build a bipartite graph.
A first set of nodes represent sentences originating from a task while a second set, sentences in a pertinent artifact.
This time,  a similarity function 
establishes edges both between the nodes belonging to a the task and to a pertinent artifact.
By focusing our attention to the nodes (or sentences) with the most edges 
exclusive to the pertinent artifact set, we can produce a summary with information potentially relevant to that task.






\subsubsection{Similarity Function}


  
To devise a similarity function that accounts for the richness of data available in the text, we base our approach on Ponzanelli et al.'s meta-information model~\cite{Ponzanelli2015}.
\red{confusing:} This model defines a similarity function that uses a set of properties or features to compute similarity and thus, establish edges in the graph.
The overall similarity value of the model is the normalized sum of individual values obtained for each feature or property.


\art{Provide formulas/formal definitions once we've agreed upon approach/chapter outline}






% https://stackoverflow.com/questions/24652078


% title: No lock screen controls ever



% Have you checked RemoteControlClient ?
% it is used for the Android Music Remote control even if the App is in Lock mode.
% RemoteControlClient was what you were looking for, but now it's deprecated and has been replaced with MediaSession.
% Above mentioned class is deprecated now.
% So please check with LINK for that and update accordingly.



% Have you checked RemoteControlClient? it is used for the Android Music Remote control even if the App is in Lock mode.(same like image you have attached)
% Please check RemoteControlClient
% Just call below method while you receiver command action for Play,Pause,Next and previous of the Song track.
% Please also check this developer app given for how to integrate RemoteControlClient: Random Music Player However UI for the RemoteControlClient defer as per the device you can not updates its UI to your own but you have control to show and display the component and control of the Music app.
% Above mentioned class is deprecated now. So please check with Media Session for that and update accordingly.
% RemoteControlClient was what you were looking for, but now it's deprecated and has been replaced with MediaSession.
% The docs are here: https://developer.android.com/reference/android/media/session/MediaSession.html








Putting it all together, our approach is based on the LexRank algorithm.
We build a bipartite graph with textual data originating from a task as well as textual data originating from a pertinent artifact.
Relationships between nodes in the graph are established using a custom similarity function.
This custom function combines lexical properties from previous related work 
and also semantic properties at the word and sentence level.
Through this function, we can identify a number of prominent nodes (or sentences)
in an artifact that likely contains information relevant to a task.



It is worth noting that,
in addition to allowing a custom function, the algorithm has a set of properties that make it appealing to our domain problem:



\begin{enumerate}
    
    \item As an unsupervised approach, the algorithm does not require training data, what facilitates applying it to a broad range of natural language artifacts;

    \item It allows setting the number of sentences that it identifies as central, i.e., among the top-ranked nodes identified, we can select $n$ of them. Hence, 
    it may be possible to configure the number of sentences identified based on 
    a compression rate of an artifact's length, e.g., 20\% (Section~\ref{aaa}), or 
    based on a developer's experience (Section~\ref{aaa});
    
\end{enumerate}




\vspace{3mm}
\begin{hangparas}{1em}{1}
    \textbf{Semantic frame patterns:} Analyzing the annotated text of \acs{DS-android}, 
    we found that certain semantic frames are more prominent in sentences deemed relevant.
    We use this knowledge to encode semantic frames in a set of \textit{relevancy patterns}. 
    Guided by Xu et al.~\cite{Xu2017}, we set the value for this property to 1 if a sentence in a pertinent artifact contains at least one relevancy pattern, otherwise 0.
\end{hangparas}


\vspace{3mm}
\begin{hangparas}{1em}{1}
    \textbf{Semantic frame similarity:} The intentionality of a task might assist us in determining sentences relevant to that task. 
    This feature computes the cosine similarity between the vectors representing the semantic frames present in the task's title and the frames of each sentence within a pertinent artifact. \art{I need to explore this in \acs{DS-android}}
\end{hangparas}


\vspace{3mm}
\begin{hangparas}{1em}{1}
    \textbf{Word embeddings similarity:} Semantic similarity, as captured via word embeddings, might be an indicator of the relevance of a sentence to a task. With this property, we measure semantic similarity using the cosine similarity between the embedding vectors of a task's title and a sentence within a pertinent artifact. 
    For that, we use the \textit{FastText} model~\cite{bojanowski2017FastText} and the asymmetric semantic similarity function described by Ye et al.~\cite{Ye2016} and used in several other studies~\cite{Huang2018, Xu2017, silva2019}.
\end{hangparas}




\subsection{Feature evaluation}


We evaluate how each individual textual property contributes towards determining 
that a sentence in a pertinent artifact is relevant to a task 
using a \red{placeholder} test. 
Informally, the higher the test output value for a feature, the more the observed outcome (i.e., whether a sentence is relevant or not to a task) depends 
on that feature.
We use this knowledge to identify if certain features contribute more or less 
to the observed outcome.



\art{I started reading chi square, but it does not apply to the features explored because some of them are continuous. I may need a logistic regression model to answer this question}


\subsubsection{Results}



Table~\ref{tbl:features-chi-square} gives insight into the predictive power of each feature we use as part of our approach. 



\input{sections/cp5/tbl-features-chi-square}






\vspace{1mm}
\setcounter{rq}{2}
\begin{enumerate}[label=\textit{RQ\arabic*},leftmargin=1.4cm]
\setcounter{enumi}{\the\numexpr \arabic{rq} - 1\relax}

\item \textit{Which properties can better identify the text relevant to a task in an artifact pertinent to that task?} 

\end{enumerate}




\vspace{1mm}
\setcounter{rq}{2}
\begin{enumerate}[label=\textit{RQ\arabic*},leftmargin=1.4cm]
\setcounter{enumi}{\the\numexpr \arabic{rq} - 1\relax}

\item \textit{What syntactic and semantic properties can better identify the text relevant to a task?} 

\end{enumerate}




\red{confusing:}
Evaluation focuses on two main questions:

\begin{enumerate}
    \item How accurately can our approach detect text considered by humans as being relevant to a particular task?
    
    \item When identifying text relevant to a task, are certain features (i.e., lexical, word-semantics, and sentence-semantics) more informative than others?
\end{enumerate}


