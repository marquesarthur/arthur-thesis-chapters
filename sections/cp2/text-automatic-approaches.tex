\section{On the Automatic Approaches for Identifying Text in Natural Language Software Artifacts}
\label{cp2:text-approaches}



Information useful to a software task can be buried in irrelevant text or attached to 
non-intuitive blocks of text, making it difficult to discover~\cite{Robillard2015}.
Researchers have long recognized the value of assisting developers in 
identifying information of relevance in this natural language
text.
In this section, we detail tools and approaches from related work.




\subsection{Pattern Matching Approaches}


Pattern matching approaches rely on regular expressions describing a sequence of tokens that represent a relevant text fragment~\cite{Bavota2016}. Tokens can either represent words or linguistic elements 
extracted using \acf{NLP}.
    
    
As examples  of pattern matching approaches,  {\small DeMIBuD}~\cite{Chaparro2017} and Knowledge Recommender (Krec)~\cite{Maalej2013, Robillard2015} are tools that detect relevant sentences in bug reports and API documentation, respectively. 
These tools derive a set of patterns from annotated data and use them as part of heuristics 
that identify relevant text. Krec assumes that any relevant sentence mentions a 
code element (e.g., a class or method name) and it uses  361 unique patterns derived from the Java 6 SE API documentation to 
detect sentences that a developer must pay attention to when reading an API document~\cite{Robillard2015}.
In a similar manner, {\small DeMIBuD} uses a set of 154 discourse patterns to detect sentences 
relevant to understanding a bugs observed or expected behaviour and steps to reproduce it.





In  Stack Overflow posts, Nadi and Treude  extend the original set of patterns from Krec~\cite{Robillard2015} with two heuristics that ultimately aim to identify sentences that help a developer decide whether they want to read the post or skip over it~\cite{nadi2020}. 
Their heuristics expect that sentences express conditions about a subject and 
they find that no single heuristic is able to identify all of the sentences 
that humans indicated as useful to \texttt{json} manipulation tasks. 







\subsection{Summarization Approaches}



Extractive text summarization techniques are used in natural language artifacts in software engineering to
produce a summary of the artifact's content. These summaries aim to represent key information that may help a developer complete their task~\cite{Bavota2016}.
There are summarization techniques based on both supervised and unsupervised learning~\cite{moreno2017}
and one can summarize the entire content of an artifact
or content that relates to a specific input query, as in query-based summarization~\cite{Huang2018, Goldsteinet1999}.
We collectively discuss these techniques in this section. 






A number of summarization approaches target bug reports and GitHub issues, largely
focusing on key information within these artifacts. 
Rastkar and colleagues~\cite{Rastkar2010} use a supervised learning approach to summarize the content of bug reports showing that conversational features used to summarize emails~\cite{Murray2008}
can also be applied to bug reports.
Lotufo et al.~\cite{Lotufo2012} proposed an unsupervised summarization approach 
that automates the identification of sentences that a developer would first read when inspecting bug reports. While their approach outperforms Rastkar's, they discuss the need to generate more diverse summaries containing 
a bug's reported problem, possible workarounds, or discussed solution~\cite{Lotufo2012}.


% \paragraph{\textbf{PageRank-based Approaches.}}


Other summarization approaches are mostly based around variations of the PageRank~\cite{Page1999} or LexRank~\cite{Erkan2004} algorithms. 
These algorithms represent all the text in an artifact as a graph.
Then, they establish relationships (\textit{i.e.,} weighted edges in the graph) between different sentences (\textit{i.e.,} nodes in the graph) and select the nodes with highest weights as the most relevant ones.
A crucial step in building the algorithm's graph is in the definition of how to establish  relationship between nodes.
Early approaches~\cite{Lotufo2012, Jiang2017} 
use \ac{VSM}~\cite{Salton1975vsm} representations to compute how similar two sentences are
if the similarity scores between the vector representations of these sentences is above some threshold. 
With the emergence new \acs{NLP} techniques, 
tools such as AnswerBot~\cite{Huang2018} or {\small CROKAGE}~\cite{silva2019}
use different word embeddings~\cite{Mikolov2013, bojanowski2017FastText} to build their graphs.





% \paragraph{\textbf{Structured Data.}} 


While many of the approaches described above
largely rely on  lexical aspects in text, researchers have also made
use
of structured textual information in the artifacts~\cite{Ponzanelli2015, Treude2016, chen2016}. 
For example, recognizing the value of structured data available on Stack Overflow, Ponzanelli et al. 
proposed a summarization technique that mixes natural language text and structured data to produce more accurate summaries for Stack Overflow answers~\cite{Ponzanelli2015}. 
As another example, DeepSum~\cite{Li2018} pre-processes a bug report dividing sentences containing software elements, the reporter of the bug, and any other sentences in the bug report to produce summaries containing more diverse information.


% \paragraph{\textbf{Query-based Summarization.}} 


A smaller number of approaches have focused
on the problem that is the focus of this
thesis, namely identifying text in artifacts
related to a specific task-at-hand.
These approaches often apply to Stack Overflow 
as with AnswerBot's relevant and salient text selection algorithm~\cite{Xu2017}. AnswerBot poses the problem of finding task-relevant text 
as a query-based extractive summarization problem and it identifies relevant text based on 
the content of the text, how similar that content is with regards to a task and the structured data available on each of the answers in a Stack Overflow post (i.e., number of votes or whether an answer is the accepted answer).


\subsection{Machine Learning Approaches}


\acf{ML} approaches take the text of a natural language software artifact and identify 
the sentences likely relevant to a particular software task using \textit{unsupervised} or 
\textit{supervised learning} methods~\cite{zhang2005machine}.



Supervised learning approaches use a set of features and labeled data (which often requires significant manual effort) to train classifiers in detecting relevant sentences.
We have already presented supervised approaches that use text summarization (\textit{i.e.,}~\cite{Rastkar2010})
and there are also approaches that classify a tutorial fragment as relevant or not~\cite{Jiang2016b}.
While effective, supervised approaches may be outperformed by more light-weight approaches such as 
pattern matching~\cite{Bavota2016}.
As an example, Chaparro    
and colleagues 
compared their pattern matching approach ({\small DeMIBuD}) to a supervised version of {\small DeMIBuD} finding that the 
ML approach did not provide significant gains in the classification of relevant text in bug reports~\cite{Chaparro2017}.



Unsupervised learning approaches do not require labelled data and determine relevant sentences according to properties inferred from the data. DeepSum~\cite{Li2018} and Lotufo et al.'s~\cite{Lotufo2012} techniques are examples of unsupervised approaches in the scope of text summarization. Other unsupervised approaches (\textit{i.e.,} {\small FRAPT}~\cite{Jiang2017} or HoliRank~\cite{Ponzanelli2015, Ponzanelli2017}) are mostly based around variations of the PageRank~\cite{Page1999} or LexRank~\cite{Erkan2004} algorithms explained early in this thesis. 




The techniques that we have presented
are specific to one kind of artifact and use properties of the kind of
artifact. In contrast, the techniques we
investigate in this dissertation (Chapter~\ref{ch:identifying}) seek to identify 
task-relevant text across different artifact types.