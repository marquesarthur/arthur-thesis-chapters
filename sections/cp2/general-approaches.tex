

\section{Textual Approaches in Natural Language Software Artifacts}
\label{cp2:general-approaches}


This section provides a general overview of 
background information on automated textual approaches
applied to natural language software artifacts. 




\subsection{Pattern Matching Approaches}
\label{cp2:pattern-matching}


Regularities in the terms and in the syntactic structure of the 
might be automatically identified via pattern matching.
Pattern matching approaches use regular expressions describing a sequence of tokens that represent
the text to be identified~\cite{Bavota2016}. 
We describe two tools, Krec~\cite{Robillard2015}  and DeMIBuD~\cite{Chaparro2017}, that
illustrate how software engineering 
researchers use pattern matching in conjunction with the lexical or linguistic elements 
in the text to automatically identify 
text useful to certain software development activities.
    


Knowledge Recommender (Krec)~\cite{Robillard2015} 
is an example of a
 tool that uses lexical patterns to 
 automatically detect relevant text in  API documentation. 
Krec's premise is that relevant sentences contain a code element, such as a method or class signature.
These code elements are identifiable via regular expressions 
and Krec contains a catalog of 361 unique patterns 
that identify threats and directives on how to use some API element.
For example, Krec uses the pattern {\small \textit{$\{$may}, \textit{efficient}, \textit{code element regex$\}$}} 
to identify sentences giving instructions about an efficient way to 
perform some operation. 



{\small DeMIBuD} is a linguistic-based approach that 
automatically detect sentences discussing steps to reproduce 
a bug or the bug's expected behaviour~\cite{Chaparro2017}.
It uses a set of 154 discourse patterns
derived from nearly 3,000 bug reports 
to identify such sentences. 
For example, the pattern 
{\small \textit{$\{$subject}, \textit{should/shall (not)}, \textit{complement$\}$}}
captures common ways with which developers describe a system's expected behaviour
and empirical assessment of the patterns used by the tool has shown that it 
detects sentences of interest in bug reports with high accuracy.






Although the heuristics and regular expressions used in these and other studies~\cite{nadi2020, Maalej2013}
are lightweight and effective~\cite{Bavota2016}, 
pattern-matching approaches 
are often specific to certain kinds of domains and types of artifact~\cite{fucci2019}, 
limiting their use in the design of a generalizable technique, which is the main goal of this thesis.







\subsection{Machine Learning Approaches}
\label{cp2:machine-learning}


Regularities in the text, or in an artifact's meta-data, can also be 
engineered into features that \acf{ML} 
approaches can leverage to automatically identify and classify
text useful to certain software development activities. 


% where
% one can train a classifier
% to predict whether a sentence belongs (or not)
% to a certain class. 


Researchers pose the problem of identifying text 
in a natural language artifact 
as a binary classification problem. 
That is, the use of a number of features 
to predict whether the text is (or not)
relevant to some context~\cite{}.
In software engineering, 
binary classifiers have been used for,
for example, classify text that describes steps to reproduce a bug~\cite{Chaparro2016} or 
classify text that explains a certain API element, as when 
Petrosyan et al. used 
 sentence-level features
and meta-data features in a classifier 
that 
identifies explanations about an API element  in a web tutorial~\cite{Petrosyan2015}.




Other classification problems predict which class, out of many, some input text belongs to. 
This type of classification, often referred to as multinomial or multi-class classification, 
is of particular interest if 
we consider the taxonomies described in Section~\ref{cp2:text-in-se}.
For example, Arya et al. identified 16 categories of  information available
in open source GitHub issues (e.g., workarounds, solution discussion, task progress, etc.)~\cite{Arya2019}
and they proposed a multinomial classifier 
to automatically identify such categories.
% so that a developer could quickly find information pertaining a certain category. 








Although valuable, the cost and effort of hiring skilled workers to produce 
the labeled data for these and other supervised learning approaches
has been a major limitation 
to the usage of supervised learning 
methods in software engineering research~\cite{Arpteg2018, ferreira2021}. As an alternative,
researchers have also explored 
 unsupervised learning methods---\acs{ML} techniques that do not required training data---for the automatic 
identification of 
key information in natural language artifacts~\cite{}.





A common application of unsupervised learning in software engineering
considers the automatic generation of text summaries.
Most often, automatic summaries are produced 
using extractive techniques that select a subset of 
the sentences of an artifact which will compose the summary~\cite{a}.
Among other natural language artifacts,
extractive summarization techniques
have been applied to Stack Overflow posts~\cite{a}, coding tutorials~\cite{a},
or bug reports, as
when Lotufo et al. 
considered the kinds of sentences a developer would find relevant 
to understand a bug report when pressed with time~\cite{Lotufo2012},
% i.e., sentences with topics frequently discussed, sentences assessed by other sentences and 
% sentences that focus on the topics in the bug report's title and description,
and proposed an unsupervised summarization approach 
based on the PageRank algorithm~\cite{Page1999}
to identify these sentences. 




A second set of unsupervised methods focus on clustering data.
These techniques identify 
subsets in the data that have similar properties or features 
and techniques such as \acf{LDA}~\cite{blei2003latent}  have been use both to 
bootstrap the categorization of information in 
natural language artifacts and as part of tools that identify 
portions of the text in an artifact pertinent to some software activity. 
As an example of the former, 
 Allamanis and Sutton
applied \acs{LDA}
to gain insight into the types of questions 
asked on Stack Overflow~\cite{Allamanis2013}.
For the latter, tools such as FRAPT
use \acs{LDA} to identity topics in a web tutorial
and then extract sentences explaining API elements from each of the topics identified~\cite{Jiang2017}.



% Despite the significant 
% contributions brought by using machine learning methods, 
One substantial challenge inherent the supervised and 
unsupervised \acs{ML} approaches that we discussed
is that researchers must engineer which 
features their \acs{ML} technique will use~\cite{ferreira2021}.
Given the specificity and cost of engineering such features, 
\acs{ML} approaches have limitations similar to pattern 
matching approaches when we consider their use across 
different kinds of artifacts.




% For example, Rastkar et al. uses conversational features in 
% the text of a bug report to assist in determining which sentences 
% to include in the bug report's summary~\cite{Rastkar2010}
% while Petrosyan and colleagues use 
% linguistic and structural properties 
% in the text of API documents to identify key text 
% explaining API elements~\cite{Petrosyan2015}.




\subsection{Deep Learning Approaches}
\label{cp2:deep-learning}





In contrast to the human-engineered features,
\acf{DL} approaches allow the automatic extraction of features 
from training data~\cite{Deng2018, zhang2021deep},
which makes 
deep learning an interesting 
approach to
uncover regularities in the text of a natural language software artifact
that might not obvious or easily identified
by software engineering researchers.



% Deep learning has led to groundbreaking advancements in many 
% research areas (e.g., machine translation~\cite{lopez2008translation}) 
% and, 

Given the wide range of applications that both propose \acs{DL} models~\cite{} and that use 
these models for a certain purpose (e.g., machine translation~\cite{lopez2008translation}), 
we focus
on \acs{DL} applications in the 
software engineering domain~\cite{ferreira2021, li2018deep, watson2022}.
First we discuss neural embeddings and then, we present 
neural network models 
for the same range of problems 
that machine learning approaches apply to.






A common application of \acs{DL} in software engineering is the usage of neural, or word, embeddings~\cite{Mikolov2013}
for information retrieval purposes. 
Neural, or word, embeddings produce vector representations in a continuous space,
where words with similar meanings are typically close in the vector space model~\cite{harris1954distributional, mikolov2013efficient}. 
Researchers have found that word
embeddings mitigate lexical mismatches in the text found across different 
natural language software artifacts,
using them as a way to compare the semantic similarity of the text~\cite{mihalcea2006}.
Their use has improved many information retrieval task,
as shown by Ye et al.'s evaluation of word embeddings
for bug localization tasks~\cite{Ye2016}
or Huang and colleagues' study on 
the usage of word embeddings for API recommendation tasks~\cite{Huang2018}. 


% Due to how word embeddings assist the in identification 
% of similar text, they have also been widely used in unsupervised text 
% summarization approaches~\cite{Ponzanelli2017, Xu2017, Jiang2017}.
% \red{For example, AnswerBot's relevant and salient text selection algorithm~\cite{Xu2017}
% uses word embeddings to find which sentences in a Stack Overflow answer 
% are most similar to a question posed by a developer 
% and that should be included in the summary that the tool produces 
% to answer the developer's question. }





Provided that \acs{DL} address challenges originating from feature engineering, 
many other \acs{DL} studies in software engineering~\cite{ferreira2021,li2018deep, watson2022}
use neural network architectures 
in binary or multinomial classifiers as well as in extractive text summarization.
For example, Fucci et al. used a 
recurrent neural network (\acs{RNN}) with 
\acf{LSTM}~\cite{aa}
to identify the types of information available in 
API documentation~\cite{fucci2019} or when Li et al. used a stepped auto-encoder~\cite{aa}
to produced more accurate and diverse summaries 
for bug reports~\cite{li2018deep}.
State-of-the-art architectures such as \acf{BERT}
have been used for requirements traceability~\cite{Araujo2021}.
Nonetheless, these and other applications of deep learning 
to natural language software artifacts target specific
types or artifact and evaluating their 
usage across different types of artifacts 
is outside the scope of the studies we surveyed.



In Chapter~\ref{ch:identifying}, we consider 
how we can use 
word embeddings and \acs{BERT}-based \acs{DL}
model 
to automatically identify task-relevant text
in different kinds of natural language artifacts.

%  (BERT~\cite{Devlin2018Bert})
