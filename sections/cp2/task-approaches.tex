



\section{Task-specific Techniques}
\label{cp2:task-approaches}



Although certainly valuable, the techniques mentioned thus far
do not extract text tailored to 
a specific software task. In other words, they identify 
a set of sentences in an input artifact regardless of the developer's task. 
In contrast, this section focus on task-specific techniques.
% where core component to identifying information specific to a task 
% is a query which represents the task being performed~\cite{Bavota2016}. 
% This query can be explicitly formulated by a developer 
% or implicitly derived from the code or the context 
% in which a developer is working.

%  (as described in Section~\ref{cp2:unsupervised})
A small number of studies~\cite{Xu2017, silva2019, liu2019qapi} consider specific types of artifacts and
they formulate the problem of identifying task-specific text as 
a special case of query-based summarization~\cite{Goldsteinet1999}. That is, 
instead of producing a summary for the entire content of 
an artifact,
these studies first use information retrieval techniques
to identify a subset 
of sentences relevant to a task and then, 
a summary of these key sentences is produced.



AnswerBot~\cite{Xu2017} is an example of a query-based summarization tool.
It uses word embeddings to find which sentences in a Stack Overflow answer 
are most similar to a query representing a developer's task.
The tool uses semantic similarity in conjunction with 
Stack Overflow's meta-data to decide which text  it should include
in the summary that it will produce. 
A second query-based summarization tool by Liu et al. parses the content 
of API documents to build a knowledge graph of the information within this type of artifact. 
Their tool, KG-APISumm~\cite{liu2019qapi}, uses this graph 
to find nodes with text that matches the text of an input task, 
producing a task-specific summary for this type of artifact. 


These techniques
target one type of artifact and they also use an artifact's meta-data to 
assist in the automatic identification of task-relevant text, thus
limiting their use across the
many different kinds of artifacts developers encounter
daily in their work. 
For example, AnswerBot uses the number of votes an answer has to decide which text it will use to produce a summary~\cite{Xu2017}
while KG-APISumm uses 
code blocks, HTML anchor links and other properties which are exclusive to API documents to build the knowledge graph used as part of its summarization approach~\cite{liu2019qapi}.
We differ from these studies 
by not using assumptions 
specific to certain kinds of artifacts
and by considering how to identify task-relevant text 
in a more generalizable manner.





% As a result, these approaches are constrained to specific artifacts and do not 
% generalize to the different kinds of artifacts a developer might consult. 





% the text identified by these techniques might indeed 


% contain information relevant to a specific software task,
% a key component that distinguish the techniques 
%  is that they 




% We now consider applications that assist developers in 
% searching for artifacts that contain information 
% for a particular task
% and that automate the identification 
% of text likely relevant to a developer's task;




% We use the terms change task and task to refer to any change to a software system that serves a predefined purpose, such as a bug fix or an enhancement.


% In this section, we detail textual approaches that automate the identification 
% of text likely relevant to a developer's task. 



% Task-specific automated approaches 
% use many of the techniques discussed in Section~\ref{cp2:general-approaches}
% in conjunction with the text in a developer's task
%  or the code in a developer's workspace
% as potential ways  
% to identify task specific information.
% A seminal tool that exemplifies 
% how these sources assist in locating task specific information is
% Hipikat~\cite{Cubranic2005}.
% It takes a query explicitly prompted by a developer 
% or implicitly based on the code that the developer 
% has inspect and  
% it recommends artifacts from a project's archive 
% that are pertinent to the query.
% While Hipikat makes recommendations at the artifact level,  
% this approach could be fine-tuned to identify 
% parts of an artifact that are relevant~\cite{Cubranic2005}
% and other studies have since built upon this core idea. 
% For example, Deep Intellisense 
% uses pattern matching to 
% identify 
% artifacts containing text that 
% would assist a developer
% better understand the code that they inspect~\cite{Holmes2008}
% while 
% CueMeIn takes the code 
% a developer is currently editing and it
% automatically finds 
% excerpts from web tutorials 
% that might contain explanations 
% for the classes and methods 
% of the method being edited~\cite{sun2021}. 










% In Section~\ref{cp2:general-approaches}, we described 
% techniques for mining textual data. 
% These techniques identify text that might assist a developer 
% in performing certain activities such as finding 
% text describing how to use an API~\cite{Robillard2015} 
% or finding text with decisions made by members of a development 
% community~\cite{fu2021}.