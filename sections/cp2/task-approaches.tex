



\section{Task-specific Automated Textual Approaches}
\label{cp2:task-approaches}



In this section, we detail textual approaches that automate the identification 
of text likely relevant to a developer's task. 



Task-specific automated approaches 
use many of the techniques discussed in Section~\ref{cp2:general-approaches}
in conjunction with the text in a developer's task
 or the code in a developer's workspace
as potential ways  
to identify task specific information.
A seminal tool that exemplifies 
how these sources assist in locating task specific information is
Hipikat~\cite{Cubranic2005}.
It takes a query explicitly prompted by a developer 
or implicitly based on the code that the developer 
has inspect and  
it recommends artifacts from a project's archive 
that are pertinent to the query.
While Hipikat makes recommendations at the artifact level,  
this approach could be fine-tuned to identify 
parts of an artifact that are relevant~\cite{Cubranic2005}
and other studies have since built upon this core idea. 
For example, Deep Intellisense 
uses pattern matching to 
identify 
artifacts containing text that 
would assist a developer
better understand the code that they inspect~\cite{Holmes2008}
while 
CueMeIn takes the code 
a developer is currently editing and it
automatically finds 
excerpts from web tutorials 
that might contain explanations 
for the classes and methods 
of the method being edited~\cite{sun2021}. 






With regards to the content in a developer's task (as captured through a feature 
request or a bug report in an issue tracking system),
a second potential way to find text relevant to the task at hand is through query-based summarization~\cite{Goldsteinet1999}.
Query-based summarization allows 
producing a summary tailored to a particular query
and a small number of approaches have focused on
producing task-specific summaries~\cite{Xu2017, silva2019, liu2019qapi}.
For example, AnswerBot~\cite{Xu2017}, a state-of-the-art summarization tool, 
uses word embeddings to find which sentences in a Stack Overflow answer 
are most similar to a question  representing a developer's task.
The tool uses semantic similarity in conjunction with 
Stack Overflow's meta-data to decide which text should it include
in the summary that answers the developer's question. 





Task-specific techniques in related-work either consider a single type of artifact
or have assumptions that prevent using them across different kinds of artifacts 
in a generalizable manner. For example, 
Deep Intellisense was evaluated in the artifacts of a single organization, 
Microsoft, and the regular expressions that it uses might be unique to 
how this organization stores its data. As another example, 
query-based summarization approaches such as AnswerBot also use 
an artifact's meta-data and it may not be possible to apply it 
to artifacts that lack such meta-data. 
We differ from these studies 
by not using assumptions 
specific to certain kinds of artifacts
and by considering how to identify task-relevant text 
across different types of artifacts.



% The possible techniques that we explore in Chapter~\ref{ch:identifying} 
% build upon these concepts. We assume that 
% a set of artifacts pertinent to a task 
% is available and we use the content of a developer's task 
% to identify text relevant to that task
% in these pertinent artifacts.





% and researchers have extended this idea to  
% publicly available data, e.g., GitHub issues~\cite{Viviani2019}
% or pull requests~\cite{freire2021}. 






% Even though most of the summarization approaches that apply to natural language software artifacts 
% summarize an artifact on its whole~\cite{Rastkar2010, Murray2008, Lotufo2012, Ponzanelli2015},


% Summarization approaches often favour the diversity of information to be included in a summary~\cite{Carbonell1998,li2018deep}
% and thus, a summary might not contain all of the information needed 
% to correctly complete a software task.
% For example, if several relevant sentences discuss different yet similar content about 
% a particular technology, it is less likely that a summary will contain all of these sentences. 
% In contrast, the approaches that we explore in Chapter~\ref{ch:identifying}
% seek to identify the text that is most relevant to a task regardless 
% of its diversity.

