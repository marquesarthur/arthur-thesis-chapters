\section{Natural Language Software Artifacts}
\label{cp2:text-in-se}




Software engineering researchers
have investigated several questions on the nature of the text in software artifacts.
These studies have focused on the analysis of different kinds of artifacts and have deepened researchers' knowledge of the type of information available in them,
 which is often a first step towards the design of automated tools 
for identifying and extracting text from these artifacts~\cite{Arya2019, Maalej2013}.
In this section, we outline 
different types of investigations considered,
presenting seminal studies for each category.


\art{unsure if I need to have subsections here}

\subsubsection{Textual Analysis} 

A significant body of work has 
focused on the \textit{syntactic analysis} of the natural language text 
in software artifacts. An early example
is Ko and colleagues' analysis of bug report titles~\cite{Ko2006},
where they identify regularities in the text of nearly 200,000 bug report titles,
discussing how such regularities can 
assist in the automatic identification 
of information in bug reports. 
This study was followed by many others where 
software engineering researchers have 
observed other regularities both in bug reports~\cite{Rastkar2010, Chaparro2016}
and also in other types of artifacts, such as 
when Robillard  and Chhetri inspected 1,000 API elements in the Java SDK 6 
reference documentation~\cite{Maalej2013} or when 
Nadi and Treude examined 20 Stack Overflow posts~\cite{nadi2020}.
The syntactic analysis in these and other studies (e.g.,~\cite{chaparro2019} or~\cite{lin2019})
shows that regular expressions or 
linguistic patterns 
might be used to automatically extract 
relevant information from the text  
in the artifacts studied, as we explain in Section~\ref{cp2:pattern-matching}.

Although certainly valuable, these studies have focused on the analysis of 
text in a single type of artifact. In contrast, this thesis investigates 
text relevant to a task across different artifact types.





\subsubsection{Structural Analysis}

Several other studies have 
investigated structured data, or \textit{meta-data}, available in
natural language software artifacts~\cite{Ponzanelli2015}. In Section~\ref{cp1:example},
we have presented an example of structured data in a Stack Overflow post, 
i.e., the number of votes an answer has and whether it is the accepted answer.
Among other examples, we mention 
code blocks in API documentation or web tutorials 
and also stack traces or other fields included in a bug report~\cite{Davies2014, Breu2010},
which, for example, have been used to 
extract complementary 
information that assists a developer in understanding a bug report~\cite{bettenburg2008}.


Considering how several techniques and tools make use of an artifact's meta-data, 
researchers have also
investigated the frequency with which
meta-data is available~\cite{Davies2014, bettenburg2008makes, uddin2015} 
as well as how often developers update it~\cite{ahmad2018, dig2006, shi2011}.
Findings from these studies led 
software engineering researchers to discuss the promises and perils of 
using an artifact's meta-data~\cite{kalliamvakou2014, ahmad2018}.
For example, an accepted answer on Stack Overflow 
might not be the correct answer~\cite{wang2018}
and 
valuable information exists in elements not
associated with any meta-data~\cite{zhang2019so},
which we discussed as part of our 
scenario about finding information about Android notifications (Section~\ref{cp1:example}).


To better understand the impact of meta-data in finding information 
relevant to a task, Chapter~\ref{ch:identifying} 
examines how using (or not) meta-data available on 
Stack Overflow affects the automatic identification 
of task-relevant text.





\subsubsection{Semantic Analysis} 

Information within natural language artifacts 
serve different purposes and several other studies have
 proposed \textit{taxonomies} classifying the type of information
 available in the text of
 different kinds of natural language software artifacts.
For example, Di Sorbo et al. have found that 
text in development mailing lists can be classified according to the developers' intentions (e.g., feature request, solution proposal, etc.)~\cite{Sorbo2015}.
They have sampled 100 emails and identified that text for feature requests 
often contain expressions in the form of suggestions
(e.g., `\textit{we should add a new button}'), whereas solution proposals 
are often expressed in the form of attempts (e.g., `\textit{let's try a new method to compute cost}'),
suggesting that one can automate this classification to assist developers in finding 
certain type of information.
Other examples include Maalej and Robillard's taxonomy of patterns of knowledge in API documentation~\cite{Maalej2013}
or Arya et al. analysis of information types in Open Source issues~\cite{Arya2019}.



The taxonomies or categorizations presented in these studies are often based
on a need for access to the meaning of
sentences in the natural language text~\cite{berners2001, calero2006, witte2007}.
However, they originate from strenuous manual analysis 
of hundreds of artifacts and the information types identified 
might not extend to different kinds of artifacts.
In Chapter~\ref{ch:characterizing}, 
we use a general linguistic approach~\cite{fillmore1976frame} to
examine the meaning of the task-relevant text in 
API documents, GitHub issues and Stack Overflow posts.

