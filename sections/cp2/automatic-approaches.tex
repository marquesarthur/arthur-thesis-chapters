\section{Automatic Text Identification Approaches}
\label{cp2:automatic-approaches}



Information useful to a software task can be buried in  
non-intuitive blocks of text, making it difficult to discover~\cite{Robillard2015}.
In this section, we detail tools and approaches from related work
that seek to assist developers in 
identifying information within the natural language
text of software artifacts.


\gm{Need to convince this is a comprehensive survey.}
\art{Clarify this in the next meeting.}



\subsection{Pattern Matching Approaches}
\label{cp2:pattern-matching}


Pattern matching approaches rely on regular expressions describing a sequence of tokens that represent
 a relevant text fragment~\cite{Bavota2016}. Tokens can either represent words or linguistic elements 
extracted using \acf{NLP}.
    

Knowledge Recommender (Krec)~\cite{Robillard2015} 
and {\small DeMIBuD}~\cite{Chaparro2017} are examples 
of tools that use word patterns and linguistic patterns to 
detect relevant sentences in   API documentation and bug reports, respectively. 
These tools use a set of patterns derived from annotated data to identify relevant text. Krec  uses  361 unique patterns
to 
detect relevant sentences mentioning key aspects about API usage~\cite{Robillard2015}.
In turn, {\small DeMIBuD} uses a set of 154 discourse patterns to detect sentences 
relevant to understanding a bugs expected behaviour and steps to reproduce it,
which are essential to bug triaging tasks~\cite{Chaparro2017}.




% In Stack Overflow posts,
% Nadi and Treude have both applied the original set of patterns from Krec~\cite{Robillard2015} 
% and proposed heuristics that use conditional clauses (i.e., sentences with the word `\textit{if}')
% to identify text that help a developer 
% decide whether they want to carefully inspect a Stack Overflow posts or skip it. 



Although the heuristics and regular expressions used in these and other studies~\cite{nadi2020, Maalej2013}
are often light-weight and effective~\cite{Bavota2016}, 
pattern matching approaches are often specific to certain kinds of domain and 
types of artifact~\cite{fucci2019}, what limits
using them in a general manner. 
Our approach to automatically identify task-relevant text 
is different from these studies because it does not use syntactic properties of the text, 
instead we use semantic-based approaches to determine text of relevance to a task (Chapter~\ref{ch:identifying}).





\subsection{Summarization Approaches}
\label{cp2:summarization}



Extractive text summarization techniques are used in natural language artifacts in software engineering to
produce a summary of the artifact's content. 
Although general summarization approaches produce a summary of the entire content of an artifact, 
query-based summarization allows 
producing a summary tailored to address some topic (i.e., the query)~\cite{Goldsteinet1999}, which is a potential way to identify 
text relevant to a developer's task. 


Even though most of the summarization approaches that apply to natural language software artifacts 
summarize an artifact on its whole~\cite{Rastkar2010, Murray2008, Lotufo2012, Ponzanelli2015},
a small number of approaches have focused on
producing task specific summaries~\cite{Xu2017, silva2019}.
These approaches pose the problem of finding task-relevant text 
as a query-based summarization problem and
state-of-the-art tools such as AnswerBot~\cite{Xu2017}
identify relevant text in Stack Overflow posts 
based on 
the content of the text, how similar that content is with regards to a input query (i.e., task)
and the structured data available on each of the answers in a post.



Although we refrain from using structural data 
or assumptions on the content of an artifact (as done by AnswerBot)
because this might hinder the design of a
generalizable technique, in 
Chapter~\ref{ch:identifying} 
we compare the techniques that we explore in this thesis
 to  the state-of-the-art
 in query-based extractive summarization. 



\subsection{Machine Learning Approaches}
\label{cp2:machine-learning}


\acf{ML} approaches take the text of a natural language software artifact and identify 
the sentences likely relevant to a particular software task using \textit{supervised} or 
\textit{unsupervised learning} methods~\cite{zhang2005machine}.



Supervised learning approaches use a set of features and labeled data
 to train classifiers with the goal of identifying sentences relevant to 
 certain software task.
Most of the supervised approaches of interest to us  
consider the automatic classification of text according to the taxonomies 
detailed in Section~\ref{cp2:text-in-se}.
For the types of artifacts that these techniques apply to, 
the automatic classification of the 
 content of the text 
might assist a developer 
in finding information that is pertinent to their task~\cite{fucci2019, Arya2019}.
% Other approaches identify relevant 
% parts of software tutorials~\cite{Jiang2016b}
% or API documents~\cite{fucci2019, Maalej2013},
% but despite their value, 
Although valuable, the cost and effort of hiring skilled workers to produce 
the labeled data for these and other supervised learning approaches
has been a major limitation 
to the usage of supervised learning 
methods in the software engineering field~\cite{Arpteg2018}.


% DeepSum~\cite{Li2018} and

Unsupervised learning approaches do not require labelled data and determine 
relevant sentences according to properties inferred from the data. 
Lotufo et al.'s~\cite{Lotufo2012} adaptation of the LexRank algorithm~\cite{Erkan2004} is an example of 
an unsupervised approach that identify key information in bug reports
and many other studies (\textit{e.g.,}~\cite{Jiang2017, Ponzanelli2015,  Ponzanelli2017}) have used 
variations of LexRank~\cite{Erkan2004} to identify key information in a natural
language software artifact.
LexRank and its base algorithm, PageRank~\cite{Page1999}, represent all the text in an artifact as a graph.
Then, they establish relationships (\textit{i.e.,} weighted edges in the graph) 
between different sentences (\textit{i.e.,} nodes in the graph) 
and select the nodes with highest weights as the most relevant ones.
A crucial step in building the graph is in  
establishing  relationships between nodes
and early approaches~\cite{Lotufo2012, Jiang2017} 
have used \ac{VSM}~\cite{Salton1975vsm} 
for this purpose while more modern ones~\cite{Huang2018, silva2019}
have used word embeddings~\cite{Mikolov2013, bojanowski2017FastText},
which we detail in Section~\ref{cp2:deep-learning}.
% To identify key sentences using these techniques, one must 
% assume that certain elements will be cross-referenced 
% or mentioned multiple types in a document, 
% what is only common in certain types of software artifacts, e.g., mailing lists~\cite{Bacchelli2012}.




One substantial challenge of standard machine learning
approaches is that researchers must engineer which 
features or properties of the text to use~\cite{ferreira2021}.
For example, Rastkar et al. uses conversational features in 
the text of a bug report to assist in determining which sentences 
to include in the bug report's summary~\cite{Rastkar2010}
while Petrosyan and colleagues use 
linguistic and structural properties 
in the text of API documents to identify key text 
explaining API elements~\cite{Petrosyan2015}.
Given the specificity of such features, 
researchers have questioned the generalizability
of standard \acs{ML} approaches~\cite{Xiao2018, fucci2019}
and they have sought aprroches that address these 
limitations, as detailed in the next section.






\subsection{Deep Learning Approaches}
\label{cp2:deep-learning}




%  through a series of mathematical transformations

In contrast to the human-engineered features,
\acf{DL} approaches allow the automatic extraction of features 
from training data~\cite{Deng2018, zhang2021deep}.
Deep learning has led to groundbreaking advancements in many 
research areas (e.g., machine translation~\cite{lopez2008translation}) 
and, given its wide range of applications, 
we focus
on its usage in natural language text appearing in software engineering artifacts~\cite{ferreira2021, li2018deep, watson2022}.






A common application of \acs{DL} in software engineering is the usage of neural, or word, embeddings~\cite{Mikolov2013}
for information retrieval purposes. 
Software engineering researchers have identified that the text 
in software task 
often differs from the text in the artifacts that are related to that tasks. 
These so-called \textit{lexical mismatches}~\cite{Ye2016, Huang2018} 
 make it difficult to identify information of interest 
to a task and researchers have found that word
embeddings mitigate lexical gaps between the text of different software artifacts,
what improves retrieval of information useful to a developer's task~\cite{Ye2016}. 


Neural, or word, embeddings produce vector representations in a continuous space,
where words with similar meanings are typically close in the vector space model~\cite{harris1954distributional, mikolov2013efficient}. 
Their usage has allowed researchers to improve 
the identification API elements pertinent to a programming task~\cite{Ye2016} 
or to more accurately assess the quality of the content in bug reports~\cite{chaparro2019}.
Word embeddings have become a common way 
to compare the semantic similarity of the text~\cite{mihalcea2006},
being applied in query-based summarization techniques such as 
AnswerBot~\cite{Xu2017}
or PageRank-based approaches such as HoliRank~\cite{Ponzanelli2017}.



Many other \acs{DL} studies in software engineering~\cite{ferreira2021,li2018deep, watson2022}
use neural network architectures 
in a variety of software engineering tasks, including
code comprehension~\cite{allamanis2015, mi2018}, 
community forum analysis~\cite{Lin2018, wang2019}, 
or requirements traceability~\cite{chen2019, guo2017}.
Although effective, these applications of deep learning
target specific
types of artifacts and the ones 
that target more than one type 
often comprise a natural language artifact 
paired with source code~\cite{watson2022}. 
In contrast, 
we consider how we can use a \acs{DL}
model to automatically 
identify text relevant to a software task
in different kinds of natural language artifacts (Chapter~\ref{ch:identifying}).

