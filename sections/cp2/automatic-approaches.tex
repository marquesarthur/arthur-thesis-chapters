\section{Automatic Text Identification Approaches}
\label{cp2:text-approaches}



Information useful to a software task can be buried in irrelevant text or attached to 
non-intuitive blocks of text, making it difficult to discover~\cite{Robillard2015}.
In this section, we detail tools and approaches from related work
that seek to assist developers in 
identifying information within the natural language
text of software artifacts.





\subsection{Pattern Matching Approaches}
\label{cp2:pattern-matching}


Pattern matching approaches rely on regular expressions describing a sequence of tokens that represent
 a relevant text fragment~\cite{Bavota2016}. Tokens can either represent words or linguistic elements 
extracted using \acf{NLP}.
    
    
As examples  of pattern matching approaches,  {\small DeMIBuD}~\cite{Chaparro2017}
 and Knowledge Recommender (Krec)~\cite{Maalej2013, Robillard2015} are tools that detect relevant sentences in bug reports and API documentation, respectively. 
These tools derive a set of patterns from annotated data and use them as part of heuristics 
that identify relevant text. Krec  uses  361 unique patterns
to 
detect relevant sentences mentioning a 
code element (e.g., a class or method name) in API documentation~\cite{Robillard2015}.
In a similar manner, {\small DeMIBuD} uses a set of 154 discourse patterns to detect sentences 
relevant to understanding a bugs observed or expected behaviour and steps to reproduce it,
which are essential to bug triaging tasks.




In Stack Overflow posts,
Nadi and Treude~\cite{nadi2020} have both applied the original set of patterns from Krec~\cite{Robillard2015} 
and proposed heuristics that rely on the conditional clauses (i.e., sentences with the word `\textit{if}')
to identify text that help a developer 
decide whether they want to carefully inspect a Stack Overflow posts or skip it. 



Although the heuristics and regular expressions used in the aforementioned studies 
are often light-weight and effective~\cite{Bavota2016, Maalej2013}, 
pattern matching approaches are specific to certain kinds of domain and 
types of artifact~\cite{fucci2019}, what limits
using them in a general approach.





\subsection{Summarization Approaches}
\label{cp2:summarization}



Extractive text summarization techniques are used in natural language artifacts in software engineering to
produce a summary of the artifact's content. 
A summary represents key information that may help a developer complete their task~\cite{Bavota2016}.
There are summarization techniques based on both supervised and unsupervised learning~\cite{moreno2017}
and one can summarize the entire content of an artifact
or content specific input query, as in \textit{query-based} summarization~\cite{Huang2018, Goldsteinet1999}.




A number of summarization approaches target bug reports and GitHub issues, largely
focusing on identifying key information within these artifacts. 
Rastkar and colleagues~\cite{Rastkar2010} use a supervised learning approach to summarize the content 
of bug reports showing that conversational features used to summarize emails~\cite{Murray2008}
can also be applied to bug reports while
Lotufo et al.~\cite{Lotufo2012} proposed an unsupervised summarization approach 
that automates the identification of sentences that a developer would first read when
a inspecting bug report.



While many of the approaches described above
largely rely on  lexical aspects in text, researchers have also made use
of structured textual information in the artifacts~\cite{Ponzanelli2015, Treude2016, chen2016}. 
For example, Ponzanelli et al. 
proposed a summarization technique that mixes natural language text and structured data 
available on Stack Overflow
to produce more accurate summaries for Stack Overflow answers~\cite{Ponzanelli2015}. 
As another example, DeepSum~\cite{Li2018} pre-processes a bug report dividing sentences 
containing software elements, the reporter of the bug, and any other sentences 
in the bug report to produce summaries containing more diverse information.




A smaller number of summarization approaches have focused on
producing task specific summaries~\cite{Xu2017, silva2019}.
These approaches pose the problem of finding task-relevant text 
as a query-based extractive summarization problem and
tools such as AnswerBot~\cite{Xu2017}
identify relevant text in Stack Overflow posts 
based on 
the content of the text, how similar that content is with regards to a input query (i.e., task)
and the structured data available on each of the answers in a Stack Overflow post 
(i.e., number of votes or whether an answer is the accepted answer).
As the state-of-the-art, Chapter~\ref{ch:identifying}
compares AnswerBot to the techniques that we explore in this thesis.



\subsection{Machine Learning Approaches}
\label{cp2:machine-learning}


\acf{ML} approaches take the text of a natural language software artifact and identify 
the sentences likely relevant to a particular software task using \textit{supervised} or 
\textit{unsupervised learning} methods~\cite{zhang2005machine}.



Supervised learning approaches use a set of features and labeled data
 to train classifiers with the goal of identifying sentences relevant to 
 certain software activity.
We have already presented supervised approaches that use text summarization (\textit{i.e.,}~\cite{Rastkar2010})
and there are also approaches that identify relevant 
parts of software tutorials~\cite{Jiang2016b}
or API documents~\cite{fucci2019, Maalej2013}.
Despite their value, 
the cost and effort of hiring skilled workers to produce 
labeled data in software engineering artifacts 
has been a major limitation 
to the usage of supervised learning 
methods~\cite{Arpteg2018}.





Unsupervised learning approaches do not require labelled data and determine 
relevant sentences according to properties inferred from the data. 
DeepSum~\cite{Li2018} and Lotufo et al.'s~\cite{Lotufo2012} techniques are examples of 
unsupervised approaches in the scope of text summarization. 



Other unsupervised approaches 
(\textit{i.e.,} {\small FRAPT}~\cite{Jiang2017} or HoliRank~\cite{Ponzanelli2015, Ponzanelli2017})
are mostly based around variations of the PageRank~\cite{Page1999} or LexRank~\cite{Erkan2004} algorithms. 
These algorithms represent all the text in an artifact as a graph.
Then, they establish relationships (\textit{i.e.,} weighted edges in the graph) 
between different sentences (\textit{i.e.,} nodes in the graph) 
and select the nodes with highest weights as the most relevant ones.
A crucial step in building the graph is in the definition of 
how to establish  relationships between nodes.
Early approaches~\cite{Lotufo2012, Jiang2017} 
use \ac{VSM}~\cite{Salton1975vsm} 
for this purpose while more modern ones~\cite{Huang2018, silva2019}
use different word embeddings~\cite{Mikolov2013, bojanowski2017FastText},
which we detail in Section~\ref{cp2:deep-learning}.
To identify key sentences using these techniques, one must 
assume that certain elements will be cross-referenced 
or mentioned multiple types in a document, 
what is only common in certain types of software artifacts, e.g., mailing lists~\cite{Bacchelli2012}.








\subsection{Deep Learning Approaches}
\label{cp2:deep-learning}



One substantial challenge of standard \acf{ML}
approaches is that researchers must engineer which 
features or properties of the text to use~\cite{ferreira2021}.
For example, Rastkar et al. uses conversational features in 
the text of a bug report to assist in determining which sentences 
to include in the bug report's summary~\cite{Rastkar2010}
while Petrosyan and colleagues use 
linguistic and structural properties 
in the text of API documents to identify key text 
explaining API elements~\cite{Petrosyan2015}.
Given the specificity of such features, 
researchers have questioned the generalizability
of standard \acs{ML} approaches~\cite{Xiao2018, fucci2019}.



In contrast to the human-engineered features,
\acf{DL} approaches allow the automatic extraction of features 
from training data through a series of mathematical transformations~\cite{Deng2018, zhang2021deep}.
Deep learning has led to groundbreaking advancements in many 
research areas (e.g., machine translation~\cite{lopez2008translation}) 
and, given its wide range of applications, 
we focus
on its usage in natural language text appearing in software engineering artifacts~\cite{ferreira2021, li2018deep, watson2022}.









Software engineering researchers have identified that the text 
in software task 
often differs from the text in the artifacts that are related to that tasks~\cite{Huang2018}. 
These so-called \textit{lexical mismatches}~\cite{Ye2016} 
 make it difficult to identify information of interest 
to a task and a number of studies have used \acs{DL}
neural embeddings~\cite{Mikolov2013} to bridge lexical gaps between the text of different software artifacts. 


Neural, or word, embeddings produce vector representations in a continuous space,
where words with similar meanings are typically close in the vector space model~\cite{harris1954distributional, mikolov2013efficient}. 
Their usage has allowed researchers to improve 
the identification API elements pertinent to a programming task~\cite{Ye2016} 
or to more accurately assess the quality of the content in bug reports~\cite{chaparro2019}.
Word embeddings have become a common way 
to compare the semantic similarity of the text~\cite{mihalcea2006},
being applied in query-based summarization techniques such as 
AnswerBot~\cite{Xu2017}
or PageRank-based approaches such as HoliRank~\cite{Ponzanelli2017}.



Many other \acs{DL} studies in software engineering~\cite{ferreira2021,li2018deep, watson2022}
use neural network architectures 
in a variety of software engineering tasks, including
code comprehension~\cite{allamanis2015, mi2018}, community forum analysis~\cite{Lin2018, wang2019}, or requirements traceability~\cite{chen2019, guo2017}.
DeepSum~\cite{Li2018}, which we described earlier, is an example of a 
summarization approach that uses an encoder-decoder architecture~\cite{cho2014-encoder-decoder}  to identify 
sentences of interest in bug reports. As another example,
Xi et al.~\cite{Xia2017} use a \ac{CNN}~\cite{krizhevsky2012CNN} 
to identify sentences in GitHub issues discussing topics such as 
problem discovery, solution proposal, or feature requests.



Few studies in software engineering have considered more modern neural networks~\cite{watson2022}
able to establish relationships not only between words but also sentence pairs (e.g., \acs{BERT}~\cite{Devlin2018Bert}). 
These models might assist in determining 
implicit relationships between the text in a task and the text in a relevant sentence within a software artifact.
As such, Chapter~\ref{ch:identifying} describes how we use \acs{BERT} for automatically 
identifying text relevant to a software task.
