\section{Automatic Text Identification Approaches}
\label{cp2:automatic-approaches}



Information useful to a software task in a natural language artifact can be buried in  
non-intuitive blocks of text, making it difficult to discover~\cite{Robillard2015}.
In this section, we detail tools and approaches from related work
that seek to automatically identify such information.


\gm{Need to convince this is a comprehensive survey.}
\art{Clarify this in the next meeting.}



\subsection{Pattern Matching Approaches}
\label{cp2:pattern-matching}


Pattern matching approaches rely on regular expressions describing a sequence of tokens that represent
 a relevant text fragment~\cite{Bavota2016}. Tokens can either represent words or linguistic elements 
extracted using \acf{NLP}.
    

Knowledge Recommender (Krec)~\cite{Robillard2015} 
is an example of a
 tool that uses word patterns to 
 automatically detect relevant sentences in  API documentation. 
It contains 361 unique patterns derived from annotated the Java reference documentation
where patterns such as {\small \textit{$\{$may}, \textit{efficient}, \textit{code element$\}$}} could be used to identify sentences giving instructions about efficient ways to 
use some API element. 

As an example of an approach based on linguistic patterns,
{\small DeMIBuD}~\cite{Chaparro2017} uses a set of 154 discourse patterns
derived from nearly 3,000 bug reports 
to
automatically detect sentences discussing steps to reproduce 
a bug or the bug's expected behaviour.
For example, the pattern 
{\small \textit{$\{$subject}, \textit{should/shall (not)}, \textit{complement$\}$}}
captures common ways with which developers describe a system's expected behaviour~\cite{Chaparro2017}.





Although the heuristics and regular expressions used in these and other studies~\cite{nadi2020, Maalej2013}
are often light-weight and effective~\cite{Bavota2016}, 
pattern-matching approaches 
are often specific to certain kinds of domains and types of artifact~\cite{fucci2019}, which limits
using them in a general manner. 
Our approach to identifying task-relevant text 
is different from the approaches in these studies because we do not use syntactic properties of the text, 
instead we rely on the semantics 
of words and sentences to determine the text likely relevant to a task (Chapter~\ref{ch:identifying}).





\subsection{Summarization Approaches}
\label{cp2:summarization}



Extractive text summarization techniques 
produce a summary of an artifact's content
and while general summarization approaches produce a summary of the entire content of an artifact, 
query-based summarization allows 
producing a summary tailored to address some specific topic (i.e., query)~\cite{Goldsteinet1999}, which is a potential way to identify 
text relevant to a developer's task. 


Even though most of the summarization approaches that apply to natural language software artifacts 
summarize an artifact on its whole~\cite{Rastkar2010, Murray2008, Lotufo2012, Ponzanelli2015},
a small number of approaches have focused on
producing task-specific summaries~\cite{Xu2017, silva2019}.
For example, 
 AnswerBot~\cite{Xu2017},
 a state-of-the-art summarization tool, 
identifies text
in Stack Overflow posts 
based on 
the content of the text, how similar that content is with regards to an input query (i.e., task)
and the structured data available on each of the answers in a post.



Summarization approaches often favour the diversity of information to be included in a summary~\cite{Carbonell1998,li2018deep}
and thus, a summary might not contain all of the information needed 
to correctly complete a software task.
For example, if several relevant sentences discuss different yet similar content about 
a particular technology, it is less likely that a summary will contain all of these sentences. 
In contrast, the approaches that we explore in Chapter~\ref{ch:identifying}
seek to identify the text that is most relevant to a task regardless 
of its diversity.



\red{~\cite{liu2019qapi}}


\subsection{Machine Learning Approaches}
\label{cp2:machine-learning}


\acf{ML} approaches take the text of a natural language software artifact and identify 
the sentences likely relevant to a particular software task using \textit{supervised} or 
\textit{unsupervised learning} methods~\cite{zhang2005machine}.



Supervised learning approaches use a set of features and labelled data
 to train classifiers with the goal of identifying sentences relevant to 
 certain software tasks.
Most of the supervised approaches of interest to us  
consider the automatic classification of text according to the taxonomies 
detailed in Section~\ref{cp2:text-in-se}.
For the types of artifacts that these techniques apply, 
the automatic classification of the 
 content of the text 
might assist a developer 
in finding information that is pertinent to their task~\cite{fucci2019, Arya2019}.
Although valuable, the cost and effort of hiring skilled workers to produce 
the labelled data for these and other supervised learning approaches
has been a major limitation 
to the usage of supervised learning 
methods in the software engineering field~\cite{Arpteg2018, ferreira2021}.



Unsupervised learning approaches do not require labelled data and determine 
relevant sentences according to properties inferred from the data/features available. 
Lotufo et al.'s~\cite{Lotufo2012} adaptation of the LexRank algorithm~\cite{Erkan2004} is an example of 
an unsupervised approach that identifies key information in bug reports
and many other studies (\textit{e.g.,}~\cite{Jiang2017, Ponzanelli2015,  Ponzanelli2017}) have used 
variations of LexRank~\cite{Erkan2004} to identify key information in natural
language software artifacts.
LexRank and its base algorithm, PageRank~\cite{Page1999}, represent all the text in an artifact as a graph.
Then, they establish relationships (\textit{i.e.,} weighted edges in the graph) 
between different sentences (\textit{i.e.,} nodes in the graph) 
and select the nodes with the highest weights as the most relevant ones.
A crucial step in building the graph is in  
establishing  relationships between nodes
and early approaches~\cite{Lotufo2012, Jiang2017} 
have used \ac{VSM}~\cite{Salton1975vsm} 
for this purpose while more modern ones~\cite{Huang2018, silva2019}
have used word embeddings~\cite{Mikolov2013, bojanowski2017FastText},
which we detail in Section~\ref{cp2:deep-learning}.




One substantial challenge of standard machine learning
approaches is that researchers must engineer which 
features or properties of the text to use~\cite{ferreira2021}.
For example, Rastkar et al. uses conversational features in 
the text of a bug report to assist in determining which sentences 
to include in the bug report's summary~\cite{Rastkar2010}
while Petrosyan and colleagues use 
linguistic and structural properties 
in the text of API documents to identify key text 
explaining API elements~\cite{Petrosyan2015}.
Given the specificity and cost of engineering such features, 
researchers have sought approaches, such as deep learning, to address these 
limitations. 






\subsection{Deep Learning Approaches}
\label{cp2:deep-learning}





In contrast to the human-engineered features,
\acf{DL} approaches allow the automatic extraction of features 
from training data~\cite{Deng2018, zhang2021deep}.
Deep learning has led to groundbreaking advancements in many 
research areas (e.g., machine translation~\cite{lopez2008translation}) 
and, given its wide range of applications, 
we focus
on its usage in natural language text appearing in software engineering artifacts~\cite{ferreira2021, li2018deep, watson2022}.






A common application of \acs{DL} in software engineering is the usage of neural, or word, embeddings~\cite{Mikolov2013}
for information retrieval purposes. 
Software engineering researchers have identified that the text 
in a software task 
often differs from the text in the artifacts related to that task. 
These so-called \textit{lexical mismatches}~\cite{Ye2016, Huang2018} 
 make it difficult to identify information of interest 
to a task and researchers have found that word
embeddings mitigate such lexical gaps,
what improves retrieval of information useful to a developer's task
across different software artifacts~\cite{Ye2016}. 


Neural, or word, embeddings produce vector representations in a continuous space,
where words with similar meanings are typically close in the vector space model~\cite{harris1954distributional, mikolov2013efficient}. 
Their usage has allowed researchers to improve 
the identification API elements pertinent to a programming task~\cite{Ye2016} 
or to more accurately assess the quality of the content in bug reports~\cite{chaparro2019}.
Word embeddings have become a common way 
to compare the semantic similarity of the text~\cite{mihalcea2006},
being applied in query-based summarization techniques such as 
AnswerBot~\cite{Xu2017}
or PageRank-based approaches such as HoliRank~\cite{Ponzanelli2017}.
In Chapter~\ref{ch:identifying}, we describe how we use 
word embeddings to automatically identify task-relevant text.



Many other \acs{DL} studies in software engineering~\cite{ferreira2021,li2018deep, watson2022}
use neural network architectures 
in a variety of software engineering tasks, including
code comprehension~\cite{allamanis2015, mi2018}, 
community forum analysis~\cite{Lin2018, wang2019}, 
or requirements traceability~\cite{chen2019, guo2017}.
Although effective, these applications of deep learning
target specific
types of artifacts and the ones 
that target more than one type 
often comprise a natural language artifact 
paired with source code~\cite{watson2022}. 
We consider how we can use a \acs{DL}
model to automatically 
identify text relevant to a software task
in different kinds of natural language artifacts (Chapter~\ref{ch:identifying}).

