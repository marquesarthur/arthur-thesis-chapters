\section{Automatic Text Identification Approaches}
\label{cp2:automatic-approaches}



Information useful to a software task can be buried in  
non-intuitive blocks of text, making it difficult to discover~\cite{Robillard2015}.
In this section, we detail tools and approaches from related work
that seek to assist developers in 
identifying information within the natural language
text of software artifacts.


\art{Question to Gail: how do I convince that this is a comprehensive survey?}



\subsection{Pattern Matching Approaches}
\label{cp2:pattern-matching}


Pattern matching approaches rely on regular expressions describing a sequence of tokens that represent
 a relevant text fragment~\cite{Bavota2016}. Tokens can either represent words or linguistic elements 
extracted using \acf{NLP}.
    

Knowledge Recommender (Krec)~\cite{Robillard2015} 
and {\small DeMIBuD}~\cite{Chaparro2017} are examples 
of tools that use words and linguistic patterns to 
detect relevant sentences in bug reports and API documentation, respectively. 
These tools use a set of patterns derived from annotated data to identify relevant text. Krec  uses  361 unique patterns
to 
detect relevant sentences mentioning a 
code element (e.g., a class name or method signature) in API documentation~\cite{Robillard2015}.
Likewise, {\small DeMIBuD} uses a set of 154 discourse patterns to detect sentences 
relevant to understanding a bugs expected behaviour and steps to reproduce it,
which are essential to bug triaging tasks.




% In Stack Overflow posts,
% Nadi and Treude have both applied the original set of patterns from Krec~\cite{Robillard2015} 
% and proposed heuristics that use conditional clauses (i.e., sentences with the word `\textit{if}')
% to identify text that help a developer 
% decide whether they want to carefully inspect a Stack Overflow posts or skip it. 



Although the heuristics and regular expressions used in these and other studies (e.g.,~\cite{nadi2020})
are often light-weight and effective~\cite{Bavota2016, Maalej2013}, 
pattern matching approaches are specific to certain kinds of domain and 
types of artifact~\cite{fucci2019}, what limits
using them in a general manner. 
Our approach to automatically identify task-relevant text 
is different from these studies because it does not use syntactic properties of the text, 
instead we use semantic-based approaches to determine text of relevance to a task (Chapter~\ref{ch:identifying}).





\subsection{Summarization Approaches}
\label{cp2:summarization}



Extractive text summarization techniques are used in natural language artifacts in software engineering to
produce a summary of the artifact's content. 
Although general summarization approaches summarize the entire content of an artifact, 
query-based summarization~\cite{Goldsteinet1999} allows 
producing a summary specific to a input query, which is a potential way to identify 
text relevant to a developer's task. 


Even though most of the summarization approaches that apply to natural language artifacts 
summarize an artifact on its whole~\cite{Rastkar2010, Murray2008, Lotufo2012, Ponzanelli2015},
a small number of approaches have focused on
producing task specific summaries~\cite{Xu2017, silva2019}.
These approaches pose the problem of finding task-relevant text 
as a query-based summarization problem and
state-of-the-art tools such as AnswerBot~\cite{Xu2017}
identify relevant text in Stack Overflow posts 
based on 
the content of the text, how similar that content is with regards to a input query (i.e., task)
and the structured data available on each of the answers in a Stack Overflow post 
(i.e., number of votes or whether an answer is the accepted answer).



Although we refrain from using structural data 
or assumptions on the content of an artifact (as done by AnswerBot)
because this might hinder the design of a
generalizable technique, in 
Chapter~\ref{ch:identifying} 
we compare the techniques that we explore in this thesis
 to  the state-of-the-art
 in query-based extractive summarization. 



\subsection{Machine Learning Approaches}
\label{cp2:machine-learning}


\acf{ML} approaches take the text of a natural language software artifact and identify 
the sentences likely relevant to a particular software task using \textit{supervised} or 
\textit{unsupervised learning} methods~\cite{zhang2005machine}.



Supervised learning approaches use a set of features and labeled data
 to train classifiers with the goal of identifying sentences relevant to 
 certain software task.
We have already presented supervised approaches that use text summarization (\textit{i.e.,}~\cite{Rastkar2010})
and there are also approaches that identify relevant 
parts of software tutorials~\cite{Jiang2016b}
or API documents~\cite{fucci2019, Maalej2013}.
Despite their value, 
the cost and effort of hiring skilled workers to produce 
labeled data in software engineering artifacts 
has been a major limitation 
to the usage of supervised learning 
methods~\cite{Arpteg2018}.





Unsupervised learning approaches do not require labelled data and determine 
relevant sentences according to properties inferred from the data. 
DeepSum~\cite{Li2018} and Lotufo et al.'s~\cite{Lotufo2012} techniques are examples of 
unsupervised approaches in the scope of text summarization. 



Other unsupervised approaches 
(\textit{e.g.,}~\cite{Jiang2017, Ponzanelli2015,  Ponzanelli2017})
are mostly based around variations of the PageRank~\cite{Page1999} or LexRank~\cite{Erkan2004} algorithms. 
These algorithms represent all the text in an artifact as a graph.
Then, they establish relationships (\textit{i.e.,} weighted edges in the graph) 
between different sentences (\textit{i.e.,} nodes in the graph) 
and select the nodes with highest weights as the most relevant ones.
A crucial step in building the graph is in  
establishing  relationships between nodes.
Early approaches~\cite{Lotufo2012, Jiang2017} 
use \ac{VSM}~\cite{Salton1975vsm} 
for this purpose while more modern ones~\cite{Huang2018, silva2019}
use different word embeddings~\cite{Mikolov2013, bojanowski2017FastText},
which we detail in Section~\ref{cp2:deep-learning}.
% To identify key sentences using these techniques, one must 
% assume that certain elements will be cross-referenced 
% or mentioned multiple types in a document, 
% what is only common in certain types of software artifacts, e.g., mailing lists~\cite{Bacchelli2012}.








\subsection{Deep Learning Approaches}
\label{cp2:deep-learning}



One substantial challenge of standard \acf{ML}
approaches is that researchers must engineer which 
features or properties of the text to use~\cite{ferreira2021}.
For example, Rastkar et al. uses conversational features in 
the text of a bug report to assist in determining which sentences 
to include in the bug report's summary~\cite{Rastkar2010}
while Petrosyan and colleagues use 
linguistic and structural properties 
in the text of API documents to identify key text 
explaining API elements~\cite{Petrosyan2015}.
Given the specificity of such features, 
researchers have questioned the generalizability
of standard \acs{ML} approaches~\cite{Xiao2018, fucci2019}.



In contrast to the human-engineered features,
\acf{DL} approaches allow the automatic extraction of features 
from training data through a series of mathematical transformations~\cite{Deng2018, zhang2021deep}.
Deep learning has led to groundbreaking advancements in many 
research areas (e.g., machine translation~\cite{lopez2008translation}) 
and, given its wide range of applications, 
we focus
on its usage in natural language text appearing in software engineering artifacts~\cite{ferreira2021, li2018deep, watson2022}.









Software engineering researchers have identified that the text 
in software task 
often differs from the text in the artifacts that are related to that tasks~\cite{Huang2018}. 
These so-called \textit{lexical mismatches}~\cite{Ye2016} 
 make it difficult to identify information of interest 
to a task and a number of studies have used \acs{DL}
neural embeddings~\cite{Mikolov2013} to bridge lexical gaps between the text of different software artifacts. 


Neural, or word, embeddings produce vector representations in a continuous space,
where words with similar meanings are typically close in the vector space model~\cite{harris1954distributional, mikolov2013efficient}. 
Their usage has allowed researchers to improve 
the identification API elements pertinent to a programming task~\cite{Ye2016} 
or to more accurately assess the quality of the content in bug reports~\cite{chaparro2019}.
Word embeddings have become a common way 
to compare the semantic similarity of the text~\cite{mihalcea2006},
being applied in query-based summarization techniques such as 
AnswerBot~\cite{Xu2017}
or PageRank-based approaches such as HoliRank~\cite{Ponzanelli2017}.



Many other \acs{DL} studies in software engineering~\cite{ferreira2021,li2018deep, watson2022}
use neural network architectures 
in a variety of software engineering tasks, including
code comprehension~\cite{allamanis2015, mi2018}, community forum analysis~\cite{Lin2018, wang2019}, or requirements traceability~\cite{chen2019, guo2017}.
DeepSum~\cite{Li2018}, which we described earlier, is an example of a 
summarization approach that uses an encoder-decoder architecture~\cite{cho2014-encoder-decoder}  to identify 
sentences of interest in bug reports. As another example,
Xi et al.~\cite{Xia2017} use a \ac{CNN}~\cite{krizhevsky2012CNN} 
to identify sentences pertaining to Di Sorbo et al.'s topics~\cite{Sorbo2015}.



Few studies in software engineering have considered more modern neural networks~\cite{watson2022}
able to leverage hidden features not only between words but also between sentence pairs (e.g., \acs{BERT}~\cite{Devlin2018Bert}). 
These models might assist in determining 
implicit relationships between the text in a task and the text in a relevant sentence within a software artifact.
As such, Chapter~\ref{ch:identifying} describes how we use \acs{BERT} for automatically 
identifying text relevant to a software task.
