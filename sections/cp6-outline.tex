\setcounter{chapter}{5}
\setcounter{rq}{1}


\chapter{Evaluating Text Automatically Identified}
\label{ch:assisting}



In a previous chapter, we have shown that semantic-based techniques can identify part of the text 
that human annotators deemed relevant to a number of artifacts associated with Android tasks.
This, however, does not address whether the text automatically identified assists a software developer
 in completing her task.



To shorten this gap, this chapter presents a \textit{controlled experiment} 
where we evaluate how developers complete software tasks when assisted, or not, by a tool that 
embeds one of our semantic-based techniques. 
We use three software tasks related to well-known Python modules and
we report qualitative and quantitative results from \red{12} participants with software development background who attempted these tasks. 



We start by describing experimental procedures (Section~\ref{cp6:procedures}) and then by presenting the results of our experiment (Section~\ref{cp6:results}). Section~\ref{cp6:summary} summarizes our key findings.



\input{sections/cp6/procedures}
\input{sections/cp6/results}
\input{sections/cp6/summary}
