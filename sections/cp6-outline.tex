\setcounter{chapter}{5}
\setcounter{rq}{1}


\chapter{Evaluating Text Automatically Identified}
\label{ch:assisting}




In a previous chapter, we have compared different semantic-based techniques for automatically identifying text relevant to a software task.
Such a comparison assisted us in determining the most promising approach, but it did not address whether 
the text automatically identified  
using these techniques can assist a software developer while working on a task.



To fill this gap, this chapter presents a controlled experiment that 
investigates how a tool that embeds one of such semantic-based techniques 
might impact how a developer works on a task. 
The experiment 
compares the solutions of developers who attempted three Python programming tasks with and without tool support
and reports the usefulness of the text automatically identified by the tool.





We start by outlining our experimental design (Section~\ref{cp6:design}) and then 
by detailing experimental procedures (Sections~\ref{cp6} to~\ref{cp6}).
In Sections~\ref{cp6:correctness} to~\ref{cp6:usefulness}, we report results from the experiment.
We conclude this chapter presenting threats to the validity of our experiment (Section~\ref{cp6:threats}) and 
 summarizing our key findings (Section~\ref{cp6:summary}).


\clearpage







% We conclude this chapter presenting threats to the validity of our experiment (Section~\ref{cp6:threats}) and 
%  summarizing our key findings (Section~\ref{cp6:summary}).






\input{sections/cp6/design}
% \input{sections/cp6/tool}
\input{sections/cp6/tasks}
\input{sections/cp6/participants}
\input{sections/cp6/procedures}

\art{I'm trying to set results in a manner similar to how we did in the ICPC paper---present an overview of what we evaluate in the section, describe the method of evaluation and then, the results.}

\input{sections/cp6/results-correctness}
\input{sections/cp6/results-comparison}
\input{sections/cp6/results-usefulness}
\input{sections/cp6/threats}
\input{sections/cp6/summary}




% to the tool identified text
% as well as from the analysis of the participants' perception on the tool's usefulness.


% i.e., did they find the text automatically identified by our tool useful to their working task?



% This experiment allows us to investigate whether \textit{the text automatically identified by the tool is similar to text that developers would deem relevant}.





















% A participant first attempted one randomly assigned tasks where, while performing the task, they indicated what text---in a set of artifacts that we made available to the participant---they deemed relevant to the task at hand. Then, in another randomly assigned task, we investigate whether the text identified
% automatically by our tool changes how a participant completed a task. 





% attempted these tasks. 
% \gm{A bit more description of
% the setup might be needed.}
% We publicly share experimental materials and results to help future research in the field~\cite{experiment_material}.


% We describe experimental procedures (Section~\ref{cp6:procedures}) before
% presenting the results of our experiment (Section~\ref{cp6:results}). Section~\ref{cp6:summary} summarizes our key findings.







% Early in this thesis, we have shown research methods for 
% bringing together tasks from an existing
% well-known domain, Android development, with relevant text
% for solving that task from multiple artifact types.






% This chapter presents a \textit{controlled experiment}
% that builds upon the research methods presented earlier in this thesis.
% We detail three tasks related to well-known Python modules
% and we describe 
% how \red{15} developers with varying background 
% identify relevant text for solving these task
% in multiple artifact types.












% complete software tasks when assisted, or not, by a tool that 
% embeds one of our semantic-based techniques. 
% \gm{'evaluate how' is a bit unspecified
% as 'how' isn't usually evaluated -
% maybe restate as 'investigate how tools that ... impact how ...'?}




% assists a software developer while working on a task.
% We built upon the research procedures used to create the \acs{DS-android} corpus and we report how d identify relevant text in artifacts pertinent to three tasks from to well-known Python modules. This experiment also allows us to 


% Through 




% To what extent the text automatically identified by a tool that embeds a semantic-based technique assists developers 
% complete a task




% \art{find how to connect this}
% This experiment provides complementary data to our early evaluation 
% on whether automatic approaches can identify text that humans deem useful to a software task. 
% To this end, we use three software tasks related to w and
% report results from \red{15} participants with software development background 


