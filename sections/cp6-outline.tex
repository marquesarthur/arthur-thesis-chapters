\setcounter{chapter}{5}
\setcounter{rq}{1}


\chapter{Evaluating Text Automatically Identified}
\label{ch:assisting}


% In early chapters of this thesis, we have presented a research method to create  a corpus with tasks and annotated data representing text---in multiple types of artifacts---that human annotators deemed relevant for solving Android development tasks. We 
% used this corpus to design and evaluate techniques that use semantics to identify part of the text that human 
% annotators deemed relevant for the tasks and artifacts in such corpa.


In this chapter, we build upon the research outlined  early on in this thesis to
present a \textit{controlled experiment} that focuses on how  a
software developer working on a task can benefit from a tool that automatically identifies text 
relevant to that task.



On the one hand, 
the experiment (Figure~\ref{fig:tool-experiment-procedures})
adapts corpus creation procedures from Chapter~\ref{ch:android-corpus}
to show how 16 participants with software development background identify 
text---in multiple types of artifacts---relevant for solving three Python programming tasks while they performed these tasks \textit{without} any tool support. 
On the other hand, 
it investigates if participants can more correctly complete these tasks 
when using a tool 
that  automatically highlights 
task-relevant text using one of the semantic-based techniques 
from Chapter~\ref{ch:identifying}. 
In summary, this allow us to investigate whether:

\medskip
\begin{bluequote}
    \textit{text potentially relevant to a task and automatically identified  by a tool using a semantic-based technique assist a software developer working on a task.}
\end{bluequote}



\begin{figure}
    \centering
    \includegraphics[width=1.05\textwidth]{cp6/tool-experiment-pipeline.pdf}
    \caption{Summary of experimental procedures}
    \label{fig:tool-experiment-procedures}
\end{figure}






% the experiment uses evaluation procedures  to compare if 
% the text manually identified by the participants
% is similar to the text automatically identified.




To show how this experiment allows such a investigation, we begin by detailing experimental procedures (Section~\ref{})
and then, by comparing the correctness of the tasks performed \textit{with} or \textit{without} tool support (Section~\ref{}).
In Section~\ref{}, we discuss how  the text that participants deemed relevant compares to the text 
shown in the tool-assisted task. Section~\ref{} discusses the usefulness of the automatically identified text according to the feedback provided by the participants.
We conclude presenting threats to the validity of our experiment (Section~\ref{}) and 
 summarizing our key findings (Section~\ref{}).





% to the tool identified text
% as well as from the analysis of the participants' perception on the tool's usefulness.


% i.e., did they find the text automatically identified by our tool useful to their working task?



% This experiment allows us to investigate whether \textit{the text automatically identified by the tool is similar to text that developers would deem relevant}.





















% A participant first attempted one randomly assigned tasks where, while performing the task, they indicated what text---in a set of artifacts that we made available to the participant---they deemed relevant to the task at hand. Then, in another randomly assigned task, we investigate whether the text identified
% automatically by our tool changes how a participant completed a task. 





% attempted these tasks. 
% \gm{A bit more description of
% the setup might be needed.}
% We publicly share experimental materials and results to help future research in the field~\cite{experiment_material}.


% We describe experimental procedures (Section~\ref{cp6:procedures}) before
% presenting the results of our experiment (Section~\ref{cp6:results}). Section~\ref{cp6:summary} summarizes our key findings.







% Early in this thesis, we have shown research methods for 
% bringing together tasks from an existing
% well-known domain, Android development, with relevant text
% for solving that task from multiple artifact types.






% This chapter presents a \textit{controlled experiment}
% that builds upon the research methods presented earlier in this thesis.
% We detail three tasks related to well-known Python modules
% and we describe 
% how \red{15} developers with varying background 
% identify relevant text for solving these task
% in multiple artifact types.










\clearpage


\art{Overall notes Feb, Thu 17, 2020}

To what extent the text automatically identified by such a tool assists developers complete a task


we are mostly interested in seeing how a dev can use the text identified or whether it changes how they complete a task



we design the experiment in such a way that it provides complementary data to the data we used for the quantitative evaluation 
of the semantic based approaches presented in Chapter 5, i.e., participants first manually indicate what text they deem relevant to a task

in a second task, the tool shows them highlights



In the previous chapter, we identified the most promising technique that could identify task-relevant text 


we are now interest in observing whether embedding this technique into a tool changes how developers work in a task...



unindent sections


add footnote about terminology


this is a between subjects experiment, not within 



analysis: report how many times devs selected code

threat: report that the natura of the tasks that we have selected, i.e., programming tasks, may have lead participants to select more code snippets than text

in more abstract or decision making tasks, we could have observed different results. Nonetheless, objectively measuring the correcteness of the tasks and designingg them in such a way that they could be done offline was challening

even more if we consider that we had to do the experiment during a pandemic

\clearpage




\input{sections/cp6/procedures}
\input{sections/cp6/results}
\input{sections/cp6/summary}





% complete software tasks when assisted, or not, by a tool that 
% embeds one of our semantic-based techniques. 
% \gm{'evaluate how' is a bit unspecified
% as 'how' isn't usually evaluated -
% maybe restate as 'investigate how tools that ... impact how ...'?}




% assists a software developer while working on a task.
% We built upon the research procedures used to create the \acs{DS-android} corpus and we report how d identify relevant text in artifacts pertinent to three tasks from to well-known Python modules. This experiment also allows us to 


% Through 




% To what extent the text automatically identified by a tool that embeds a semantic-based technique assists developers 
% complete a task




% \art{find how to connect this}
% This experiment provides complementary data to our early evaluation 
% on whether automatic approaches can identify text that humans deem useful to a software task. 
% To this end, we use three software tasks related to w and
% report results from \red{15} participants with software development background 


