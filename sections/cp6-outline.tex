\setcounter{chapter}{5}
\setcounter{rq}{1}


\chapter{Evaluating Text Automatically Identified}
\label{ch:assisting}




\art{Rather than an affirmative study that just says ``yes it does help'' or ``no it doens't'', I'm trying to phrase it in as a complimentary study}



In a previous chapter, we have shown that semantic-based techniques automatically identify part of the text 
that human annotators deemed relevant to a number of artifacts associated with Android tasks.
Although such evaluation provides empirical data on the role of semantics in the identification of task-relevant information, it does not address whether the text identified as relevant indeed assists software developers in completing a software task.



In this chapter, we present a \textit{user study} where we seek to provide further data supporting, or refuting, the role of semantics in identifying task-relevant text. 
We present three software tasks related to well-known Python modules and we show 
results from \red{20} software developers who each performed two randomly assigned tasks. 
Based on the developers' responses, we compare the text identified as relevant though manual (developers)
and automatic (semantic-based technique) means. We also report the developers' perception on the usefulness of text automatically identified for each task. 


We start by describing the experimental procedures of our user study (Section~\ref{cp6:procedures}) and then by presenting the results of this user study  (Section~\ref{cp6:results}). Section~\ref{cp6:summary} summarizes our key findings.



\input{sections/cp6/procedures}
\input{sections/cp6/results}
\input{sections/cp6/summary}
