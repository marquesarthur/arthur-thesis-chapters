\setcounter{chapter}{5}
\setcounter{rq}{1}


\chapter{Evaluating Text Automatically Identified}
\label{ch:assisting}


% In early chapters of this thesis, we have presented a research method to create  a corpus with tasks and annotated data representing text---in multiple types of artifacts---that human annotators deemed relevant for solving Android development tasks. We 
% used this corpus to design and evaluate techniques that use semantics to identify part of the text that human 
% annotators deemed relevant for the tasks and artifacts in such corpa.


In this chapter, we build upon the research outlined  early on in this thesis to
present a \textit{controlled experiment}~\cite{Lazar2017-cp2} that focuses on how  a
software developer working on a task can benefit from a tool that automatically identifies text 
relevant to that task.



On the one hand, 
the experiment (Figure~\ref{fig:tool-experiment-procedures})
adapts corpus creation procedures from Chapter~\ref{ch:android-corpus}
to show how 16 participants with software development background identify 
text---in multiple types of artifacts---relevant for solving three Python programming tasks while they performed these tasks. % \textit{without} any tool support 
On the other hand, 
it investigates if participants can more correctly complete these tasks 
when using a tool 
that  automatically highlights 
task-relevant text using one of the semantic-based techniques 
from Chapter~\ref{ch:identifying}. 


In summary, this allow us to investigate whether:

\medskip
\begin{bluequote}
    \textit{text potentially relevant to a task and automatically identified  by a tool using a semantic-based technique assist a software developer working on that task.}
\end{bluequote}



\begin{figure}
    \centering
    \includegraphics[width=1.05\textwidth]{cp6/tool-experiment-pipeline.pdf}
    \caption{Summary of experimental procedures}
    \label{fig:tool-experiment-procedures}
\end{figure}




To show how this experiment allows such a investigation, we begin by detailing
experimental procedures (Sections~\ref{cp6:design} to~\ref{cp6:procedures}).
Then, we detail how we analyze the data obtained from the experiment by:

\newpage

\begin{itemize}
    \item assessing the correctness of the tasks performed \textit{with} or \textit{without} tool support (Section~\ref{cp6:correctness});
    \item comparing  the text that participants deemed relevant to the text automatically identified
    and shown in the tool-assisted task (Section~\ref{cp6:comparison}), and;
    \item discussing the usefulness of the automatically identified text according to the feedback provided by the participants (Section~\ref{cp6:usefulness}).
\end{itemize}
 
We conclude this chapter presenting threats to the validity of our experiment (Section~\ref{cp6:threats}) and 
 summarizing our key findings (Section~\ref{cp6:summary}).


 \input{sections/cp6/design}
 \input{sections/cp6/participants}
 \input{sections/cp6/tasks}
 \input{sections/cp6/tool}
 \input{sections/cp6/procedures}

\art{I am not sure about the titles for the sections below}

 \input{sections/cp6/correctness}
 \input{sections/cp6/comparison}
 \input{sections/cp6/usefulness}
 \input{sections/cp6/threats}
 \input{sections/cp6/summary}
 



% to the tool identified text
% as well as from the analysis of the participants' perception on the tool's usefulness.


% i.e., did they find the text automatically identified by our tool useful to their working task?



% This experiment allows us to investigate whether \textit{the text automatically identified by the tool is similar to text that developers would deem relevant}.





















% A participant first attempted one randomly assigned tasks where, while performing the task, they indicated what text---in a set of artifacts that we made available to the participant---they deemed relevant to the task at hand. Then, in another randomly assigned task, we investigate whether the text identified
% automatically by our tool changes how a participant completed a task. 





% attempted these tasks. 
% \gm{A bit more description of
% the setup might be needed.}
% We publicly share experimental materials and results to help future research in the field~\cite{experiment_material}.


% We describe experimental procedures (Section~\ref{cp6:procedures}) before
% presenting the results of our experiment (Section~\ref{cp6:results}). Section~\ref{cp6:summary} summarizes our key findings.







% Early in this thesis, we have shown research methods for 
% bringing together tasks from an existing
% well-known domain, Android development, with relevant text
% for solving that task from multiple artifact types.






% This chapter presents a \textit{controlled experiment}
% that builds upon the research methods presented earlier in this thesis.
% We detail three tasks related to well-known Python modules
% and we describe 
% how \red{15} developers with varying background 
% identify relevant text for solving these task
% in multiple artifact types.












% complete software tasks when assisted, or not, by a tool that 
% embeds one of our semantic-based techniques. 
% \gm{'evaluate how' is a bit unspecified
% as 'how' isn't usually evaluated -
% maybe restate as 'investigate how tools that ... impact how ...'?}




% assists a software developer while working on a task.
% We built upon the research procedures used to create the \acs{DS-android} corpus and we report how d identify relevant text in artifacts pertinent to three tasks from to well-known Python modules. This experiment also allows us to 


% Through 




% To what extent the text automatically identified by a tool that embeds a semantic-based technique assists developers 
% complete a task




% \art{find how to connect this}
% This experiment provides complementary data to our early evaluation 
% on whether automatic approaches can identify text that humans deem useful to a software task. 
% To this end, we use three software tasks related to w and
% report results from \red{15} participants with software development background 


