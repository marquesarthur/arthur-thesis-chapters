\setcounter{chapter}{5}


\chapter{Evaluating an Automated Approach to Task-Relevant Text Identification}
\label{ch:assisting}

% \gcm{It would be better if the title was more aligned
% with the earlier chapters. Maybe something like "Evaluating an Automated Approach to Task-Relevant Text Identification"}

In the last chapter, we showed that semantic-based approaches can help identify text in artifacts relevant to a task. 
In this chapter, we consider whether the identified best approach---\textit{BERT with no filters}---can assist a software developer while they \textit{work} on a task.

% we use the  semantic-based technique to
% identify sentences that likely contain information useful to the input task,
% embedding the approach 



% To investigate how semantic-based approaches might assist developers performing a task,

For that, we embed the approach into a web browser plug-in that automatically highlights 
text relevant to a particular software task in web pages inspected by a developer.
We investigate benefits brought by using this tool, which we call \acs{tool}, through a controlled experiment,
thus addressing whether developers can effectively complete a software task 
when provided with task-relevant text automatically extracted from natural language artifacts. 
We report how  24 participants completed three Python programming tasks 
with and without \acs{tool}
where results show that participants considered the majority of the text automatically identified in the artifacts 
they perused useful and that 
the tool assisted them in producing a more correct solution for one of the experimental tasks.



We start by outlining the evaluation approach  (Section~\ref{cp6:method}). We then
detail experimental procedures  (Section~\ref{cp6:experiment}) before reporting
results from the experiment 
(Section~\ref{cp6:results}).
Section~\ref{cp6:summary} concludes the chapter.



\input{sections/cp6/motivation}
\input{sections/cp6/experiment}
\input{sections/cp6/results}
\input{sections/cp6/summary}




% \gcm{Sorry if I missed it but do you outline the number of artifacts and type of artifacts per task?}
% \art{sorry. I will add that to the Artifacts session}



% \art{I'm trying to set results in a manner similar to how we did in the  (``Task-Relevant Knowledge Identification'')ICPC paper---present an overview of what we evaluate in the section, describe the method of evaluation and then, the results.}


% \input{sections/cp6/summary}

%  (``Task-Relevant Knowledge Identification'')