\setcounter{chapter}{5}
\setcounter{rq}{1}


\chapter{Evaluating Text Automatically Identified}
\label{ch:assisting}




Previously, we have compared different semantic-based techniques for automatically identifying text in an artifact that is of potential relevance to a software task.
Although, such a comparison answered whether semantic-based approaches can identify text that humans deem relevant,
it did not address whether 
the text automatically identified  
can assist a software developer while they \textit{work} on a task.



To fill this gap, this chapter presents a controlled experiment that 
investigates how a tool that embeds one of the semantic-based techniques from Chapter~\ref{ch:identifying} might impact a developer's work. 
With this experiment, we report qualitative and quantitative aspects drawn from data obtained from 24 participants who attempted Python
programming tasks when assisted (or not) by a tool that automatically highlights text that it determines as relevant to the participant's task. 


We start by outlining how we plan to evaluate such a tool (Section~\ref{cp6:method}) and then 
by detailing experimental procedures (Sections~\ref{cp6:experiment}).
Section~\ref{cp6:results} reports results from the experiment as well as threats to its validity.
Section~\ref{cp6:summary} concludes the chapter.

% ---\textit{BERT with no filters}---

\clearpage







% We conclude this chapter presenting threats to the validity of our experiment (Section~\ref{cp6:threats}) and 
%  summarizing our key findings (Section~\ref{cp6:summary}).




\input{sections/cp6/motivation}
\input{sections/cp6/experiment}
\input{sections/cp6/results}
\input{sections/cp6/threats}
\input{sections/cp6/summary}




% \art{I'm trying to set results in a manner similar to how we did in the ICPC paper---present an overview of what we evaluate in the section, describe the method of evaluation and then, the results.}


% \input{sections/cp6/summary}

