

% \subsection{Characterizing Task-relevant Text}




% Many studies have provided possible properties that guide identifying text relevant to a developer's task~\cite{Bavota2016}.
% Some of these studies 
% detail qualitative aspects that individuals use to assess relevance~\cite{Barry1994, Freund2015, Forward2002}.
% Other studies provide quantitative data in the form of 
% text annotated as relevant for certain software tasks~\cite{Jiang2016b, Robillard2015}. 
% Despite their significant contributions,
% these studies do not investigate whether the cues used to determine relevant text in certain tasks and artifacts 
%  also apply to different tasks and artifacts. 
% Existing quantitative data also does not examine if and how different developers 
% use the same properties to determine text relevant to their tasks~\cite{Freund2013, Freund2015}.
% Therefore, we identify the need to empirically characterize 
% how relevant text is found across different software tasks and artifact types. 



% With this study, we investigate whether
% there is consistency in the text within natural language artifacts
% that software developers identify as relevant to a task and, if there
% is consistency, what are common characteristics in the textual fragments identified as task-relevant?
% Determining common cues that
% developers use regardless of an artifact's type
% will provide us the basis for the design of techniques that automatically determine
% the relevance of text to tasks.





% \subsubsection{Approach}



% \subsubsection{Evaluation}



% \subsubsection{Novelty}




% % While existing studies shed light upon many aspects regarding relevance,
% % one premise
% % in many software engineering approaches is the existence of corpora containing information
% % annotated as relevant~\cite{Jiang2016b, Robillard2015}.
% % To create a
% % corpus, annotators often follow a coding guideline and reconcile disagreements.  However, there are many criteria in how individuals
% % assess relevance~\cite{Barry1994, Barry1998, Freund2015} and they may
% % or may not reach consensus~\cite{Saracevic2007c}.
% % By investigating how several software developers assess textual cues in the content of a patch to determine its relevance (Section~\ref{sec:rq-initial-study}),
% % we aim to add to the existing theory of assessing relevance within a patch.