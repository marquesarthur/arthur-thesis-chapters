\section{Evaluation}
\label{cp5:evaluation}

\red{confusing:}
Evaluation focuses on two main questions:

\begin{enumerate}
    \item How accurately can our approach detect text considered by humans as being relevant to a particular task?
    
    \item When identifying text relevant to a task, are certain features (i.e., lexical, word-semantics, and sentence-semantics) more informative than others?
\end{enumerate}



\red{should be something as in: determine which techniques work best for detecting task relevant text?}


To answer these questions, we use our approach to identify sentences relevant to
the tasks in the \acs{DS-android} corpus,
comparing the sentences identified against the 
sentences in the corpus' golden data. 
For this evaluation, we set the target number of sentences identified (i.e., length of a summary) to 20\% of the content of an artifact,
which is a value derived from our study on characterizing 
task-relevant information in natural language artifacts~\cite{marques2020}.





\subsection{Accuracy}


Motivated by the fact that the text marked by the annotators in the \acs{DS-android} differs
(Section~\ref{cp4:corpus-relevant-text}),
we follow procedures outlined by Lotufo et al.~\cite{Lotufo2012}
to measure how accurately our approach identifies text relevant to a task considering two evaluation scenarios:


\begin{enumerate}
    \item one where the text marked only by two or more annotators is considered relevant, and;
    
    \item a second where we consider that any of the text marked by the annotators contributes towards task completion.
\end{enumerate}

\red{flip the above discuss this as a classification problem?}


Standard precision and recall metrics~\cite{Manning2009IR} are suitable metrics for the first scenario. In the second, we use pyramid precision and pyramid recall metrics
to compute accuracy based on how many annotators marked the text~\cite{Nenkova2004, Lotufo2012}. 
To understand these metrics, we refer to the evaluation outcomes of Table~\ref{tbl:type-I-II-errors}. 
The \textit{relevant} and \textit{not-relevant} columns represent the text 
marked (or not) by annotators~\cite{Lotufo2012} for a particular task and a pertinent artifact. Rows represent the text automatically identified by our approach for the same task and pertinent artifact.


\input{sections/cp5/tbl-eval-outcomes.tex}


\subsubsection{Precision and recall}

\red{I don't need recall anymore. Do I?}


For standard precision and recall metrics, the relevant column in Table~\ref{tbl:type-I-II-errors} represents \textit{text marked by two or more annotators}. 
In this scenario, $precision$ is the fraction of the sentences
 identified that are relevant over the total number of sentences identified, as shown in Equation~\ref{eq:cp5:precision}.


\begin{equation}
\label{eq:cp5:precision}    
    Precision = \frac{TP}{TP + FP}
\end{equation}

\vspace{2mm}
Recall represents how many of all marked sentences are identified by our approach (Equation~\ref{eq:cp5:recall}).


\begin{equation}
\label{eq:cp5:recall}        
    Recall = \frac{TP}{TP + FN}
\end{equation}




\subsubsection{Pyramid precision and pyramid recall}


For pyramid precision and pyramid recall, the relevant column in Table~\ref{tbl:type-I-II-errors} represents \textit{text marked by any annotator}. 


Pyramid precision considers the ``optimal'' sentences that could be identified, i.e., the sentences marked by most of the annotators, and compares that to the sentences actually identified. Let $marked(s)$ be a function that returns the number of annotators who marked the sentence $s$, pyramid precision is defined as 
the summation of $marked(s)$ for the set of sentences identified ($s \in S$) over the set of optimal sentences derived from the golden data ($s \in G$).



\begin{equation}
\label{eq:cp5:pyramid-precision}    
    Pyramid Precision =  \frac{
        \sum_{s \in S} \text{ } marked(s)
    }{
        \sum_{s \in G} \text{ } marked(s)
    }
\end{equation}


% \vspace{3mm}
% As an example consider an artifact with a total of 6 marked sentences,
% where 2 sentences were marked by the three annotators, 3 by two annotators and 1 sentence by a single annotator. 
% When identifying three sentences, the optimal solution is $8$ $(2 \times 3 + 1 \times 2)$.
% Hence, an approach that identifies three sentences maked by one, two and all annotators 
% would have a pyramid precision score of $0.75$ or $\frac{ (1 + 2 + 3)}{(2 \times 3 + 1 \times 2)}$.




\vspace{2mm}
Pyramid recall calculates the fraction of the sentences identified by our approach over 
any of the sentences marked by the annotators. Since we updated the set of $TP$ and $FN$ of Table~\ref{tbl:type-I-II-errors}, we can still compute pyramid precision using Equation~\ref{eq:cp5:recall}.




\subsubsection{Results}


When interpreting results, we favour precision/pyramid precision instead of recall/pyramid recall.
Our rationale is that a developer might completely abandon reading of an artifact due to a false positive.
Thus, even if we are not able to identify all sentences that are indeed relevant, true positives may encourage a developer to inspect a pertinent artifact more thoroughly~\cite{Singer1998, Brandt2009a}.




Table~\ref{tbl:approach-results-overall} shows the accuracy of our approach 
over all features as well as per group of features (i.e., lexical, word-semantics, and sentence-semantics).


\input{sections/cp5/tbl-results-overall}


Table~\ref{tbl:approach-results-task} shows evaluation metrics by type of task, i.e., GitHub or StackOverflow.

\input{sections/cp5/tbl-results-by-task}



We also investigate if an artifact type influences the observed accuracy. 
Table~\ref{tbl:approach-results-artifacts} gives insight into the accuracy of our approach for the artifact types in the \acs{DS-android} corpus. Results comprise the set of features 
that provide the best accuracy results. A breakdown of results per group of features is available under Appendix~\ref{a}.


We use a Wilcoxon-Mann-Whitney test~\cite{mannWhitneyU} to check if differences in the  results for each 
type of artifact are statistically significant. Results of the test indicate that... 

\input{sections/cp5/tbl-results-by-artifact}


\subsection{Feature evaluation}


We evaluate how each individual textual property contributes towards determining 
that a sentence in a pertinent artifact is relevant to a task 
using a \red{placeholder} test. 
Informally, the higher the test output value for a feature, the more the observed outcome (i.e., whether a sentence is relevant or not to a task) depends 
on that feature.
We use this knowledge to identify if certain features contribute more or less 
to the observed outcome.



\art{I started reading chi square, but it does not apply to the features explored because some of them are continuous. I may need a logistic regression model to answer this question}


\subsubsection{Results}



Table~\ref{tbl:features-chi-square} gives insight into the predictive power of each feature we use as part of our approach. 



\input{sections/cp5/tbl-features-chi-square}





\subsection{Threats to validity}


