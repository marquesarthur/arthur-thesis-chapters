\clearpage

\section{Evaluation}
\label{cp5:evaluation}


Our evaluation focuses 
on determining what portion of the text identified by human annotators the techniques that we explore can automatically identify.
Our experimental setup is as follows.



\subsubsection{Metrics}


Ultimately, the goal of our evaluation is to assist the design of tools that help developers quickly locate the text relevant to their tasks, surfacing or highlighting the identified text in an artifact under inspection~\cite{Robillard2015}.
This means that the text identified by a technique represents a set containing the information to be surfaced and thus, 
we use \textit{precision} and \textit{recall} to measure the accuracy of each technique~\cite{Manning2009IR}.
We use the evaluation outcomes detailed in Table~\ref{tbl:type-I-II-errors} to understand each metric.

\medskip
\input{sections/cp5/tbl-eval-outcomes}



\paragraph{\textbf{Precision}}

Precision measures the fraction of the sentences identified that are relevant over the total number of sentences identified, as shown in Equation~\ref{eq:cp5:precision}.



\begin{equation}
\label{eq:cp5:precision}    
    Precision = \frac{TP}{TP + FP}
\end{equation}


\paragraph{\textbf{Recall}} Recall represents how many of all marked sentences are identified by a technique (Equation~\ref{eq:cp5:recall}).


\begin{equation}
\label{eq:cp5:recall}        
    Recall = \frac{TP}{TP + FN}
\end{equation}



\paragraph{\textbf{Pyramid Precision \& Pyramid Recall}} 

We measure precision and recall considering sentences marked by two or more annotators; this, however ignores the fact that text marked by a single annotator can equally contribute towards task completion~\cite{marques2020}. To address the text identified by a single annotator, we 
follow evaluation procedures outlined by Lotufo et al.~\cite{Lotufo2012} and we
also report pyramid precision and pyramid recall. These metrics are similar to the ones defined in 
Equations~\ref{eq:cp5:precision} and~\ref{eq:cp5:recall} but treat any of the text marked as relevant.






\subsubsection{Baseline}


As done by~\cite{Lin2021} and~\cite{Ye2016}, we use a standard VSM lexical similarity approach as a baseline. Our rationale to use 
lexical similarity is based on the fact that 
both our card sorting analysis (\red{Chapter~\ref{aaa}}) and related work~\cite{Ko2006a, Freund2015} has shown that developers often use keyword-matching as a simple and quick search strategy to locate text that might contain information relevant to their tasks.


Following procedures analogous to the semantic similarity-based technique (Section~\ref{cp5:approach-w2v}), we compute lexical similarity and output the top-n sentences with highest similarity as the ones likely relevant to an input software task.




\subsubsection{Techniques Configuration}



We configure each technique to identify a target number of 10 relevant sentences per input task-artifact.
Our decision is based on the fact that 
no more than 20\% of the content in the corpora are deemed relevant to a task, which, on average, accounts for 8.93 sentences (\red{Chapter~\ref{}}),
and that other researchers have also used the same target number in their experimental setups, e.g., ~\cite{Xu2017} or~\cite{Lotufo2012},
what will facilitate comparing our results to related work.




% For this evaluation, we set the target number of sentences identified (i.e., length of a summary) to 20\% of the content of an artifact,
% which is a value derived from our study on characterizing 
% task-relevant information in natural language artifacts~\cite{marques2020}.




As for training data, we explore two configurations:


\boldparagraph{BERT\textsubscript{DS-android}}{
We split the data available in the \acs{DS-android} dataset and use part of it for training. 
As done by other studies (e.g.,~\cite{Chaparro2017, fucci2019, Petrosyan2015}), we use standard cross-validation techniques to ensure  that no data used for evaluation, i.e., testing, is also used
during the model's training procedures, i.e., training and validation. More specifically, we use 10-fold cross-validation with 70\%, 20\% and 10\% splits for training, validation and testing. \red{ref} We refer to this configuration as \texttt{BERT\textsubscript{DS-android}}.
}

\boldparagraph{BERT\textsubscript{DS-synthetic}}{
To study the impacts of training data on BERT, we train the model in a smaller dataset containing six tasks and a total of 1874 sentences, from which 602 of them were deemed relevant by 20 participants with software development experience (\red{Chapter~\ref{aaa}}). The tasks in this dataset were not drawn from open-source systems, but rather created to stimulate the participants' information-seeking behaviour. Due to the synthetic nature of the tasks in this dataset, we refer to this configuration as \texttt{BERT\textsubscript{DS-synthetic}}.
}


\subsection{Results}






\input{sections/cp5/tbl-results-overall}






\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{cp5/results_lollipop}
    \caption{Evaluation results for each technique configuration \red{Draft: I will split the figure in 3. One for each metric. The markers will represent the filters}}
    % \label{fig:webview-task}
\end{figure}







% \art{Change from tables to plots. Similar to Sarah and Treude~\cite{nadi2020}, show Venn diagram with how many sentences each approach identifies and overlaps between them}
\art{Consider changing from tables to plots.}

\art{I've got all results without seframe filters. This week, filters will be my main focus}

% \input{sections/cp5/tbl-results-with-filters}


% \input{sections/cp5/tbl-results-by-artifact}



