\section{Evaluation}
\label{cp5:evaluation}




\subsection{Method}



--- We use the \acs{DS-android} corpus to evaluate our proposed techniques  \vspace{3mm}


\subsubsection{Setup}

--- We consider the following configurations  \vspace{3mm}

\begin{enumerate}
    \item Word-based properties
    \item Sentence-based properties
    \item Combination of Word-based + Sentence-based properties
\end{enumerate}



\subsubsection{Metrics}


Due to how the text marked by the annotators differ
and considering the purpose which motivated the creation of the \acs{DS-android} corpus
(i.e., to evaluate techniques able to detect the text relevant to a task automatically),
we suggest the usage of metrics outlined by Lotufo et al.~\cite{Lotufo2012} for evaluation purposes. 
That is,
one can consider the text marked only by two or more annotators as relevant
and use standard precision and recall metrics~\cite{Manning2009IR}, or one can consider that
any of the text marked by the annotators 
contributes to task completion and weight the number of 
annotators who marked the text through the usage of the pyramid 
precision and pyramid recall metrics~\cite{Nenkova2004, Lotufo2012}. 
\art{I wanted to give a quick summary of how the text in the corpus can be used for the purpose to which it was created. I am not certain if this would be better here or in another secion.} \gm{I think a different place is better. The important point here is what
is in the corpus - all sentences marked by annotators and who marked which?}





--- Since the number of annotators that deemed a sentence as useful to a task 
in the \acs{DS-android} corpus varies, we consider 
two main evaluation scenarios:


% --- Guided by Lotufo et al.'s evaluation of techniques for the 
% detection of the most relevant sentences in a bug report based~\cite{Lotufo2012},
% we consider two evaluation scenarios:
\begin{itemize}
    \item only text marked by the majority of the annotators is relevant to a task.
    
    \item text marked by any annotator contains valuable information.
\end{itemize}
 


The first is a conservative scenario where text marked by a single annotator can be an indicator of unrelated information, which may not contribute to task completion. 
We use precision and recall~\cite{Manning2009IR} as metrics for this evaluation scenario. 


Pyramid 
precision and pyramid recall metrics, as reported by Lotufo et al.~\cite{Nenkova2004, Lotufo2012}, are suitable metrics for the second evaluation scenario. 
If more annotators marked a sentence, it is likely that that sentence contains key information for completing a task albeit the metrics do not exclude sentences marked by a single annotator because these sentences provide a glimpse of an individual's perception of relevancy~\cite{Petrosyan2015}. 

% For instance, 
% a sentence describing how to use a certain API may be relevant to a novice developer using that API for the first time while it may be ``common knowledge'' to a more experienced developer.


% To understand precision and recall metrics metrics, Table~\ref{tbl:type-I-II-errors} show all possible outcomes  when evaluating a technique able to automatically detect text relevant to a software task. The \textit{relevant} and \textit{not-relevant} columns represent the text 
% marked (or not) by \textit{at least two} annotators~\cite{Lotufo2012}. Rows represent the text identified by an automatic approach.


% \input{sections/cp5/tbl-eval-outcomes.tex}



% For a given task $t$ and artifact $a$, $precision$ is the fraction of the sentences identified that are marked as relevant by at least two annotators over the total number of sentences identified, as shown in Equation~\ref{eq:cp4:precision}.


% \begin{equation}
% \label{eq:cp4:precision}    
%     Precision(t, a) = \frac{TP}{TP + FP}
% \end{equation}


% Recall ($recall$) represents how many of all sentences marked by at least two annotators are identified by a technique (Equation~\ref{eq:cp4:recall}).


% \begin{equation}
% \label{eq:cp4:recall}        
%     Recall(t, a) = \frac{TP}{TP + FN}
% \end{equation}



\subsection{Results}



\subsection{Threats to validity}





