\section{Evaluation}
\label{cp5:evaluation}


Evaluation focuses 
on answering the following research questions:


\begin{enumerate}[label=\textit{RQ\arabic*}]

    \item \textit{How accurately can techniques build upon word semantics identify text deemed relevant to a task?}
    
    \item \textit{Do sentence semantics help to identify text relevant to a software task?}

    \item \textit{How do semantic techniques compare to techniques in related work?}
\end{enumerate}
    





\subsection{Assessing the accuracy of word semantic techniques}


To answer RQ1 we compute evaluation metrics when we try to identify the sentences deemed relevant by human annotators in the \acs{DS-android} dataset.


Due to training procedures, BERT might be biased towards the tasks and text marked in this dataset. We minimize risks that might arise from this fact, by reporting results when we train the model using 10-fold cross-validation and by reporting results when we 
train the model with a different dataset.


\subsubsection{Metrics}


Our aim is to identify text that contains information that a developer would deem relevant to the task that they are currently performing. 
The text would be highlighted in an artifact so that the developer can quickly locate and inspect it. 
This means that the text identified represents a set containing task-relevant and thus, 
set-based metrics are appropriate to measure the accuracy of each technique~\cite{Manning2009IR}.


We use standard Precision and Recall metrics to measure accuracy.





\paragraph{\textbf{Precision}}

Provide details for metric.



\begin{equation}
\label{eq:cp5:precision}    
    Precision = \frac{TP}{TP + FP}
\end{equation}


\paragraph{\textbf{Recall}}

Provide details for metric.


\begin{equation}
\label{eq:cp5:recall}        
    Recall = \frac{TP}{TP + FN}
\end{equation}



\paragraph{\textbf{Pyramid Precision \& Pyramid Recall}} 

Measure precision and recall when true positives (TP) represent text marked at least by two annotators.


\subsection{Assessing the accuracy of sentence semantic techniques}


To answer RQ2 we apply the frame semantic filters described in Section~\ref{cp5:approach-filters} to the techniques that use word semantics and report precision and recall metrics to assess if the filters improve the detection of text relevant to a software task.




\subsection{Comparing techniques with related work}


To answer RQ3, we apply a set of techniques, further detailed in this section, from related work to the tasks and artifacts used in our evaluation. 


In this evaluation, we compare techniques from related work to the technique that yielded the best evaluation results obtained in our investigation of RQ1 and RQ2.


\subsubsection{VSM}



We use a standard VSM lexical similarity approach as a baseline for comparison. Our rationale to use 
lexical similarity is based on the fact that 
both our card sorting analysis (\red{Chapter~\ref{aaa}}) and related work~\cite{Ko2006a, Freund2015} has shown that developers often use keyword-matching as a simple and quick search strategy to locate text that might contain information relevant to their tasks.


VSM has also been used by ~\cite{Lin2021} and ~\cite{Ye2016} to \dots



\art{When I did a first run of these approaches, they did not detect much of the text marked by the annotators, so should I sill consider them for comparison purposes? If not, what are valid alternatives?}


\subsubsection{AnswerBot}


\acf{AnsBot} automates generation of answer summaries for a developer's task for Stack Overflow posts.

The tool uses a set of features, ranging from lexical and semantical features to features that leverage meta-data available in Stack Overflow to detect useful text that will be used in the answer summary provided by the tool. 

Given the scope of the tool, we compare the accuracy our technique to AnswerBot when identifying relevant text in 
Stack Overflow artifacts.


\subsubsection{Krec}


\acf{Krec} automates the identification of information fragments that a developer cannot afford to ignore when reading API documentation.

To detect these fragments, the tool relies on a set of word patterns that indicate if a sentence contains useful information. Hence, we compare the accuracy our our technique to Krec when detecting relevant text in API documentation.


\subsubsection{Hurried Bug Report}


\acf{Hurried} estimates the attention that a developer would hypothetically give to different sentences in a bug report. 


We consider that the text that the tool identifies as the one that a developer would attend to first most likely contains information relevant to a developer task. Hence we compare the accuracy of \acf{Hurried} to ours for the bug reports artifacts in our dataset.


\subsection{Results}




\input{sections/cp5/tbl-results-overall}


\input{sections/cp5/tbl-results-with-filters}


\input{sections/cp5/tbl-results-by-artifact}



