\section{Evaluation}
\label{cp5:evaluation}




\subsection{Method}



--- We use the \acs{DS-android} corpus to evaluate our proposed techniques  \vspace{3mm}


\subsubsection{Setup}

--- Explain each evaluation scenario, e.g., using only word-level properties, sentence-level properties, etc.



\subsubsection{Metrics}






Motivated by the fact that the text marked by the annotators in the \acs{DS-android} differs
(Section~\ref{cp4:corpus-relevant-text}),
we follow evaluation metrics outlined by Lotufo et al.~\cite{Lotufo2012}
to measure how accurately the techniques that are part of our approach identify text relevant to a task.
That is,
one can consider the text marked only by two or more annotators as relevant
and use standard precision and recall metrics~\cite{Manning2009IR}, or one can consider that
any of the text marked by the annotators 
contributes to task completion and use pyramid 
precision and pyramid recall metrics
to compute accuracy based on how many annotators marked the text~\cite{Nenkova2004, Lotufo2012}. 


--- Detail each metric



% --- Since the number of annotators that deemed a sentence as useful to a task 
% in the \acs{DS-android} corpus varies, we consider 
% two main evaluation scenarios:


% % --- Guided by Lotufo et al.'s evaluation of techniques for the 
% % detection of the most relevant sentences in a bug report based~\cite{Lotufo2012},
% % we consider two evaluation scenarios:
% \begin{itemize}
%     \item only text marked by the majority of the annotators is relevant to a task.
    
%     \item text marked by any annotator contains valuable information.
% \end{itemize}
 


% The first is a conservative scenario where text marked by a single annotator can be an indicator of unrelated information, which may not contribute to task completion. 
% We use precision and recall~\cite{Manning2009IR} as metrics for this evaluation scenario. 


% Pyramid 
% precision and pyramid recall metrics, as reported by Lotufo et al.~\cite{Nenkova2004, Lotufo2012}, are suitable metrics for the second evaluation scenario. 
% If more annotators marked a sentence, it is likely that that sentence contains key information for completing a task albeit the metrics do not exclude sentences marked by a single annotator because these sentences provide a glimpse of an individual's perception of relevancy~\cite{Petrosyan2015}. 

% For instance, 
% a sentence describing how to use a certain API may be relevant to a novice developer using that API for the first time while it may be ``common knowledge'' to a more experienced developer.


% To understand precision and recall metrics metrics, Table~\ref{tbl:type-I-II-errors} show all possible outcomes  when evaluating a technique able to automatically detect text relevant to a software task. The \textit{relevant} and \textit{not-relevant} columns represent the text 
% marked (or not) by \textit{at least two} annotators~\cite{Lotufo2012}. Rows represent the text identified by an automatic approach.


% \input{sections/cp5/tbl-eval-outcomes.tex}



% For a given task $t$ and artifact $a$, $precision$ is the fraction of the sentences identified that are marked as relevant by at least two annotators over the total number of sentences identified, as shown in Equation~\ref{eq:cp4:precision}.


% \begin{equation}
% \label{eq:cp4:precision}    
%     Precision(t, a) = \frac{TP}{TP + FP}
% \end{equation}


% Recall ($recall$) represents how many of all sentences marked by at least two annotators are identified by a technique (Equation~\ref{eq:cp4:recall}).


% \begin{equation}
% \label{eq:cp4:recall}        
%     Recall(t, a) = \frac{TP}{TP + FN}
% \end{equation}



\subsection{Results}



\subsection{Threats to validity}





