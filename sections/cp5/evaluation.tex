
\section{Evaluation}
\label{cp5:evaluation}



The goal of our evaluation is to assist the design of tools that
can apply to multiple kinds of software artifacts
so that software developers can rely upon
one technique instead of many. To be useful, the one technique
that applies to multiple artifacts must 
perform similarly to techniques specialized
to particular kinds of artifacts.


Therefore, we first evaluate whether any of the
six techniques we introduce performs
similarly to a technique specialized to one
kind of artifact. We chose to compare
the performance of our techniques to
that of AnswerBot on Stack Overflow
answers because it is the technique closest to our work (Chapter~\ref{ch:related-work}). This
evaluation helps answer: 


\begin{enumerate}[label=\textit{RQ\arabic*},leftmargin=*]

    \item  \textit{How do our six semantic-based techniques compare to a state-of-the-art technique that is specific to a particular artifact type?} 

\end{enumerate}


% \medskip
% \begin{bluequote}
%     \textit{How do our six semantic-based techniques compare to a state-of-the-art technique that is specific to a particular artifact type?} 
% \end{bluequote}





We then consider whether the semantic-based
techniques can perform equivalently well across
multiple artifact types.
This evaluation helps answer:




\begin{enumerate}[label=\textit{RQ\arabic*},leftmargin=*]
    \setcounter{enumi}{1}

    \item  \textit{Which semantic-based technique provides the best results?} 

\end{enumerate}

% \begin{bluequote}
%     \textit{Which semantic-based technique provides the best results?} 
% \end{bluequote}





We use \textit{precision} and \textit{recall} metrics~\cite{manning2010IR} to measure what portion of the text identified by human annotators in the \acs{DS-android} corpus the techniques automatically identify.
In this context, we believe recall to be the most important metric since failure to identify text that is relevant to a task means that a developer will have an incomplete or partial view of the information needed,
what can lead to faults~\cite{Murphy2005}.




\subsection{Experimental Setup}
\label{cp5:config}

To evaluate the six techniques and determine how much of the task-relevant text in the \acs{DS-android} corpus they are able to identify, we use the following experimental setup.

\subsubsection{Techniques' Output}


We configure each technique to identify a target number of 10 relevant sentences for each task-artifact pair.
This decision is based on the fact that no more than 15\% of the content of any artifact in our dataset is deemed relevant to a task, which, on average, accounts for 8.93 sentences (Chapter~\ref{ch:android-corpus}).
Researchers have also used the same target number of 10 sentences when evaluating techniques  (e.g.,~\cite{Xu2017} or~\cite{Lotufo2012}) able to identify relevant text for certain kinds of artifacts, which will also facilitate comparing our results to related work.


\subsubsection{Training \& Testing Data}


In addition to configuring the techniques' output, two of our techniques require training data for fine-tuning purposes (\texttt{BERT}) and to derive task-artifact frame pairs (\texttt{frame-associations}).
We ensure that no data used to evaluate these techniques is also used to train them by 
splitting the dataset into two portions, one for training and another for testing, each with an equal number of tasks.
Due to our comparison with AnswerBot, we also ensure that all tasks used for testing purposes have associated Stack Overflow artifacts.
That is, we create our test set randomly selecting 25 tasks in the dataset that contain a 
Stack Overflow artifact among its associated artifacts.


% Appendix~\ref{cp5:appendix} shows evaluation results when we use 
% 10 fold cross-validation. 


\subsubsection{Metrics}



We compute values for \textit{precision} and \textit{recall} metrics based on the sentences 
deemed relevant to a task by \textit{at least two} human annotators.
To minimize risks from a scenario where \rev{BERT training procedures produce a model with}   peak or bottom performance, 
we compute results for \rev{this} technique over 10 distinct executions over the test data, 
reporting the average of each metric over all runs. \rev{The other techniques have the same output across distinct runs.}



% We compute values for \textit{precision} and \textit{recall} metrics based on the sentences 
% deemed relevant to a task by \textit{at least two} human annotators.
% To minimize risks from a scenario where one of the techniques and their underlying approaches has a peak or
% bottom performance due to training procedures or due to factors beyond our control, 
% we compute results for each technique over 10 distinct executions over the test data, 
% reporting the average of each metric over all runs.


For a detailed definition of each metric, we refer to the evaluation outcomes in Table~\ref{tbl:type-I-II-errors}, where  columns represent  labels provided by the annotators and rows,
the text identified as relevant or not by a technique.

% 

\medskip
\input{sections/cp5/tbl-eval-outcomes}



\paragraph{\textbf{Precision}}

Precision measures the fraction of the sentences identified that are relevant over the total number of target sentences identified (Equation~\ref{eq:cp5:precision}).



\begin{equation}
\label{eq:cp5:precision}    
    Precision = \frac{TP}{TP + FP}
\end{equation}



\paragraph{\textbf{Recall}} Recall represents how many of all the annotated sentences are identified by a technique (Equation~\ref{eq:cp5:recall}).


\begin{equation}
\label{eq:cp5:recall}        
    Recall = \frac{TP}{TP + FN}
\end{equation}



\medskip
Precision means identifying only text that is relevant, whereas recall means identifying all relevant text.
Ideally, we would aim for a technique with high precision and high recall. Unfortunately, this is often not possible and we must reach a compromise.
As described earlier, our goal is to support developers to locate text that might be relevant to their task, 
and not locating all the relevant text may lead to incomplete or incorrect solutions, thus the reason we favour recall.






\subsection{Comparison to AnswerBot}
\label{cp5:comparison}


As we have presented in Chapter~\ref{ch:related-work}, AnswerBot is a state-of-the-art summarization technique that applies to Stack Overflow posts~\cite{Xu2017}. Its relevant passage selection algorithm is close to our domain problem (i.e., identifying sentences likely relevant to an input task)
and thus, the reason why we have decided to compare our techniques to it. 


For a fair comparison between our techniques and AnswerBot (\texttt{AnsBot}), we must ensure that we obtain measurements for all the approaches under the same circumstances. 
This could mean applying our techniques to the tasks and artifacts used in AnswerBot's original evaluation or applying both our semantic-based techniques and AnswerBot to our test data.
It would be interesting to report results for both scenarios, but golden data in AnswerBot represents how human evaluators judged the target sentences outputted by the tool.
This comprises only a portion of the entire text available in a Stack Overflow answer, which can have more text that these evaluators could have judged as relevant. 
Since we do not have access to AnswerBot's original judges to construct this data,
 we evaluate our techniques and \texttt{AnsBot} only on the tasks and Stack Overflow artifacts in the portion of the \acs{DS-android} dataset that we use for testing.


Since \texttt{AnsBot} uses both textual properties and meta-data, 
we also evaluate how much of the tool's accuracy relies on properties such as the number of votes an answer got on the platform
or whether the answer has been accepted.
For that, we disable any of the features used by \texttt{AnsBot} that rely on meta-data and we compute evaluation metrics once more.
We refer to this configuration in our results as \texttt{AnsBot\textsubscript{text}}.


\subsubsection{Results}




Table~\ref{tbl:techniques-results-answerbot} shows values of precision and recall for  \texttt{AnsBot} and the semantic techniques that we explore. 
In the table, rows provide details about a specific base technique while columns discriminate 
precision and recall values and which post-processing filters were applied. 
We also mark results from paired Wilcoxon-Mann-Whitney tests~\cite{mannWhitneyU} 
that check if the results between each technique and \texttt{AnsBot} are statistically different.


\input{sections/cp5//tbl-results-stackoverflow}


Overall, we observe that \texttt{AnsBot} achieves 0.59 precision and 0.63 recall---values explainable by the fact that the tool is tailored specifically to Stack Overflow.
When we compare \texttt{AnsBot} to our techniques, we observe that the base
 \texttt{BERT} approach and the one using \texttt{frame-associations} achieve result comparable to \texttt{AnsBot}. Interestingly, the \texttt{frame-elements} filter does not provide significant improvements to the base approach. 
% The lack of differences may be explained by the fact that, to determine relevance, the attention mechanism used by BERT already correlates the text in a task and in an artifact (Section~\ref{cp5:approach-bert}), 
% and that the vector representations in the model are able to infer contextual information, which may serve as an implicit way to identify a sentence's meaning.
For \texttt{word2vec}, we observe that the technique's results are significantly lower than \texttt{AnsBot}, which suggests that it fails to identify relevant text identified by \texttt{AnsBot}. 


Without meta-data, 
results (Table~\ref{tbl:techniques-results-answerbot-text})
 indicate that
all \texttt{BERT} techniques
achieve significantly higher recall than \texttt{AnsBot\textsubscript{text}}.
This suggests that, to determine relevancy, a neural network might implicitly 
use some of the properties of text that appears in highly voted or accepted answers.


\input{sections/cp5/tbl-results-stackoverflow-text}


% Results from our comparison to a state-of-the-art approach, namely AnswerBot, 
% indicate that text-based semantic techniques achieve comparable accuracy  
% when identifying relevant information in Stack Overflow artifacts.



\medskip
\begin{bluequote}
    \textit{Results from our comparison to a state-of-the-art approach, namely AnswerBot, 
    indicate that text-based semantic techniques achieve comparable accuracy  
    when identifying relevant information in Stack Overflow artifacts.}
\end{bluequote}






\subsection{Evaluation Across all Artifact Types}
\label{cp5:results-all}





Provided that we can identify task-relevant textual information with accuracy comparable to a state-of-the-art approach, 
we measure how much of the text that is relevant to a task (within an artifact)
 our semantic-based techniques can identify.
To this end, we compute precision and recall metrics over all the artifacts in the test data. 
To assist comparisons, we also report results for the text identified by a
standard \acf{VSM}~\cite{Salton1975vsm} lexical similarity approach. Several studies~\cite{Ko2006a, Freund2015, marques2020}
have shown that developers often use keyword-matching as a first search strategy to locate text that might contain information relevant to their tasks.
Thus, this \texttt{baseline} might assist us in interpreting 
how much of the task-relevant text a developer would identify by themselves
when inspecting an artifact at first glance.


We also consider precision and recall metrics per type of artifact, i.e., API documentation, Stack Overflow answers, GitHub issues, and miscellaneous web pages.
We measure how much each metric varies across these artifacts reporting their standard deviation.
The less variation there is, the more a technique consistently identifies task-relevant textual information regardless of an artifact type.


\subsubsection{Results}


Table~\ref{tbl:techniques-results-per-artifact} summarizes the average of precision and recall metrics when identifying task-relevant textual information for all of the test data.
Based on the overall results for each technique, we observe that recall scores range from $0.43$ to $0.58$. 
We also note that applying sentence-level filters improves precision and recall values for \texttt{word2vec}. Notably, \texttt{word2vec} with the \texttt{frame-elements} filter achieves up to $0.49$ recall.
For \texttt{BERT}, recall values range from $0.56$ to $0.58$ and, similar to our evaluation with Stack Overflow artifacts,  we observe that sentence-level filters do not provide substantial changes. 



When we compare evaluation metrics to the \acs{VSM} \texttt{baseline}, we find that the baseline achieves precision and recall scores of $0.30$ and $0.33$ for the same data. 
Although this result is not surprising, it suggests that a developer using keyword-matching could miss much of the task-relevant textual information in an artifact.


Table~\ref{tbl:techniques-results-per-artifact} also details
evaluation metrics artifact-type wise.
Certain techniques, such as \texttt{word2vec} with \texttt{frame-elements}, have better results in specific 
types of artifacts, such as GitHub issues. However, \texttt{word2vec} results are significantly lower 
for other types, such as API documentation or Stack Overflow answers. 
Without filters, \texttt{BERT} performs better at Stack Overflow. For this technique,
we also find that filters decrease 
the amount of variation in the evaluation metrics. Notably, 
\texttt{BERT} with \texttt{frame-associations} is the technique with the highest recall and 
second lowest standard deviation 
suggesting that the technique performs well across different types of artifacts.



\afterpage{
    \input{sections/cp5/tbl-results-overall}
}



When we compare techniques without any filters, differences in the evaluation metrics of each base technique may be explained by how  \texttt{word2vec} and \texttt{BERT} compute relevance. 
That is, all the words in a sentence  contribute equally to \texttt{word2vec}'s identification of task-relevant text via its cosine similarity function.
If an artifact lacks much of the text that appears in a task description, this technique may not be able to identify the text deemed relevant. 
\texttt{BERT} shortens this gap because relevance is computed via its attention mechanism, which might assign more 
weight to words key to determining that the text is relevant to a task thus, explaining why the technique has better evaluation metrics.


With sentence-level filters, there are improvements to most of the types of artifacts evaluated. 
However, in certain techniques, the filters decreased the relevant text identified, as when using the  \texttt{frame-elements} filter in Stack Overflow artifacts.
It is possible that the text in these artifacts lacks any of the frames that we defined as meaningful, and thus, the reason why this filter reduced the amount of text identified.  





\medskip
\begin{bluequote}
    \textit{Results from our evaluation across all artifact types indicate that 
    semantic techniques can find up to 58\% of the task-relevant textual information in an artifact. 
    Some of our techniques also show consistent results over different types of artifacts, providing initial evidence on the generalizability of semantic-based techniques 
     across a variety of software artifacts.}
\end{bluequote}







\subsection{Threats to Validity}
\label{cp5:validity-threats}

% In this section, we discuss threats to the validity of our empirical assessment.



We rely on the golden data of \acs{DS-android}
to evaluate the explored techniques; this impacts the 
conclusions we draw.
This golden data  relies on the
human annotations of text in artifacts that
are related to a task-at-hand (Chapter~\ref{ch:android-corpus}).
There is a possibility that the text marked by the annotators might not contain information 
associated with the solution for the task, or that an annotator missed highlighting
text that they deemed relevant.
We minimize this threat by considering only the 
sentences marked  by at least two annotators as the golden data used in the evaluation of techniques.
We also asked annotators to write a short set of instructions for the tasks they annotated
and provided them an in-house tool that streamlined annotation procedures.
These procedures help mitigate  risks to the validity of our conclusions. We note that
there are no objective means to quantify which text assists task-completion~\cite{Saracevic2007b, Saracevic2007c}. 






The internal validity of the evaluation
may be impacted by the differing number of
types of artifacts in the data set.
While most of the tasks in the data set have
associated 
 Stack Overflow artifacts, not as many have artifacts from GitHub.
These differences may affect the training
and testing of each technique. These
risks are mitigated in that 
 the content of Stack Overflow and GitHub artifacts is similar in length in the data set.



Other internal threats arise from the BERT pre-trained model we used. 
There are base models trained on text from Wikipedia~\cite{Devlin2018Bert},  online news~\cite{peters2018elmo}, 
or source code~\cite{feng2020-codebert}, to  cite a few,
and we could have explored how usage of these different models could impact the accuracy or our 
techniques. Nonetheless, we decided to use 
the BERT base model as published on the model's original paper~\cite{Devlin2018Bert}
to establishing a baseline for future comparisons. 
After fine-tuning, the model we used outperformed 
the word2vec technique with software engineering embeddings~\cite{Efstathiou2018}
and we believe that using the BERT base model did not represent a risk to our study.
Future research should consider how this model compares with other 
general or software-specific pre-trained models.



A second model-related threat concerns how we
divided our data for training and evaluation purposes. 
We split the Android tasks and their associated artifacts into two equal parts using 
25 tasks for training and 25 other tasks for evaluation.
This division might have affected BERT's overall accuracy and it would have been interesting 
to explore other data splits. 
We refrained from investigating other data splits 
because we were interested in testing our approach over 
a number of tasks with Stack Overflow artifacts
so we could compare it to AnswerBot (Section~\ref{cp5:comparison})
 and other splits 
could have affected this evaluation.





The selection of tasks in the Android development domain could 
affect the generalizability of our work. Most notably, 
aspects such as programming languages, frameworks, associated technologies, and others~\cite{baltes2019}
influence the information sought by a developer as well as what they find relevant.
We mitigate this threat by reporting how one of the most 
most promising semantic-based techniques identified as part of our evaluation applies 
to unseen tasks and artifacts associated with the Python programming language (Chapter~\ref{ch:assisting}). 
The three annotators that we recruited to construct the \acs{DS-android} dataset are also not representative 
of the entire developer population. 

% We leave the investigation of software tasks in other programming languages or domains---and their associated natural language software artifacts---to future work.

