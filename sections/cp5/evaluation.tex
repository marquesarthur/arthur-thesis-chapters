\clearpage

\section{Evaluation}
\label{cp5:evaluation}



The goal of our evaluation is to assist the design of tools that use the explored techniques to help developers in locating text relevant to their task. 
To evaluate and compare the techniques,
we report \textit{precision} and \textit{recall} metrics~\cite{manning2010IR} measuring what portion of the \acs{DS-android} text identified by human annotators the techniques automatically identify.
In this context, we believe recall to be the most important metric since failure to identify text that is relevant to a task means that a developer will have an incomplete or partial view of the information needed,
what can lead to faults~\cite{Murphy2005}.
Experimental procedures are as follows.



\subsubsection{Baseline}


Similar to the approach taken to
evaluate similar techniques~\cite{Lin2021} and~\cite{Ye2016}, we use a standard \texttt{VSM} lexical similarity approach as a baseline. Our rationale to use 
lexical similarity as a baseline is based on the fact that 
both our qualitative analysis (\red{Chapter~\ref{aaa}}) and
related work~\cite{Ko2006a, Freund2015} have shown that developers often use keyword-matching as a first search strategy to locate text that might contain information relevant to their tasks.


Analogous to the semantic similarity-based technique (Section~\ref{cp5:approach-w2v}), the baseline technique uses VSM to compute lexical similarity scores. The baseline outputs the top-n sentences with the highest similarity as the ones likely relevant to an input software task.




\subsubsection{Setup}



We configure each technique to identify a target number of 10 relevant sentences per input task-artifact. \gm{Are you thinking of sticking with 10 or making it some \% of the
artifacts?}
This decision is based on the fact that no more than 20\% of the content of any artifact in the corpora is deemed relevant to a task, which, on average, accounts for 8.93 sentences (\red{Chapter~\ref{}}).
Researchers have also used the same target number of 10 sentences when evaluating techniques  (e.g.,~\cite{Xu2017} or~\cite{Lotufo2012}) able to identify relevant text for certain kinds of artifacts what will also facilitate comparing our results to related work.


\subsubsection{Training Data}


In addition to configuring the techniques' output, two of our techniques use part of the  \acs{DS-android} data for fine-tuning purposes (\texttt{BERT}) and to derive sentence-task frame pairs (\texttt{frame-associations}).
We ensure that no data used to evaluate these techniques is also used in their setup by 
splitting the dataset using standard cross-validation techniques.
We split the dataset into 10 folds, each containing 5 tasks used for evaluation purposes. 
We use the remaining 45 tasks to mine sentence-task frame pairs and to train BERT. 
For BERT, we further split the training data and use 10\% of it to validate the model~\cite{Chaparro2017, fucci2019, Petrosyan2015}.
We refer to this setup of BERT as \texttt{BERT\textsubscript{DS-android}}.
\gm{Not quite sure which part is BERT - DS Android}


To study the impacts of training data on BERT, we also train the model on a smaller data set containing six tasks and a total of 1874 sentences, from which 602 of them were deemed relevant by 20 participants with software development experience (\red{Chapter~\ref{aaa}}). Due to the synthetic nature of the tasks in this dataset, we refer to this 
second configuration as \texttt{BERT\textsubscript{DS-synthetic}}.




\subsubsection{Metrics}


We compute values for \textit{precision} and \textit{recall} metrics based on the golden labels available in the \acs{DS-android} corpus and sentences deemed relevant to a task by at least two human annotators.
For a detailed definition of each metric, we refer to the evaluation outcomes in Table~\ref{tbl:type-I-II-errors}, where  columns represent  labels provided by the annotators and rows,
the text identified as relevant or not by a technique.

% 

\medskip
\input{sections/cp5/tbl-eval-outcomes}



\paragraph{\textbf{Precision}}

Precision measures the fraction of the sentences identified that are relevant over the total number of target sentences identified, as shown in Equation~\ref{eq:cp5:precision}.



\begin{equation}
\label{eq:cp5:precision}    
    Precision = \frac{TP}{TP + FP}
\end{equation}



\paragraph{\textbf{Recall}} Recall represents how many of all the annotated sentences are identified by a technique (Equation~\ref{eq:cp5:recall}).


\begin{equation}
\label{eq:cp5:recall}        
    Recall = \frac{TP}{TP + FN}
\end{equation}



\medskip
Precision means identifying only text that is relevant, whereas recall means identifying all relevant text.
Ideally, we would aim for a technique with high precision and recall. Unfortunately, this is often not possible and we must reach a compromise.
Based on studies that have observed that developers face more challenges finding information related to their task~\cite{Robillard2015, Maalej2013}, we argue that in the bound number of sentences outputted by a technique, developers can quickly discard what they judge that is not relevant. \gm{Wording in previous sentence needs fixing.} Hence, we aim to find as much of the relevant text within a technique's output, thus the reason why we favour recall.







\subsection{Results}


Table~\ref{tbl:techniques-results-overall} shows evaluation results. 
In the table, rows provide details about a specific technique while columns discriminate 
precision and recall values and which filters were applied. 
When interpreting the results, it is worth noting that achieving high accuracy in \acs{DS-android} is challenging
due to the fraction of sentences deemed relevant, which comprises 20\% of the entire data.
\gm{Not sure if you are arguing the value is low or high.}
For example, API documents in the corpus have an average of 109.22 sentences per document, from which 9.62 sentences are relevant. 
This means that, with a target number of 10 sentences identified per technique, a $1.0$ recall is only possible when a technique identifies the exact 10 sentences that are relevant for this kind of artifact.



\input{sections/cp5/tbl-results-overall}



The \texttt{baseline} technique, which uses VSM, achieves precision and recall scores of $0.30$ and $0.33$, respectively. 
Although this result is not surprising, it corroborates results from related literature showing limitations of lexicon-based approaches, as detailed in Section~\ref{cp5:background}.





Using \texttt{word2vec}, precision and recall scores increase to $0.46$ and $0.45$. We also observe that applying sentence-semantic filters to \texttt{word2vec}'s output further improves precision and recall values. Notably, the 
\texttt{frame-associations} filter achieves up to $0.52$ recall. These results suggest that both word semantics and sentence
semantics shorten lexical gaps when identifying information relevant to a software task.


Recall values for 
\texttt{BERT} range from $0.57$ to $0.60$ for 
the \texttt{DS-synthetic} and the \texttt{DS-android} configurations.
Sentence semantic filters do not provide substantial changes for \texttt{BERT}, where \texttt{frame-associations} yields $0.61$ recall. 
We believe that the 
lack of differences between the two \texttt{BERT} models are explained by 
the fact that the model uses \textit{transfer learning} and thus, the  
large corpora used to train the model before fine-tuning assists prediction steps even for tasks and artifacts that the model was not trained on. In turn, the lack of improvements when applying semantic filters might relate to how the model's \textit{attention mechanism} may already correlate the text in a task and an artifact (Section~\ref{cp5:bert}).



