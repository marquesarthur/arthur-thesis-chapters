\section{Evaluation}
\label{cp5:evaluation}


Evaluation focuses on two main questions:

\begin{enumerate}
    \item How accurately can our approach detect text considered by humans as being relevant to a particular task?
    
    \item When identifying text relevant to a task, are certain features (i.e., lexical, word-semantics, and sentence-semantics) more informative than others?
\end{enumerate}


To answer these questions, we use our approach to identify sentences relevant to
the tasks in the \acs{DS-android} corpus,
comparing the sentences identified against the 
sentences in the corpus' golden data. 
For this evaluation, we set the target number of sentences identified (i.e., length of a summary) to 20\% of the content of an artifact,
which is a value derived from our study on characterizing 
task-relevant information in natural language artifacts~\cite{marques2020}.





\input{sections/cp5/evaluation-accuracy}

\input{sections/cp5/evaluation-features}



\subsection{Threats to validity}


