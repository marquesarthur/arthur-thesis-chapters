\section{Comparison to Prior Work}
\label{cp5:comparison}



In this section, we evaluate how the techniques that we have proposed and that build on semantic approaches
compare to prior techniques in the literature. Since we are not aware of approaches that 
apply to all the tasks and artifacts in our corpus, we focus on two of the types of artifacts available, 
namely \red{Stack Overflow} answers and \red{Github} issue discussions.
For these artifact types, we compare our techniques to: 



\begin{itemize}
    \item \acf{AnsBot}, a tool that automates generating answers for a developer's task by identifying relevant information on Stack Overflow~\cite{Xu2017};
    \item \acf{Hurried}, a tool that automates the identification of sentences that a developer would first read when inspecting bug reports~\cite{Lotufo2012}.
\end{itemize}



This comparison follows experimental procedures similar to the ones outlined in Section~\ref{cp5:evaluation},
where we report precision and recall metrics using the text in the \acs{DS-android} corpus marked by two or more annotators
as our golden data.

% We report results using the technique/configuration with the best evaluation metrics for the 
% kinds of artifacts considered, leaving a full comparison to \red{Appendix~\ref{}}.



\subsubsection{Comparing the text identified in Stack Overflow answers}



\acs{AnsBot} produces an answer to a developer's technical questions through three steps. 
First it identifies question on the Q\&A platform that are related to the developer's task. 
In a second step, it identifies relevant text within the related questions. 
In the tool's final step, the text identified is used to produce an answer that is succinct yet diverse~\cite{Xu2017}.


AnswerBot uses both textual properties and meta-data available on Stack Overflow and, 
on \acs{AnsBot}'s original evaluation, Xu et al. reported precision scores ranging from $0.66$ to $1.0$. 
Although these results suggest that their tool performs better than our proposed techniques, their evaluation followed a different experimental setup. 
\acs{AnsBot}'s accuracy represents how 
 human evaluators judged the text identified in the tool's output, which comprises only a portion of the entire text of an artifact. On the other hand, 
we evaluate if we can identify text that human annotators deemed relevant when they inspecting the entire text of an artifact.



Due to these differences, we decided to apply \acs{AnsBot} to the tasks and Stack Overflow artifacts in our corpus, measuring precision and recall values for the text identified by the tool. 
We compare results from \texttt{AnsBot} and each of our techniques using
a Wilcoxon-Mann-Whitney test~\cite{mannWhitneyU} to check for statistically significant differences
and, in case of any, computing the magnitude of the difference~\cite{cohen2013statistical}. 



Table~\ref{tbl:techniques-results-stackoverflow} shows values of precision and recall for  \texttt{AnsBot} and the semantic techniques that we explore. 
Overall, we observe that \acs{AnsBot} achieves 0.62 precision and 0.62 recall in our corpus.
Table~\ref{tbl:techniques-results-stackoverflow} also highlights scenarios where \texttt{AnsBot} outperforms our techniques.
Both \texttt{BERT} models achieve comparable results to \texttt{AnsBot}.
We also observe statistically significant differences for 
\texttt{word2vec} without any filters and with \texttt{frame-associations}, suggesting that the technique
fails to identify relevant text identified by \texttt{AnsBot}.


\input{sections/cp5/tbl-results-stackoverflow}



Since \acs{AnsBot} uses both textual properties and meta-data, 
we also investigate how much of the tool's accuracy relies on Stack Overflow specific data.
We disable any of the features used by the tool that rely on meta-data and we compute evaluation metrics once more.
We refer to this configuration in Table~\ref{tbl:techniques-results-stackoverflow} as \texttt{AnsBot\textsubscript{text}}.
Results from this comparison indicate that, without meta-data, all \texttt{BERT} techniques
achieve results 
significantly higher than  \texttt{AnsBot\textsubscript{text}}
while \texttt{word2vec} with \texttt{frame-elements} achieves comparable results.


\medskip
\begin{bluequote}
    \textit{For Stack Overflow artifacts, we can achieve accuracy comparable  
    to a state-of-the-art approach, namely AnswerBot, without relying on Stack Overflow's meta-data.}
\end{bluequote}





\subsubsection{Comparing the text identified in GitHub issue discussions}








% \acf{AnsBot} automates generation of answer summaries for a developer's task using answers available on Stack Overflow.

% The tool uses a set of features, ranging from textual features to features that leverage  Stack Overflow's meta-data (e.g., number of votes an answer got, if a user accepted an answer as correct, etc) to detect useful text that will be used in the answer summary provided by the tool. 

% Given the scope of the tool, we compare the accuracy our technique to AnswerBot when identifying relevant text in 
% Stack Overflow artifacts.




% \subsection{Results}




% AnswerBot relevant text detection algorithm is of particular interest to us. 
% The tool uses three types of features: query related, textual related, and user oriented features to 
% determine if a given sentence is relevant to a query, i.e., the developer's task.
% These features range from checking the co-occurrance of terms available in the developer's question 
% and in a sentence under inspection to whether a sentence originates from the answer with the highest number 
% of votes in the post. Therefore,





% In turn, \texttt{AnsBot\textsubscript{text}} achieves 0.54 precision and 0.53 recall.
% These 
% results suggest that usage of meta-data plays an important role in identifying relevant text for this type of artifact.
% When comparing \texttt{word2vec} to \acs{AnsBot}, we observe that 
%  is the only configuration with results comparable to \acs{AnsBot}.

% We argue similarities in the results arise from the fact that both approaches use the 
% word embeddings trained for the software engineering domain~\cite{Efstathiou2018}
% and that each approach defines a set of semantic filters based on the meaning of a sentence~\cite{Xu2017}. 



% With reagrds to , \texttt{BERT\textsubscript{DS-android}} with 
% \texttt{frame-associations} 

