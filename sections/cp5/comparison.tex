\section{Comparison to Prior Work}
\label{cp5:comparison}



This section evaluates how the techniques that we have proposed and that build on semantic approaches
compare to prior techniques in the literature. Since we are not aware of approaches that apply to all the tasks and artifacts in our corpus, we focus on two of the types of artifacts available, 
namely \red{Stack Overflow} answers and \red{Github} issue discussions.
For these artifact types, we compare our techniques to: 



\begin{itemize}
    \item \acf{AnsBot}, a tool that automates generating answers for a developer's task by identifying relevant information on Stack Overflow~\cite{Xu2017};
    \item \acf{Hurried}, a tool that automates the identification of sentences that a developer would first read when inspecting bug reports~\cite{Lotufo2012}.
\end{itemize}


\red{Explain upfront why these two}

This comparison follows experimental procedures similar to the ones outlined in Section~\ref{cp5:evaluation},
where we report precision and recall metrics using the text in the \acs{DS-android} corpus marked by two or more annotators
as our golden data.

% We report results using the technique/configuration with the best evaluation metrics for the 
% kinds of artifacts considered, leaving a full comparison to \red{Appendix~\ref{}}.



\subsubsection{Comparing the text identified in Stack Overflow answers}
\label{cp5:comparison-answerbot}


\acs{AnsBot} produces an answer to a developer's technical questions through three steps. 
First, it identifies questions on the Q\&A platform that are related to the developer's task. 
The second step identifies relevant text within the related questions
using both textual properties and meta-data available on Stack Overflow. 
In the tool's final step, the tool uses the text identified to produce an answer that is succinct yet diverse~\cite{Xu2017}.


On \acs{AnsBot}'s original evaluation, Xu et al. reported precision scores ranging from $0.66$ to $1.0$. 
Although these results suggest that their tool performs better than our proposed techniques, their evaluation followed a different experimental setup. 
\acs{AnsBot}'s accuracy represents how human evaluators judged the text identified in the tool's output, which comprises only a portion of the entire text of an artifact. On the other hand, 
we evaluate if we can identify text that human annotators deemed relevant when inspecting an artifact's entire text.



Due to these differences, we decided to apply \acs{AnsBot} to the tasks and Stack Overflow artifacts in our corpus, measuring precision and recall values for the text identified by the tool. 
We compare results from \texttt{AnsBot} and each of our techniques using
a Wilcoxon-Mann-Whitney test~\cite{mannWhitneyU} to check for statistically significant differences and, in case of any, computing the magnitude of the difference~\cite{cohen2013statistical}. 



Table~\ref{tbl:techniques-results-stackoverflow} shows values of precision and recall for  \texttt{AnsBot} and the semantic techniques that we explore. 
Overall, we observe that \acs{AnsBot} achieves 0.62 precision and 0.62 recall in our corpus---values explainable by the fact that the tool is tailored specifically to Stack Overflow.


When we compare \texttt{AnsBot} to our techniques, we observe that 
both \texttt{BERT} models achieve comparable results.
We also observe statistically significant differences for 
\texttt{word2vec} without any filters and with \texttt{frame-associations}, suggesting that these techniques
fails to identify relevant text identified by \texttt{AnsBot}.


\input{sections/cp5/tbl-results-stackoverflow}



Since \acs{AnsBot} uses both textual properties and meta-data, 
we also investigate how much of the tool's accuracy relies on Stack Overflow-specific data.
We disable any features that use meta-data, and we compute evaluation metrics once more.
We refer to this configuration in Table~\ref{tbl:techniques-results-stackoverflow} as \texttt{AnsBot\textsubscript{text}}.
Results from this comparison indicate that, without meta-data, all \texttt{BERT} techniques
achieve results 
significantly higher than  \texttt{AnsBot\textsubscript{text}}
while \texttt{word2vec} with \texttt{frame-elements} achieves comparable results.




Results from this comparison indicate that we can achieve accuracy comparable  
to a state-of-the-art approach, namely AnswerBot, 
when identifying relevant information in Stack Overflow artifacts.






\subsubsection{Comparing the text identified in GitHub issue discussions}
\label{cp5:comparison-hurried}


\acs{Hurried} produces a summary of a bug report according to three major hypotheses on what makes a sentence relevant. 
That is, the relevance of a sentence is higher:
\textit{(1)} the more topics, or terms, it shares with other sentences  in the issue discussion, as when multiple sentences discuss the same software component;
\textit{(2)} the more a sentence shares terms with a task's title or description; and 
\textit{(3)} the more that sentence is evaluated by other sentences.
These three hypotheses are used to build a graph of relationships between all the sentences in a bug report, which 
the tool inputs to PageRank~\cite{Page1999} to identify a target number of sentences as the most 
 relevant ones to the task at hand. 



\acs{Hurried} original evaluation focuses on identifying sentences that summarize the information in a bug report, where the tool achieves precision and recall values of $0.59$ and $0.51$, respectively.
Our evaluation differs from theirs as we focus on sentences containing key information to complete a software task, which is essentially different from summarizing a bug report on its own. These differences prompted us to apply \acs{Hurried} to the tasks and Github issues available in our corpus\footnote{This does not require modifications to the tool. We only change the tool's input parameters.}. When comparing \acs{Hurried} to our techniques, we follow procedures analogous to the ones used for AnswerBot (Section~\ref{cp5:comparison-answerbot}).



Table~\ref{tbl:techniques-results-github} presents evaluation metrics when identifying text that is relevant to a task 
in GitHub artifacts. In our corpus, \texttt{Hurried} achieves 0.53 precision and 0.51 recall---values that support their findings on the fact that 50\% of the information in a summary is not relevant~\cite{Lotufo2012}.
We observe that, with the exception of \texttt{word2vec}, all our techniques achieve comparable results
and that \texttt{BERT\textsubscript{DS-synthetic}} yields the best results. 




\input{sections/cp5/tbl-results-github}



Results from this comparison suggest that we can achieve accuracy comparable to or higher than 
bug summarizers when identifying relevant information in artifacts from Github.





