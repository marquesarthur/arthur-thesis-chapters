\section{Comparison to Related Work}
\label{cp5:comparison}





In this section, we evaluate how the techniques that we have proposed and that build on semantic approaches
compare to other techniques in the literature. Since we are not aware of approaches in the literature 
that apply to all the tasks and artifacts in our corpus, we focus on two of the types of artifacts available, 
namely \red{Stack Overflow} answers and \red{Github} issue discussions.
For these artifact types, we compare the text identified 
by our techniques to the text identified by techniques that apply to the same artifacts: 



\begin{itemize}
    \item \acf{AnsBot}, a tool that automates generation answers for a developer's task by identifying relevant information in Stack Overflow answers;
    \item \acf{Hurried}, a tool that automates the identification of sentences a developer would first read when inspecting bug reports.
\end{itemize}





This comparison follows experimental procedures similar to the ones outlined in Section~\ref{cp5:evaluation}.
We report results using the technique/configuration with the best evaluation metrics for the 
kinds of artifacts considered, leaving a full comparison to \red{Appendix~\ref{}}.




\subsubsection{Comparing the text identified in Stack Overflow answers}



\acs{AnsBot} produces an answer to a developer's technical questions through three steps. 
First it identifies question on the Q\&A platform that are related to the developer's question. 
In a second step, it identifies relevant text within the related posts, which are used 
to produce an answer that is succinct yet diverse.
This is the final step in the tool's approach, where the text identified in the related questions 
is combined in an answer that has no repeated information.


The tools uses both textual properties and meta-data available on Stack Overflow and, 
on \acs{AnsBot}'s original evaluation, Xu et al. reported accuracy results ranging from 0.66 to 1.0. 
These results would suggest that their tool performs better than our proposed techniques when identifying relevant text.
However, these results arise from a different experimental setup. That is, Xu et al. asks human evaluators 
to judge whether text identified by the tool contains information related to a task
while we evaluate if we can identify text that human annotators 
marked as relevant when inspecting the entire text of an artifact.


Due to these differences, we decided to apply \acs{AnsBot} to the tasks and Stack Overflow artifacts in our corpus, 
measuring precision and recall values for the relevant text identified by the tool.
Results indicate that, in our corpus, \acs{AnsBot} achieves 0.59 precision and 0.57 recall. 
In comparison, \texttt{BERT} with \texttt{frame-element} filters achieve 0.59 precision and 0.65 recall,
suggesting that our approach achieves similar accuracy without relying on an artifact's meta-data.


% \acs{AnsBot} has a comprehensive set of features related to the text itself. 
% For instance, the tool uses \textit{(1)} a set of semantic patterns that often appear in text deemed relevant,
% \textit{(2)} word embeddings trained on domain-specific corpora, and others.
Due to similarities between the textual properties in \acs{AnsBot}  and our techniques, 
we also investigate how the tool's accuracy when ignoring Stack Overflow specific data, such as the number of votes in the answer.
For this comparison, we disable any of the features used by the tool which rely on meta-data and
we compute evaluation metrics once more.
Results indicate that, without artifact specific properties, \acs{AnsBot} achieves 0.54 precision and 
0.53 recall, values lower than the ones reported by our best \texttt{BERT} technique, but still higher 
than our best \texttt{word2vec} technique.




Results from this evaluation suggest that, for Stack Overflow artifacts, we can achieve accuracy comparable  
to a state-of-the-art approach.





\subsubsection{Comparing the text identified in GitHub issue discussions}








% \acf{AnsBot} automates generation of answer summaries for a developer's task using answers available on Stack Overflow.

% The tool uses a set of features, ranging from textual features to features that leverage  Stack Overflow's meta-data (e.g., number of votes an answer got, if a user accepted an answer as correct, etc) to detect useful text that will be used in the answer summary provided by the tool. 

% Given the scope of the tool, we compare the accuracy our technique to AnswerBot when identifying relevant text in 
% Stack Overflow artifacts.




% \subsection{Results}




% AnswerBot relevant text detection algorithm is of particular interest to us. 
% The tool uses three types of features: query related, textual related, and user oriented features to 
% determine if a given sentence is relevant to a query, i.e., the developer's task.
% These features range from checking the co-occurrance of terms available in the developer's question 
% and in a sentence under inspection to whether a sentence originates from the answer with the highest number 
% of votes in the post. Therefore,