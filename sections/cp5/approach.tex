\section{Approach}
\label{cp5:approaches}


We explore three techniques to automatically identify text---in a software artifact---that is relevant to a particular software task.
These techniques encompass lexical similarity, word semantics, and sentence semantics and are detailed later in this section.



All techniques take a task and a pertinent artifact as inputs and output the sentences 
that are most likely to contain information that assists a developer in completing their task. 
To determine how many sentences a technique should identify, we consider that 
no more than 20\% of the content in the artifacts in the
 \acs{DS-synthetic} and the \acs{DS-android} corpora are deemed relevant to a task, which, on average, accounts for 8.93 sentences. 
We approximate these values to a target number of 10 sentences per input task-artifact. 
Our decision to output a certain number of sentences regardless of the technique is to have an easy framework for their evaluation.



\subsection{Lexical Similarity}

As a baseline technique, we use the Vector Space Model (VSM)~\cite{Salton1975vsm} from Information Retrieval
to compute the lexical similarity between the sentences within a pertinent artifact and a task. 
Using this model, we can identify the sentences with highest lexical similarity 
as the ones most likely to contain information relevant to the input task.




VSM represents both a task and individual sentences within an artifact as vectors of term weights,
where the weight of a term
can be computed using a Term-Frequency Inverse-Document-Frequency (\textit{tf.idf}) scheme.
\textit{tf.idf} accounts for how frequent a term is in an artifact (\textit{tf}) as well as how 
unique that term is across an entire corpora (\textit{idf})~\cite{Manning2009IR}.
Once we obtain vector representations $t$ and $s$ 
for an input task and an arbitrary artifact sentence, 
their lexical similarity can be computed 
using the cosine similarity between their vectors, as Equation~\ref{eq:lex-sim} shows:



\begin{equation}
    cos(t,s) = \frac{t^Ts}{\|t\| \|s\|}
    \label{eq:lex-sim}
\end{equation}


By automatically ranking the sentences in an artifact according to their similarity scores, i.e., from highest to lowest 
lexical,   we can  select the top-n sentences as the ones relevant to an input task.




\subsection{Word Semantics}


\art{This section may be hard to follow. Ideally, I want to provide general concepts here and expand them in the subsections below}


Harris' distributional hypothesis states that words that appear in a similar context tend to have similar meanings~\cite{harris1954distributional}.
This hypothesis is the base for neural language models~\cite{Mikolov2013, Devlin2018Bert} that capture words' semantics
as vector representations, namely \textit{word embeddings}. 


 
Early language models, such as the skip-gram model~\cite{Mikolov2013}, provide vector representations at the word level.
If two words have embeddings close in the model's vector space, we can identify semantically similar words even when if these words are lexically different~\cite{Mikolov2013}. For example, 
the model could identify that \textit{crane} and \textit{stretch} are similar words that represent the action of stretching \red{ref}.






However, such models would not be able to disambiguate if the word \textit{crane} represented a machine, a bird, or a stretch, since they operate at the word level.
In contrast, state-of-the-art models~\cite{Devlin2018Bert} can
provide different vectors for the same word based on the sentence in which a word appears.
This additional layer allows for more complex operations such as word disambiguation. 



We apply both simple and state-of-the-art model to 
automatically identify the sentences in a pertinent artifact most likely to contain information relevant to
the input task as detailed below.








\subsubsection{Semantic Similarity}






To compute the semantic similarity between the sentences within a pertinent artifact and a task,
we use the Skip-gram model~\cite{Mikolov2013} with word embeddings specifically trained for the software engineering domain~\cite{Efstathiou2018}.
Analogous to lexical similarity,  we can identify the sentences with highest semantical similarity 
as the ones most likely to contain information relevant to the input task.



Note that, since word embeddings provide vector representations at the word level, we compute vector representations 
at the sentence level by averaging the sum of the word embeddings of some input text.
Provided that we have sentence-level embeddings $w_t$ and $w_s$ for the text 
of an input task and an arbitrary artifact sentence, 
their semantic similarity can also be obtained 
using the cosine similarity: 



\begin{equation}
    cos(w_t,w_s) = \frac{w_t^Tw_s}{\|w_t\| \|w_s\|}
    \label{eq:word-sim}
\end{equation}




\subsubsection{BERT}

% https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
% https://jalammar.github.io/illustrated-transformer/
% https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html


% At the time of their introduction, language models primarily used recurrent neural networks (RNN) and convolutional neural networks (CNN) to handle NLP tasks.




% By looking at all surrounding words, the Transformer allows the BERT model to understand the full context of the word, and therefore better understand searcher intent.

% This is contrasted against the traditional method of language processing, known as word embedding, in which previous models like GloVe and word2vec would map every single word to a vector, which represents only one dimension, a sliver, of that word's meaning.






% Look at the set of encoder hidden states it received – each encoder hidden states is most associated with a certain word in the input sentence
% Give each hidden states a score (let’s ignore how the scoring is done for now)
% Multiply each hidden states by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores




\subsection{Sentence Semantics}



\clearpage


