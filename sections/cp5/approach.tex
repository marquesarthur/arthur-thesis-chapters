\clearpage

\section{Techniques}
\label{cp5:approaches}





We consider the semantic approaches outlined in Section~\ref{cp5:background} and we explore 
how can we use them, or their combination, in the design of techniques able to automatically identify text that likely contains information that is relevant to a developer task. 
More specifically, we investigate determining the relevance of a sentence to a given task using 
\textit{(1)} word2vec, or the Skip-gram model, \textit{(2)} BERT, and \textit{(3)} frame semantics.

% \begin{itemize}
%     \item using \textit{Skip-gram model}, or \textit{word2vec}, to identify sentences that are semantically similar to an input task and thus, likely to contain information relevant to that task;
    
%     \item fine-tuning \textit{BERT} to identify sentences that are potentially relevant to an input task; and

%     \item using \textit{frame semantics} to filter relevant sentence's based on their meaning.
% \end{itemize}



Each technique takes a task and a pertinent artifact as inputs and outputs the sentences that are most likely to contain information that assists a developer in completing that task.
We present details for each technique in the following sections.




\subsection{Identifying Task-relevant Text through Semantic Similarity}
\label{cp5:approach-w2v}


As a first technique, we explore if the sentences with the highest semantical similarity are most likely to contain information relevant to the input task. 


To compute the semantic similarity between the sentences $\{a_1, a_2, \dots, a_n\}$ within a pertinent artifact and the sentences $\{t_1, t_2, \dots, t_m\}$ in a task, 
we use the Skip-gram model (Section~\ref{cp5:skip-gram}) with word embeddings trained for the software engineering domain~\cite{Efstathiou2018}. 
First, we convert each of the sentences to their respective vector representations (i.e., $w_t$ and $w_a$) following the guidelines from Conneau et al.~\cite{conneau2018}, i.e., we average the sum of the embeddings for each word in a sentence.
Then, we compute their semantic similarity as the cosine similarity \red{ref} between their corresponding sentence vectors:


\begin{equation}
    sim(w_t,w_a) = \frac{w_t^Tw_a}{\|w_t\| \|w_a\|}
    \label{eq:word-sim}
\end{equation}
 
\smallskip
 As there are multiple sentences in both entities, the semantic similarity of a sentence $a_i \in A$ is the maximum value obtained for this sentence and each of the sentences in a task $T$.

 \begin{equation}
    sim(a_i, T) = max(sim(w_{a_i}, w_{t_1}), sim(w_{a_i}, w_{t_2}) ~\dots~ sim(w_{a_i}, w_{t_m})) \\
\end{equation}

\smallskip
Following these procedures, we compute the semantic similarity of all the sentences in an input artifact with regards to an input task and then, 
we sort the obtained values from highest to lowest. We output the top-n sentences with highest similarity as the ones likely relevant to an input software task.


\subsection{Identifying Task-relevant Text with BERT}
\label{cp5:approach-bert}



We fine-tune BERT to identify the sentences in a natural language artifact that are potentially relevant to an input task.



As we described in Section~\ref{cp5:bert}, we use a BERT model that has been already pre-trained in a massive corpora, namely BERT\textsubscript{uncased}~\cite{Devlin2018Bert}, and we tune it to classify if a sentence is relevant, or not, to a task.
To this end, we take the sentences $\{a_1, a_2, \dots, a_n\}$ within a pertinent artifact and the sentences $\{t_1, t_2, \dots, t_m\}$ in a task and we feed them to the model as sentence-artifact input pairs 
alongside binary labels representing whether human annotators deemed that sentence relevant. 
Using training data, the model will learn associations between each pair such that it can predict if a sentence is relevant for the task provided. 

We fine-tune the model by training it for up to 10 epochs with a \textit{batch size} of 64 and a sequence length of 64. Cross-Entropy is our loss function, and the model is trained to minimize it using the Adam optimizer at a learning rate of \textit{1e-5} with early stopping. 
We take a fully trained model and we compute the relevance of a sentence $a_i \in A$ as the most commonly predicted label for this sentence when we pair it to each of the sentences in a task $T$.


Similar to the word2vec approach, we predict the relevance of all the sentences in an input artifact with regards to an input task and we use prediction probabilities to select 
the top-n sentences with highest probability as the relevant ones.

% Since the sentences in a task and software artifact are fairly short, with an average length of 26 tokens, we also set the model's



% \begin{equation}
%     relevance(a_i, T) = mod(predict(a_i, t_1), predict(a_i, t_2) ~\dots~ predict(a_i, t_m)) \\
% \end{equation}





\subsection{Identifying Task-relevant Text with Frame Semantics}
\label{cp5:approach-filters}

% \marginpar{{\small \gm{Isn't there multiple SEframes approaches?} \art{There should be two techniques now}}}



We use frame semantics to identify sentences potentially relevant to a task based on the frame elements available in a sentence and task. 
For that, we implement a set of filters that can be applied to the previous techniques as a post-processing step~\cite{Manning2009IR}.
A filter takes the output of a technique $O$ and produces output $O^{\prime}$ where a sentence must match the filter's relevance criteria.


A first filter considers that sentences that give instructions, describe required events, obligations, and others \gm{is there a full list to refer to?}
are likely to contain information of interest. \gm{Why?}
This filter takes a sentence $a_i \in A$ and checks whether any of the frame elements of this sentence match one meaningful frame $f \in F$. 





\begin{equation}
\begin{split}
F = \{ \text{required event}, \text{using}, \text{being obligated}, \text{causation}, \text{attempt}, \text{execution} \} 
\end{split}
\end{equation}





\begin{equation}
\text{\textit{frame-elements}}(a_i) = \left \{
\begin{aligned}
    &1, && \text{if}\ frames(a_i) \cap ~F \ne \emptyset \\
    &0, && \text{otherwise}
\end{aligned} \right.
\end{equation} 


\medskip
A second filter considers that the relevance of a sentence depends on the intentionality of a task. 
For example, for a task that requires diagnosing an error, sentences in a bug report that describe success or failure are likely relevant. 
This rationale is represented by a set of associated frame pairs, where each element represents a frame originating from a sentence and another frame originating from a task.
This filter checks if sentence-task frame pairs obtained from an input artifact and input task contain any of the pairs from a pre-established set. \gm{Where do these pairs come from?}




\begin{equation}
\text{\textit{frame-association}}(a_i, T) = \left \{
\begin{aligned}
    &1, && \text{if}\ frame\_pairs(a_i, T) \cap ~P \ne \emptyset \\
    &0, && \text{otherwise}
\end{aligned} \right.
\end{equation} 

\gm{where is P defined?}

\smallskip
We derive the frame pairs that compose $P$ by splitting the tasks in our corpus.
\gm{Just the tasks or the artifacts? Which tasks? Which corpus?}
Part of the tasks and artifacts are used to automatically
mine associated frame pairs using the apriori algorithm~\cite{agrawal1994apriori}. 
\gm{More detail needed.} We test the obtained pairs in tasks and artifacts from which no pairs were derived from.  \gm{How do you test?}
Section~\ref{cp5:evaluation} further details our data split.







% ------------------------------------------------


\subsection{Techniques Summary}


Table~\ref{tbl:approaches-summary} summarizes the techniques that we explore.
The table provides a short description and identifier for each technique. From now on, we refer to each technique according to its short identifier.


\input{sections/cp5/tbl-approaches-summary}