
\section{Techniques}
\label{cp5:approaches}



We introduce six semantic techniques that might 
aid developers in identifying
task-relevant text in software artifacts. Three of the
techniques use word embeddings (word2vec)~\cite{Mikolov2013} as a base for determining a ranked list of similar sentences between an artifact
and a task description and three use BERT~\cite{Devlin2018Bert} for the same purpose.
Each of these techniques takes as input
a task description and an artifact. The technique then identifies
sentences in the artifact pertinent to the task. The output
from the technique is a ranked list of sentences extracted from
the artifact from most likely to be helpful for solving the
task to the least likely.



Table~\ref{tbl:approaches-summary} provides an overview of the six techniques.
Two techniques simply return the results of \textit{word2vec} or \textit{BERT}.
The four others apply \textit{frame semantics}~\cite{fillmore1976frame} to filter
 each of the sentences returned by the base techniques.
A first filter---\textit{frame-elements}---retains only sentences
that include certain frames. The second filter---\textit{frame-associations}---keeps
sentences with certain co-occurring 
frames which are extracted from the task description 
and the artifact's sentences. We describe word2vec, BERT and the filters in turn to
explain the six techniques.



\input{sections/cp5/tbl-approaches-summary}


\subsection{Identifying Task-relevant Text with word2vec}
\label{cp5:approach-w2v}



To form the base of the first three techniques,  we use word semantics via word2vec in conjunction with information retrieval approaches (Chapter~\ref{ch:related-work}).
Information retrieval techniques are usually applied to look for documents relevant to a query prompted by a user~\cite{Bavota2016}, where the relevance of a document to a query is based on some function that indicates how similar the document's content is to that query~\cite{Manning2009IR}. Given the right \textit{similarity function}, \acs{IR}  techniques 
can be also applied to identify sentences within a document (artifact in our problem formulation) that are relevant to a task.



% This and other challenges have been lifted by advancements in the fields of \acf{ML} and \acf{DL}~\cite{ferreira2021, li2018deep}, which walked hand-to-hand with improvements in computational power and the amount of memory available in modern computer architectures~\cite{sharafi2015}.




We use the \textit{Skip-gram} model~\cite{Mikolov2013}, also referred to as \textit{word2vec}, to define such a similarity function.
The \textit{Skip-gram} model exploits Harris' distributional hypothesis~\cite{harris1954distributional}---which states that words that appear in a similar context tend to have similar meanings---and builds vector representations, namely \textit{word embeddings}, for each of the words in a text corpus.
With a significantly large text corpus, the model associates similar vector embeddings to words that are similar in meaning~\cite{Ye2016}.


Since \textit{word2vec} represents words through a vector space, the similarity of two words $i$ and $j$ can be obtained computing the cosine similarity~\cite{Manning2009IR} between the corresponding vectors of each word, i.e., $w_i$ and $w_j$:

\begin{equation}
    sim(w_i,w_j) = \frac{w_i^Tw_j}{\|w_i\| \|w_j\|}
    \label{eq:word-sim}
\end{equation}
 
\smallskip
In turn, the similarity of two sentences can be obtained  by first computing a vector representation for the entire sentence and then, by measuring the cosine angle between the obtained sentence vectors.
To obtain sentence vectors, \rev{we are guided by related work (e.g.,~\cite{Ye2016, Jiang2016b})} and 
 we average the sum of the embeddings for each word in a sentence~\cite{conneau2018}.




Following these procedures, we use a Skip-gram model with word embeddings trained for the software engineering domain~\cite{Efstathiou2018} to compute the semantic similarity, or relevance, of the sentences $\{a_1, a_2, \dots, a_n\}$ within a pertinent artifact $A$ and the sentences $\{t_1, t_2, \dots, t_m\}$ in a task  $T$. As both entities have multiple sentences, the semantic similarity of a sentence $a_i \in A$ is the value obtained for this sentence and 
a vector representing the average of the embeddings of all the sentences in a task.


After we compute the semantic similarity of all the sentences in an input artifact with regards to an input task, we sort the obtained values from highest to lowest, outputting the top-n sentences with highest similarity as the ones likely relevant to an input software task.


% \begin{equation}
%    sim(a_i, T) = max(sim(w_{a_i}, w_{t_1}), sim(w_{a_i}, w_{t_2}) ~\dots~ sim(w_{a_i}, w_{t_m})) \\
% \end{equation}
    

\subsection{Identifying Task-relevant Text with BERT}
\label{cp5:approach-bert}


In the Skip-gram model, context refers to the sentences used to train the model. This approach to context  does not allow the model to disambiguate words based on their surrounding text. In other words, a Skip-gram model will have a single vector representation for a word such as `\textit{company}' even when it can have different meanings, i.e., a business organization or being with someone. In contrast, 
the \acf{BERT}~\cite{Devlin2018Bert} provides different representations for the same word based on the sentence in which a word appears.
This additional layer allows the model to perform more complex operations,
leading to state-of-the-art results in several language tasks~\cite{Devlin2018Bert}. 





Typically, BERT is trained 
on a massive amount of data. During training, a percentage of the tokens in a sentence---usually 15\%---are replaced with a special token and the model is optimized to predict these replaced, or \textit{masked}, tokens. 
The model relies on a mechanism, called \textit{attention}~\cite{Vaswani2017attention} that correlates and weights all non-replaced tokens in the input so that the model maximizes predictions.
To fully train a BERT model, one first creates a base model using a large amount of text and the general token prediction and sentence prediction tasks. 
Then, this base model is fine-tuned to specific tasks, such as text classification, using a dataset specific to the fine-tuning steps. The procedures of \textit{training} and \textit{fine-tuning}
allow using the model even when training data is scarce and the model transfers
what it learned during pre-training steps to the fine-tuning steps~\cite{Devlin2018Bert}.


We posit that BERT's attention mechanism and fine-tuning procedures can be used to train a model to classify 
sentences in a natural language artifact as likely relevant to software task. Hence, the second group of three techniques we introduce uses a BERT model to classify if a sentence is relevant or not to a task.
To this end, we take the sentences $\{a_1, a_2, \dots, a_n\}$ within a pertinent artifact $A$ and the sentences $\{t_1, t_2, \dots, t_m\}$ in a task $T$ and we feed them to the model as task-artifact input pairs 
alongside binary labels representing whether that sentence is relevant. 
Using training data, the model's attention mechanism will learn associations between each pair such that it can predict if a sentence is relevant for the task provided. 


\rev{We hyper-parameters commonly adopted by software engineering researchers to fine-tune our model~\cite{watson2022, ferreira2021, li2018deep}.}
We use up to 10 epochs to fine-tune BERT, setting both \textit{batch size} and a \textit{sequence length} to 64. Cross-Entropy is our loss function, and the model is trained to minimize it using the Adam optimizer at a learning rate of \textit{1e-5} with early stopping. 
Since there are far fewer relevant sentences in an artifact and, consequently,
in the entire data, we apply \textit{undersampling} of the majority class 
to improve the prediction performance on imbalanced data~\cite{tan2015online}.
For every relevant sentence in an artifact, we randomly sample four 
non-relevant sentences in that same artifact such that the final ratio 
between each class is \textit{1} relevant sentence for every \textit{4} non-relevant ones.





Following these procedures, we take a fully trained model and we fine-tune it to compute the relevance of a sentence $a_i \in A$ when we pair it to a vector that represents the sentences in a task $T$.
Similar to the word2vec approach, we predict the relevance of all the sentences in an input artifact with regards to an input task and we use prediction probabilities to select 
the top-n sentences with highest probability as the relevant ones.




% \begin{equation}
%     relevance(a_i, T) = mod(predict(a_i, t_1), predict(a_i, t_2) ~\dots~ predict(a_i, t_m)) \\
% \end{equation}





\subsection{Filtering Task-relevant Text with Frame Semantics}
\label{cp5:approach-filters}

% \marginpar{{\small \gm{Isn't there multiple SEframes approaches?} \art{There should be two techniques now}}}



Although word2vec and BERT have provided significant improvements to a diverse set of natural language tasks, 
they still capture semantics at the word level. 
If we seek to infer a sentence's meaning, an alternative would be to consider  \textit{frame semantics} (Chapter~\ref{ch:characterizing}).


We define two filters using frame semantics that can be
applied with either the word2vec or BERT base techniques as
 a post-processing step to identify sentences potentially relevant to a task based on the frame elements available in a sentence.
For that, we implement a set of filters that can be applied to the previous techniques as a post-processing step~\cite{Manning2009IR}.
A filter takes the output of a technique $O$ and produces output $O^{\prime}$ where a sentence must match the filter's relevance criteria. For both filters, we use SEFrame~\cite{marques2021} to assign frames relevant to
software engineering text.




% \subsubsection{test}

\subsubsection{Frame-elements}

This filter considers sentences identified with
frames giving instructions, describing required events, explaining system procedures, and others as
 likely to contain information of interest. The rationale for
 the set of frames used in this filer is based on Chapter~\ref{ch:characterizing}'s findings on the prominence of certain frames as an indicator of a sentence's relevance to a software task.
This filter takes a sentence $a_i \in A$ and checks whether any of the frame elements
of this sentence match one meaningful frame $f \in F$ from a pre-established set\footnote{See full list in our replication package~\cite{dsandroid}}. 

\begin{small}
\begin{equation}
F = \{ \text{being obligated}, \text{causation}, \dots \text{required event}, \text{using} \} 
\end{equation}

\begin{equation}
\text{\textit{frame-elements}}(a_i) = \left \{
\begin{aligned}
    &1, && \text{if}\ frames(a_i) \cap ~F \ne \emptyset \\
    &0, && \text{otherwise}
\end{aligned} \right.
\end{equation} 
\end{small}


\subsubsection{Frame-associations}

This filter considers that the relevance of a sentence depends on the intentionality of a task. 
For example, for a task that requires diagnosing an error, sentences in a bug report that describe success or failure 
of an action are likely relevant. 
This rationale is represented by a set of associated frame pairs, where each element represents a frame originating from a task and another frame originating from a sentence.
This filter checks if a task-artifact frame pair obtained from an input artifact and input task appear in a pre-established set of pairs\footnote{See full list in our replication package~\cite{dsandroid}}.


\begin{small}
\begin{equation}
P = \{ (\text{execution}, \text{being obligated}), \dots (\text{questioning}, \text{using}) \} 
\end{equation}

\begin{equation}
\text{\textit{frame-associations}}(a_i, T) = \left \{
\begin{aligned}
    &1, && \text{if}\ pair(a_i, T) \cap ~P \ne \emptyset \\
    &0, && \text{otherwise}
\end{aligned} \right.
\end{equation} 
\end{small}



We identify frame-pairs $p \in P$ using association rule mining~\cite{agrawal1994apriori}, which identifies 
frame elements that co-occur in a task's title and in the text marked as relevant in the artifacts associated to each task.
We mine pairs using the apriori algorithm~\cite{apriori-python} with a minimum support level---the minimum number of times the frame element
pairs must appear across the data---of $0.10$. Section~\ref{cp5:evaluation} further details the data used to identify rules.




% ------------------------------------------------


% \subsection{Techniques Summary}


% Table~\ref{tbl:approaches-summary} summarizes the techniques that we explore.
% The table provides a short description and identifier for each technique. From now on, we refer to each technique according to its short identifier.

