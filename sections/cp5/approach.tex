\section{Techniques}
\label{cp5:approaches}


\art{I'm trying to be consistent with the nomenclature: \texttt{approaches} refer to the NLP approaches that I use while \texttt{techniques} how I use them to identify relevant text}


We consider the approaches outlined in Section~\ref{cp5:background} and we explore 
how can we use them, or their combination, in the design of techniques able to automatically identify text that likely contains information that is relevant to a developer task. 
Our design space considers establishing the relevance of a sentence to a given task:

\begin{itemize}
    \item using \textit{Skip-gram model}, or \textit{word2vec}, to identify sentences that are semantically similar to an input task and thus, likely to contain information relevant to that task;
    
    \item fine-tuning \textit{BERT} to identify sentences that are potentially relevant to an input task; and

    \item using \textit{frame semantics} to filter relevant sentence's based on their meaning.
\end{itemize}


Details for each technique are presented in the following sections.
Note that each techniques takes a task and a pertinent artifact as inputs and outputs the sentences 
that are most likely to contain information that assists a developer in completing that task. 
Details on how we determine the number of sentences outputed are available in Section~\ref{cp5:evaluation}.



\subsection{Identifying Task-relevant Text through Semantic Similarity}
\label{cp5:approach-w2v}


As a first technique we explore if the sentences with the highest semantical similarity are most likely to contain information relevant to the input task. 


To compute the semantic similarity between the sentences $\{a_1, a_2, \dots, a_n\}$ within a pertinent artifact and the sentences $\{t_1, t_2, \dots, t_m\}$ in a task, we use the Skip-gram model (Section~\ref{cp5:skip-gram}) with word embeddings trained for the software engineering domain~\cite{Efstathiou2018}. First, we covert each of the sentences to their respective vector representations, $\{ w_{a_1}, w_{a_2}, \dots, w_{a_n}\}$ and $\{ w_{t_1}, w_{t_2}, \dots, w_{t_m}\}$. For that, we follow Conneau et al.'s guidelines~\cite{conneau2018} 
 and average the sum of the word embeddings in a sentence to obtain a vector representation for the entire sentence.
Then, we compute their semantic similarity as the cosine similarity \red{ref} between their corresponding vectors:


\begin{equation}
    sim(w_t,w_a) = \frac{w_t^Tw_a}{\|w_t\| \|w_a\|}
    \label{eq:word-sim}
\end{equation}
 
\smallskip
 As there are multiple sentences in both entities, the semantic similarity of a sentence $a_i \in A$ is the maximun value obtained when computing the similarity of the vector for this sentence and the vectors for each of the sentences in a task $T$.

 \begin{equation}
    sim(a_i, T) = max(sim(w_{a_i}, w_{t_1}), sim(w_{a_i}, w_{t_2}) ~\dots~ sim(w_{a_i}, w_{t_m})) \\
\end{equation}

\smallskip
Following these procedures, we sort the sentences in an artifact according to their semantic similarity, i.e., from highest to lowest, and output the top-n sentences as the ones likely relevant to an input software task.


\subsection{Identifying Task-relevant Text with BERT}
\label{cp5:approach-bert}



We fine-tune BERT to identify the sentences within an artifact that most likely contain information relevant to the task.



As we described in Section~\ref{cp5:bert}, we use a BERT model that has been already pre-trained in a massive corpora, namely BERT\textsubscript{uncased}~\cite{Devlin2018Bert}, and we tune it to identify task-relevant text.
To this end, we take the sentences $\{a_1, a_2, \dots, a_n\}$ within a pertinent artifact and the sentences $\{t_1, t_2, \dots, t_m\}$ in a task and we fed them to the model as sentence-artifact input pairs 
alongside binary labels representing whether human annotators deemed that sentence relevant. 
Using training data, the model will learn associations between each pair such that it can predict if a sentence is relevant for the task provided. 






To identify sentence within an artifact that are likely relevant to an input task, we take a fully trained model and we compute the relevance of a sentence $a_i \in A$ as the most commonly predicted label for this sentence when we pair it to each of the sentences in a task $T$.

\begin{equation}
    relevance(a_i, T) = mod(predict(a_i, t_1), predict(a_i, t_2) ~\dots~ predict(a_i, t_m)) \\
\end{equation}





\subsection{Identifying Task-relevant Text with Frame Semantics}
\label{cp5:approach-filters}

\marginpar{{\small \gm{Isn't there multiple SEframes approaches?} \art{There should be two techniques now}}}




In Section~\ref{cp5:frame-semantics}, we described how frame semantics can help identify the meaning of software engineering text. Hence, we identify sentences potentially relevant to a task based on the frame elements available in a sentence. For that, 
 we implement a set of filters that can be applied to the previous techniques as a post-processing step~\cite{Manning2009IR}.

A first filter considers that sentences that give instructions, describe required events, obligations, and others are likely to contain information of interest. This filter takes a sentence $a_i \in A$ and checks whether any of the frame elements of this sentence match one meaningful frame $f \in F$. 




\begin{equation}
\begin{split}
F = \{ \text{required event}, \text{using}, \text{being obligated} ... \} \\
\end{split}
\end{equation}


\begin{equation}
\text{\textit{meaningful}}(a_i) = \left \{
\begin{aligned}
    &1, && \text{if}\ frames(a_i) \cap ~F \ne \emptyset \\
    &0, && \text{otherwise}
\end{aligned} \right.
\end{equation} 


\smallskip
A second filter takes into account the intentionally of a task, as identified through the frame elements in the task's tile, and tries to match it to a set of meaningful frame pairs $p \in P$. For example, a sentence that has frames describing actions that one must take (\textit{required event})  is likely relevant to a task where a developer asks how to use some API element, as identified thorugh the \textit{using} frame.
\marginpar{{\small \art{I need to find a better name for this function}}}


\begin{equation}
\text{\textit{related}}(a_i, T) = \left \{
\begin{aligned}
    &1, && \text{if}\ frame\_pairs(a_i, T) \cap ~P \ne \emptyset \\
    &0, && \text{otherwise}
\end{aligned} \right.
\end{equation} 

\smallskip
To find the frame pairs that compose $P$ we \red{describe procedures on how we determine pairs}.



Considering these two filters we take the output of a technique $O$ and produce output $O^{\prime}$ containing only the sentences that match one of the proposed filters.
\art{Do they need to be filters? Can they be heuristic-based techniques on their own?}

% ------------------------------------------------


\subsection{Techniques Summary}


Table~\ref{tbl:approaches-summary} bundles the techniques that we explore.
The table provides provides a short description and identifier for the technique. From now on, we refer to each technique according to their short identifier.


\input{sections/cp5/tbl-approaches-summary}