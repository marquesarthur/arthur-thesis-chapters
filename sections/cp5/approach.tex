\section{Techniques}
\label{cp5:approaches}







We consider the approaches outlined in Section~\ref{cp5:background} and we explore 
how can we use them, or their combination, in the design of techniques able to automatically identify text that likely contains information that is relevant to a developer task. 
Our design space considers establishing the relevance of a sentence to a given task:

\begin{itemize}
    \item using \textit{Skip-gram model}, or \textit{word2vec}, to identify sentences that are semantically similar to an input task and thus, likely to contain information relevant to that task;
    
    \item fine-tuning \textit{BERT} to identify sentences that are potentially relevant to an input task; and

    \item using \textit{frame semantics} to filter relevant sentence's based on their meaning.
\end{itemize}





Details for each technique are presented in the following sections.
Note that each techniques takes a task and a pertinent artifact as inputs and outputs the sentences 
that are most likely to contain information that assists a developer in completing that task. 
Details on how we determine the number of sentences outputed are available in Section~\ref{cp5:evaluation}.


% ------------------------------------------------



\subsection{Identifying Task-relevant Text through Semantic Similarity}



As a first technique, we explore if the sentences with the highest semantical similarity are most likely to contain information relevant to the input task.


To compute the semantic similarity between the sentences $\{a_1, a_2, \dots, a_n\}$ within a pertinent artifact and the sentences $\{t_1, t_2, \dots, t_m\}$ in a task,
we first
 use the Skip-gram model (Section~\ref{cp5:skip-gram}) with word embeddings trained for the software engineering domain~\cite{Efstathiou2018} to covert each of the sentences to their respective vector representations, $\{ w_{a_1}, w_{a_2}, \dots, w_{a_n}\}$ and $\{ w_{t_1}, w_{t_2}, \dots, w_{t_m}\}$. For that, we follow Conneau et al.'s guidelines~\cite{conneau2018} 
 and average the sum of the word embeddings in a sentence to obtain vector representation for the entire sentence.






 
 
 
 
 % Since word embeddings provide vector representations at the word level,
 
 
 
 Provided that we have embeddings for a sentence in an input task and an artifact, their semantic similarity is obtained computing the cosine similarity \red{ref} between their corresponding vectors:
%  , i.e., $w_t$ and $w_a$



 \begin{equation}
     sim(w_t,w_a) = \frac{w_t^Tw_a}{\|w_t\| \|w_a\|}
     \label{eq:word-sim}
 \end{equation}
 
\smallskip
 As there are multiple sentences in both entities, the semantic similarity of a sentence $a_i \in A$ is the maximun value obtained when computing the similarity of the vector for this sentence and the vectors for each of the sentences in a task $T$.

 \begin{equation}
    sim(a_i, T) = max(sim(w_{a_i}, w_{t_1}), ~\dots~ sim(w_{a_i}, w_{t_m})) \\
\end{equation}


 

Following these procedures, we sort the sentences in an artifact according to their semantic similarity, i.e., from highest to lowest, and output the top-n sentences as the ones likely relevant to an input software task.


\subsection{Identifying Task-relevant Text with BERT}


\subsection{Identifying Task-relevant Text with Frame Semantics}











% ------------------------------------------------


\subsection{Approaches Summary}


Table~\ref{tbl:approaches-summary} bundles the approaches that we explore.
The table provides a short identifier for each approach, identifies the research topic that serve as a basis for each approach and provides a short description for them. From now on, we refer to each approach according to their short identifier.

\gm{Isn't there multiple SEframes approaches?}

\input{sections/cp5/tbl-approaches-summary}