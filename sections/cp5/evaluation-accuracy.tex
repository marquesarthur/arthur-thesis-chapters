\subsection{Accuracy}


Motivated by the fact that the text marked by the annotators in the \acs{DS-android} differs
(Section~\ref{cp4:corpus-relevant-text}),
we follow procedures outlined by Lotufo et al.~\cite{Lotufo2012}
to measure how accurately our approach identifies text relevant to a task considering two evaluation scenarios:


\begin{enumerate}
    \item one where the text marked only by two or more annotators is considered relevant, and;
    
    \item a second where we consider that any of the text marked by the annotators contributes towards task completion.
\end{enumerate}



Standard precision and recall metrics~\cite{Manning2009IR} are suitable metrics for the first scenario. In the second, we use pyramid precision and pyramid recall metrics
to compute accuracy based on how many annotators marked the text~\cite{Nenkova2004, Lotufo2012}. 



To understand evaluation metrics, we refer to the evaluation outcomes of Table~\ref{tbl:type-I-II-errors}. 
The \textit{relevant} and \textit{not-relevant} columns represent the text 
marked (or not) by annotators~\cite{Lotufo2012} for a particular task and a pertinent artifact. Rows represent the text automatically identified by our approach for the same task and pertinent artifact.


\input{sections/cp5/tbl-eval-outcomes.tex}


\subsubsection{Precision and recall}


For standard precision and recall metrics, the relevant column in Table~\ref{tbl:type-I-II-errors} represents \textit{text marked by two or more annotators}. 
In this scenario, $precision$ is the fraction of the sentences
 identified that are relevant over the total number of sentences identified, as shown in Equation~\ref{eq:cp5:precision}.


\begin{equation}
\label{eq:cp5:precision}    
    Precision = \frac{TP}{TP + FP}
\end{equation}

\vspace{2mm}
Recall represents how many of all marked sentences are present identified by our approach (Equation~\ref{eq:cp5:recall}).


\begin{equation}
\label{eq:cp5:recall}        
    Recall = \frac{TP}{TP + FN}
\end{equation}




\subsubsection{Pyramid precision and pyramid recall}


For pyramid precision and pyramid recall, the relevant column in Table~\ref{tbl:type-I-II-errors} represents \textit{text marked by any annotator}. 


Pyramid precision considers the ``optimal'' sentences that could be identified, i.e., the sentences marked by most of the annotators, and compares that to the sentences  actually identified. Let $marked(s)$ be a function that returns the number of annotators who marked the sentence $s$, pyramid precision is defined as 
the summation of $marked(s)$ for the set of sentences identified ($s \in S$) over the set of optimal sentences derived from the golden data ($s \in G$).



\begin{equation}
\label{eq:cp5:pyramid-precision}    
    Pyramid Precision =  \frac{
        \sum_{s \in S} \text{ } marked(s)
    }{
        \sum_{s \in G} \text{ } marked(s)
    }
\end{equation}


% \vspace{3mm}
% As an example consider an artifact with a total of 6 marked sentences,
% where 2 sentences were marked by the three annotators, 3 by two annotators and 1 sentence by a single annotator. 
% When identifying three sentences, the optimal solution is $8$ $(2 \times 3 + 1 \times 2)$.
% Hence, an approach that identifies three sentences maked by one, two and all annotators 
% would have a pyramid precision score of $0.75$ or $\frac{ (1 + 2 + 3)}{(2 \times 3 + 1 \times 2)}$.




\vspace{2mm}
Pyramid recall calculates the fraction of the sentences identified by our approach over 
any of the sentences marked by the annotators. As we updated the set of $TP$ and $FN$ of Table~\ref{tbl:type-I-II-errors}, we can still compute pyramid precision using Equation~\ref{eq:cp5:recall}.




\subsubsection{Results}


When interpreting results, we favor precision/pyramid precision instead of recall/pyramid recall.
Our rationale is that a developer might completly abandon reading of an artifact 
due to a false positive~\cite{Singer1998, Brandt2009a}.
Thus, even if we are not able to identify all sentences that are indeed relevant, true positives may encourage a developer to inspect the said artifact more thoroughly.
% that would otherwise provide crucial information for her task~\cite{Rastkar2010}.



Table~\ref{tbl:approach-results-overall} show the accuracy of our approach 
over all features as well as per group of features (i.e., lexical, word semantics, and sentence semantics).


\input{sections/cp5/tbl-results-overall}



Table~\ref{tbl:approach-results-task} shows evaluation metrics by type of task, i.e., GitHub or StackOverflow.

\input{sections/cp5/tbl-results-by-task}

\art{Detail accuracy per type of artifact. 
It may be worth doing it only for all features, otherwise table will have too much information.}